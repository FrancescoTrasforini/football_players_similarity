{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, validation and test splits\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the columns you want to read\n",
    "columns_to_read = ['heatmapsId', 'playerId', 'teamId', 'season', 'passes-start-heatmap']\n",
    "\n",
    "# Read the CSV file with only the specified columns\n",
    "data = pd.read_csv('_normalized-heatmaps/normalized-heatmaps-chunk-1.csv', usecols=columns_to_read)\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)  # 70% train\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.33, random_state=42)  # 20% val, 10% test\n",
    "\n",
    "# Save to CSV\n",
    "train_data.to_csv(\"_Data-split/passes-start-train.csv\", index=False)\n",
    "val_data.to_csv(\"_Data-split/passes-start-validation.csv\", index=False)\n",
    "test_data.to_csv(\"_Data-split/passes-start-test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with normalization [0,1]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Specify the columns you want to read\n",
    "columns_to_read = ['heatmapsId', 'playerId', 'teamId', 'season', 'passes-start-heatmap']\n",
    "\n",
    "# Read the CSV file with only the specified columns\n",
    "data = pd.read_csv('_normalized-heatmaps/normalized-heatmaps-chunk-1.csv', usecols=columns_to_read)\n",
    "\n",
    "# Normalize the heatmap values\n",
    "def normalize_heatmap(heatmap_str, range_min=-1, range_max=1):\n",
    "    # Convert the heatmap string to a numpy array\n",
    "    heatmap = np.array(eval(heatmap_str))  # Assuming the heatmap is stored as a string of lists\n",
    "\n",
    "    # Normalize to the desired range\n",
    "    heatmap_min, heatmap_max = heatmap.min(), heatmap.max()\n",
    "    #print(f\"Min: {heatmap_min}\\nMax: {heatmap_max}\")\n",
    "    normalized_heatmap = ((heatmap - heatmap_min) / (heatmap_max - heatmap_min))  # [0, 1] scaling\n",
    "    \n",
    "    return normalized_heatmap.tolist()  # Convert back to list\n",
    "\n",
    "# Apply normalization and save as JSON strings\n",
    "data['passes-start-heatmap'] = data['passes-start-heatmap'].apply(\n",
    "    lambda x: json.dumps(normalize_heatmap(x))\n",
    ")\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)  # 70% train\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.33, random_state=42)  # 20% val, 10% test\n",
    "\n",
    "# Save to CSV\n",
    "train_data.to_csv(\"_Data-split/passes-start-train.csv\", index=False)\n",
    "val_data.to_csv(\"_Data-split/passes-start-validation.csv\", index=False)\n",
    "test_data.to_csv(\"_Data-split/passes-start-test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing data through a sigmoid function\n",
    "# seems to work quite well as an embedder\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Specify the columns you want to read\n",
    "columns_to_read = ['heatmapsId', 'playerId', 'teamId', 'season', 'passes-start-heatmap']\n",
    "\n",
    "# Read the CSV file with only the specified columns\n",
    "data = pd.read_csv('_normalized-heatmaps/normalized-heatmaps-chunk-1.csv', usecols=columns_to_read)\n",
    "\n",
    "# Apply sigmoid to the heatmap values\n",
    "def sigmoid_heatmap(heatmap_str):\n",
    "    # Convert the heatmap string to a numpy array\n",
    "    heatmap = np.array(eval(heatmap_str))  # Assuming the heatmap is stored as a string of lists\n",
    "\n",
    "    # Apply sigmoid\n",
    "    sigmoid_heatmap = 1 / (1 + np.exp(-heatmap))  # Sigmoid function\n",
    "    \n",
    "    return sigmoid_heatmap.tolist()  # Convert back to list\n",
    "\n",
    "# Apply sigmoid transformation and save as JSON strings\n",
    "data['passes-start-heatmap'] = data['passes-start-heatmap'].apply(\n",
    "    lambda x: json.dumps(sigmoid_heatmap(x))\n",
    ")\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)  # 70% train\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.33, random_state=42)  # 20% val, 10% test\n",
    "\n",
    "# Save to CSV\n",
    "train_data.to_csv(\"_Data-split/1passes-start-train.csv\", index=False)\n",
    "val_data.to_csv(\"_Data-split/1passes-start-validation.csv\", index=False)\n",
    "test_data.to_csv(\"_Data-split/1passes-start-test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing data through a sigmoid function\n",
    "# seems to work quite well as an embedder\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Specify the columns you want to read\n",
    "columns_to_read = ['heatmapsId', 'playerId', 'teamId', 'season', 'passes-start-heatmap']\n",
    "\n",
    "# Initialize an empty list to store data from each file\n",
    "all_data = []\n",
    "\n",
    "# Loop through files and read data\n",
    "for k in range(1, 25):  # Loop through files 1 to 24\n",
    "    file_path = f'_normalized-heatmaps/normalized-heatmaps-chunk-{k}.csv'\n",
    "    data_chunk = pd.read_csv(file_path, usecols=columns_to_read)\n",
    "    all_data.append(data_chunk)\n",
    "\n",
    "# Concatenate all the data into a single DataFrame\n",
    "data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Apply sigmoid to the heatmap values\n",
    "def sigmoid_heatmap(heatmap_str):\n",
    "    # Convert the heatmap string to a numpy array\n",
    "    heatmap = np.array(eval(heatmap_str))  # Assuming the heatmap is stored as a string of lists\n",
    "\n",
    "    # Apply sigmoid\n",
    "    sigmoid_heatmap = 1 / (1 + np.exp(-heatmap))  # Sigmoid function\n",
    "    \n",
    "    return sigmoid_heatmap.tolist()  # Convert back to list\n",
    "\n",
    "# Apply sigmoid transformation and save as JSON strings\n",
    "data['passes-start-heatmap'] = data['passes-start-heatmap'].apply(\n",
    "    lambda x: json.dumps(sigmoid_heatmap(x))\n",
    ")\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)  # 70% train\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.33, random_state=42)  # 20% val, 10% test\n",
    "\n",
    "# Save to CSV\n",
    "train_data.to_csv(\"_Data-split/all-chunks-passes-start-train.csv\", index=False)\n",
    "val_data.to_csv(\"_Data-split/all-chunks-passes-start-validation.csv\", index=False)\n",
    "test_data.to_csv(\"_Data-split/all-chunks-passes-start-test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple chunks, data normalization to be implemented\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the columns you want to read\n",
    "columns_to_read = ['heatmapsId', 'playerId', 'teamId', 'season', 'passes-start-heatmap']\n",
    "\n",
    "# Initialize an empty list to store data from each file\n",
    "all_data = []\n",
    "\n",
    "# Loop through files and read data\n",
    "for k in range(1, 3):  # Loop through files 1 to 2, using roughly half of the data\n",
    "    file_path = f'_normalized-heatmaps/normalized-heatmaps-chunk-{k}.csv'\n",
    "    data_chunk = pd.read_csv(file_path, usecols=columns_to_read)\n",
    "    all_data.append(data_chunk)\n",
    "\n",
    "# Concatenate all the data into a single DataFrame\n",
    "data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)  # 70% train\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.33, random_state=42)  # 20% val, 10% test\n",
    "\n",
    "# Save to CSV\n",
    "train_data.to_csv(\"_Data-split/passes-start-train.csv\", index=False)\n",
    "val_data.to_csv(\"_Data-split/passes-start-validation.csv\", index=False)\n",
    "test_data.to_csv(\"_Data-split/passes-start-test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch is installed. Version: {torch.__version__}\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"PyTorch is not installed in this environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class HeatmapCSVLoader(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.heatmaps = self.data[\"passes-start-heatmap\"]\n",
    "        self.ids = self.data[\"heatmapsId\"]\n",
    "        self.playerIds = self.data[\"playerId\"]\n",
    "        self.teamIds = self.data[\"teamId\"]\n",
    "        self.seasons = self.data[\"season\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.heatmaps)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Parse heatmap JSON string\n",
    "        heatmap = json.loads(self.heatmaps.iloc[idx])\n",
    "        heatmap_tensor = torch.tensor(heatmap, dtype=torch.float32)  # Convert to tensor\n",
    "        \n",
    "        # Add channel dimension for PyTorch (batch, channel, height, width)\n",
    "        heatmap_tensor = heatmap_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Return heatmap tensor and its metadata\n",
    "        return heatmap_tensor, self.ids.iloc[idx], self.playerIds.iloc[idx], self.teamIds.iloc[idx], self.seasons.iloc[idx]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_set = HeatmapCSVLoader(csv_path=\"_Data-split/passes-start-train.csv\")\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "val_set = HeatmapCSVLoader(csv_path=\"_Data-split/passes-start-validation.csv\")\n",
    "val_loader = DataLoader(val_set, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels, bottleneck_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Conv Block 1\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3)\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "        \n",
    "        # Conv Block 2\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, groups=64),\n",
    "            nn.Conv2d(128, 128, kernel_size=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, groups=128),\n",
    "            nn.Conv2d(128, 128, kernel_size=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3)\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "        \n",
    "        # Conv Block 3\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3)\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "        \n",
    "        # Compute flattened size after convolutions and pooling\n",
    "        self.flattened_size = 256 * 4 * 2  # Computed from input size [1, 35, 23]\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, bottleneck_size),\n",
    "            nn.Dropout(p=0.4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv Block 1\n",
    "        x = self.conv_block1(x)\n",
    "        x, indices1 = self.pool1(x)\n",
    "        #print(\"After Pool1:\", x.shape)\n",
    "        \n",
    "        # Conv Block 2\n",
    "        x = self.conv_block2(x)\n",
    "        x, indices2 = self.pool2(x)\n",
    "        #print(\"After Pool2:\", x.shape)\n",
    "        \n",
    "        # Conv Block 3\n",
    "        x = self.conv_block3(x)\n",
    "        x, indices3 = self.pool3(x)\n",
    "        #print(\"After Pool3:\", x.shape)\n",
    "\n",
    "        # Flatten and bottleneck\n",
    "        x = self.flatten(x)\n",
    "        #print(\"After Flatten:\", x.shape)\n",
    "        bottleneck = self.dense(x)\n",
    "        \n",
    "        return bottleneck, [indices1, indices2, indices3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_channels, bottleneck_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(bottleneck_size, 256 * 4 * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(256, 4, 2))\n",
    "\n",
    "        self.unpool3 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        self.deconv_block3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        self.deconv_block2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.unpool1 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        self.deconv_block1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, input_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(input_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, indices_list, output_sizes):\n",
    "        # Dense layer and unflatten\n",
    "        x = self.dense(x)\n",
    "        x = self.unflatten(x)\n",
    "        #print(\"After Unflatten:\", x.shape)\n",
    "\n",
    "        # Unpooling and deconvolution\n",
    "        #print(f\"output_sizes[2]: {output_sizes[2]}\")\n",
    "        x = self.unpool3(x, indices_list[2], output_size=output_sizes[2])\n",
    "        x = self.deconv_block3(x)\n",
    "        #print(\"Decoder After UnPool1:\", x.shape)\n",
    "\n",
    "        #print(f\"output_sizes[2]: {output_sizes[1]}\")\n",
    "        x = self.unpool2(x, indices_list[1], output_size=output_sizes[1])\n",
    "        x = self.deconv_block2(x)\n",
    "        #print(\"Decoder After UnPool2:\", x.shape)\n",
    "\n",
    "        #print(f\"output_sizes[2]: {output_sizes[0]}\")\n",
    "        x = self.unpool1(x, indices_list[0], output_size=output_sizes[0])\n",
    "        x = self.deconv_block1(x)\n",
    "        #print(\"Decoder After UnPool3:\", x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_channels, bottleneck_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(input_channels, bottleneck_size)\n",
    "        self.decoder = Decoder(input_channels, bottleneck_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        bottleneck, indices_list = self.encoder(x)\n",
    "\n",
    "        # Verify output_sizes match shapes before pooling\n",
    "        output_sizes = [\n",
    "            torch.Size([x.size(0), 1, 35, 23]),    # Before Pool1\n",
    "            torch.Size([x.size(0), 64, 17, 11]),  # Before Pool2\n",
    "            torch.Size([x.size(0), 128, 8, 5])    # Before Pool3\n",
    "        ]\n",
    "        #for i, size in enumerate(output_sizes):\n",
    "            #print(f\"output_sizes[{i}]: {size}\")\n",
    "\n",
    "        # Decoder\n",
    "        reconstructed = self.decoder(bottleneck, indices_list, output_sizes)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: nan\n",
      "Epoch [2/10], Loss: nan\n",
      "Epoch [3/10], Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     28\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Francesco\\anaconda3\\envs\\NBA_Project\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Francesco\\anaconda3\\envs\\NBA_Project\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Francesco\\anaconda3\\envs\\NBA_Project\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# To be deleted\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "autoencoder = Autoencoder(input_channels=1, bottleneck_size=64).to(device)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for reconstruction loss\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()  # Set model to training mode\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        images, ids, playerIds, teamIds, seasons = batch  # Unpack images and ignore the rest\n",
    "        #print(f\"Images Shape: {images.shape}\")\n",
    "        images = images.to(device)  # Move images to GPU if available\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstructed = autoencoder(images)\n",
    "        loss = criterion(reconstructed, images)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0130\n"
     ]
    }
   ],
   "source": [
    "# To be deleted\n",
    "\n",
    "autoencoder.eval()  # Set model to evaluation mode\n",
    "val_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        images, ids, playerIds, teamIds, seasons = batch  # Unpack images and ignore the rest\n",
    "        images = images.to(device)  # Move images to AMD GPU\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstructed = autoencoder(images)\n",
    "        loss = criterion(reconstructed, images)\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "\n",
    "print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: save embeddings and use \n",
    "import numpy as np\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 25\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "autoencoder = Autoencoder(input_channels=1, bottleneck_size=64).to(device)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for reconstruction loss\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-4)\n",
    "\n",
    "latent_vectors_train = {}  # Store latent vectors for training data\n",
    "latent_vectors_val = {}  # Store latent vectors for validation data\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()  # Set model to training mode\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    for batch in train_loader:\n",
    "        images, ids, playerIds, teamIds, seasons = batch  # Unpack images and metadata\n",
    "        images = images.to(device)  # Move images to GPU if available\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstructed = autoencoder(images)\n",
    "        loss = criterion(reconstructed, images)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Save latent representations for training data\n",
    "        latent_output = autoencoder.encoder(images)\n",
    "        embeddings = latent_output[0]\n",
    "        embeddings = embeddings.detach().cpu().numpy()  # Convert to numpy array\n",
    "        for i, emb in enumerate(embeddings):\n",
    "            key = (playerIds[i].item(), teamIds[i].item(), seasons[i])\n",
    "            if key not in latent_vectors_train:\n",
    "                latent_vectors_train[key] = []\n",
    "            latent_vectors_train[key].append(emb)\n",
    "    \n",
    "    # Print training loss\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Validation loop (if needed)\n",
    "    autoencoder.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images, ids, playerIds, teamIds, seasons = batch\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed = autoencoder(images)\n",
    "            loss = criterion(reconstructed, images)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Save latent representations for validation data\n",
    "            latent_output = autoencoder.encoder(images)\n",
    "            embeddings = latent_output[0]\n",
    "            embeddings = embeddings.detach().cpu().numpy()  # Convert to numpy array\n",
    "            for i, emb in enumerate(embeddings):\n",
    "                key = (playerIds[i].item(), teamIds[i].item(), seasons[i])\n",
    "                if key not in latent_vectors_val:\n",
    "                    latent_vectors_val[key] = []\n",
    "                latent_vectors_val[key].append(emb)\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
    "\n",
    "# Save latent vectors for retrieval task evaluation\n",
    "np.save(\"latent_vectors_train.npy\", latent_vectors_train)\n",
    "np.save(\"latent_vectors_val.npy\", latent_vectors_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter test set to use only players with multiple heatmaps in a team/season for the retrieval task\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load test dataset\n",
    "test_data = pd.read_csv(\"_Data-split/passes-start-test.csv\")\n",
    "\n",
    "# Group by playerId, teamId, and season and count rows in each group\n",
    "grouped = test_data.groupby([\"playerId\", \"teamId\", \"season\"]).size()\n",
    "\n",
    "# Filter combinations with more than one heatmap\n",
    "valid_combinations = grouped[grouped > 1].index  # Get the indices of valid groups\n",
    "\n",
    "# Create filtered test instances\n",
    "filtered_test_instances = defaultdict(list)\n",
    "for _, row in test_data.iterrows():\n",
    "    key = (row[\"playerId\"], row[\"teamId\"], row[\"season\"])\n",
    "    if key in valid_combinations:\n",
    "        # Add heatmap and heatmapsId to corresponding key\n",
    "        heatmap = json.loads(row[\"passes-start-heatmap\"])\n",
    "        heatmaps_id = row[\"heatmapsId\"]\n",
    "        filtered_test_instances[key].append((heatmaps_id, heatmap))\n",
    "\n",
    "# Convert to a list of tuples for later use\n",
    "filtered_test_instances = [\n",
    "    (playerId, teamId, season, heatmaps)\n",
    "    for (playerId, teamId, season), heatmaps in filtered_test_instances.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129832.0,\n",
       " 124,\n",
       " '2020-2021',\n",
       " [('129832.0-124-2020-2021-7',\n",
       "   [[0.5,\n",
       "     0.5000010534990429,\n",
       "     0.5000349179772949,\n",
       "     0.5004260343633677,\n",
       "     0.5019172196022352,\n",
       "     0.5031962259347426,\n",
       "     0.5019968135333582,\n",
       "     0.5004808447853081,\n",
       "     0.5000483986411923,\n",
       "     0.5000369632145562,\n",
       "     0.5004250426258572,\n",
       "     0.5019047617388616,\n",
       "     0.5031403950909445,\n",
       "     0.5019047617388616,\n",
       "     0.5004250426258572,\n",
       "     0.5000359097155188,\n",
       "     0.5000952833383017,\n",
       "     0.5027772260194102,\n",
       "     0.5334784868096027,\n",
       "     0.6487212477107398,\n",
       "     0.7451599210563594,\n",
       "     0.680302898912057,\n",
       "     0.5654947061425503],\n",
       "    [0.5,\n",
       "     0.5000124580453532,\n",
       "     0.5004135763267948,\n",
       "     0.5050596259742157,\n",
       "     0.5229211515362477,\n",
       "     0.5389070334030098,\n",
       "     0.525550781835517,\n",
       "     0.5068744697379547,\n",
       "     0.5008385874826571,\n",
       "     0.5004713342318867,\n",
       "     0.5050826148352541,\n",
       "     0.5226013151125993,\n",
       "     0.5371242960788789,\n",
       "     0.5225218820260256,\n",
       "     0.5050278099709997,\n",
       "     0.5004464799229312,\n",
       "     0.500574973993795,\n",
       "     0.5068500620336491,\n",
       "     0.5647116913714021,\n",
       "     0.7644078895456674,\n",
       "     0.8996208417089905,\n",
       "     0.8559278811985266,\n",
       "     0.6905035018907456],\n",
       "    [0.5,\n",
       "     0.5000558330854769,\n",
       "     0.501861387313762,\n",
       "     0.5229211515362477,\n",
       "     0.6044156501166916,\n",
       "     0.6804505307324651,\n",
       "     0.6346787791449188,\n",
       "     0.5449288575338771,\n",
       "     0.5069641501239845,\n",
       "     0.5027308740216954,\n",
       "     0.5243791812489584,\n",
       "     0.6025762720059624,\n",
       "     0.6622883348332399,\n",
       "     0.600050631434094,\n",
       "     0.5225929756447397,\n",
       "     0.5023762695672356,\n",
       "     0.5054495156470273,\n",
       "     0.5280472537119889,\n",
       "     0.6008810452035436,\n",
       "     0.7933027859766313,\n",
       "     0.9303385630284863,\n",
       "     0.9279034489296276,\n",
       "     0.8403481495338261],\n",
       "    [0.5,\n",
       "     0.500092084075383,\n",
       "     0.503105198972251,\n",
       "     0.5389194159905297,\n",
       "     0.6804990896252777,\n",
       "     0.8178412073208646,\n",
       "     0.7911236777698688,\n",
       "     0.6347019158151528,\n",
       "     0.5260571353146397,\n",
       "     0.5100613261391793,\n",
       "     0.5599090744421839,\n",
       "     0.6947200295003523,\n",
       "     0.7686192439782676,\n",
       "     0.666842301922523,\n",
       "     0.5389469257181261,\n",
       "     0.5089571933958696,\n",
       "     0.5295079574906565,\n",
       "     0.6112041794409954,\n",
       "     0.7109711651896399,\n",
       "     0.7806963610367161,\n",
       "     0.8836153483714112,\n",
       "     0.9470342918175756,\n",
       "     0.9632581942372199],\n",
       "    [0.5,\n",
       "     0.5000568557040997,\n",
       "     0.501974845272496,\n",
       "     0.5259622408652003,\n",
       "     0.6363918559896715,\n",
       "     0.7930948249219498,\n",
       "     0.8188858111498403,\n",
       "     0.6808586614469042,\n",
       "     0.5408803991959693,\n",
       "     0.5283085399441347,\n",
       "     0.6279567456334251,\n",
       "     0.7648533553146449,\n",
       "     0.7753390258172242,\n",
       "     0.6461829286492575,\n",
       "     0.5459154614860143,\n",
       "     0.5607577772362915,\n",
       "     0.6215362642325136,\n",
       "     0.7330305097389325,\n",
       "     0.8554660527584979,\n",
       "     0.8773639191110687,\n",
       "     0.8959086044720437,\n",
       "     0.968620077649604,\n",
       "     0.9910481247229078],\n",
       "    [0.5,\n",
       "     0.500024916090691,\n",
       "     0.5008809397791224,\n",
       "     0.5118985594555753,\n",
       "     0.5671655938849275,\n",
       "     0.6683849843651772,\n",
       "     0.6997151346382854,\n",
       "     0.6093117499815297,\n",
       "     0.5268686283505654,\n",
       "     0.546633595567713,\n",
       "     0.7085767715403419,\n",
       "     0.8628146147137142,\n",
       "     0.8509419424119602,\n",
       "     0.7029791446608114,\n",
       "     0.658633920918183,\n",
       "     0.800227331832371,\n",
       "     0.8787886089999827,\n",
       "     0.8777603634511423,\n",
       "     0.9294592433751641,\n",
       "     0.9614696236167575,\n",
       "     0.953613621442852,\n",
       "     0.9672301069054913,\n",
       "     0.9751719701340779],\n",
       "    [0.50006829113064,\n",
       "     0.5001499315145039,\n",
       "     0.5019511144537278,\n",
       "     0.5233452535864185,\n",
       "     0.6061835883083123,\n",
       "     0.6830969218289987,\n",
       "     0.6364033869416186,\n",
       "     0.5455051809255458,\n",
       "     0.510926145249062,\n",
       "     0.5515635877145042,\n",
       "     0.7471060966866827,\n",
       "     0.9261989406081375,\n",
       "     0.9446252224711537,\n",
       "     0.8777979024853857,\n",
       "     0.9249962841008855,\n",
       "     0.98863937511215,\n",
       "     0.9949849561848471,\n",
       "     0.9870200541083288,\n",
       "     0.9748101052890218,\n",
       "     0.9800995653631438,\n",
       "     0.9731956519000466,\n",
       "     0.9624005755485877,\n",
       "     0.9155147121537403],\n",
       "    [0.5022614762487069,\n",
       "     0.5031742890980619,\n",
       "     0.5049002320170939,\n",
       "     0.5375250933261608,\n",
       "     0.6610008278839486,\n",
       "     0.7512948922188314,\n",
       "     0.6636966050953188,\n",
       "     0.540885240164786,\n",
       "     0.5110045268644853,\n",
       "     0.5607971052197814,\n",
       "     0.7815050528924329,\n",
       "     0.9558410792483046,\n",
       "     0.9774248121759197,\n",
       "     0.976213640888763,\n",
       "     0.9973565660625071,\n",
       "     0.9998619864017654,\n",
       "     0.9999359522519377,\n",
       "     0.999716119392073,\n",
       "     0.9957036105044836,\n",
       "     0.980435137019937,\n",
       "     0.9658592206094573,\n",
       "     0.9402076805573767,\n",
       "     0.8392119742515158],\n",
       "    [0.527590843588672,\n",
       "     0.5376271003097999,\n",
       "     0.5244225146007727,\n",
       "     0.527536199690165,\n",
       "     0.6000199703071738,\n",
       "     0.6609807794584247,\n",
       "     0.6045046929174606,\n",
       "     0.5451658382075417,\n",
       "     0.5448077790392666,\n",
       "     0.5973208965341356,\n",
       "     0.8319009237102991,\n",
       "     0.9743419841581574,\n",
       "     0.9886440314045196,\n",
       "     0.9941054761134481,\n",
       "     0.999716597541497,\n",
       "     0.9999877893507806,\n",
       "     0.9999970643527885,\n",
       "     0.9999875084398567,\n",
       "     0.9990667594137562,\n",
       "     0.9827937064750791,\n",
       "     0.9579414947772011,\n",
       "     0.9096821697390347,\n",
       "     0.7715101460056376],\n",
       "    [0.6231497610989362,\n",
       "     0.6649693447145363,\n",
       "     0.601823236220451,\n",
       "     0.5279340448800292,\n",
       "     0.5244438083035919,\n",
       "     0.5389645914455043,\n",
       "     0.5449634933517933,\n",
       "     0.6050691719521694,\n",
       "     0.669862486810893,\n",
       "     0.6955080274774494,\n",
       "     0.9036575346295638,\n",
       "     0.9940334510824931,\n",
       "     0.9985565267520478,\n",
       "     0.9982444183148615,\n",
       "     0.9994228402470039,\n",
       "     0.9999687497499367,\n",
       "     0.9999977690179241,\n",
       "     0.9999846947699583,\n",
       "     0.9978916466443665,\n",
       "     0.9639932095239603,\n",
       "     0.9036636067058097,\n",
       "     0.8003891185239776,\n",
       "     0.6564466182808594],\n",
       "    [0.7159569859012553,\n",
       "     0.7790299495496872,\n",
       "     0.6805708710506566,\n",
       "     0.5424727954965455,\n",
       "     0.5054017178891856,\n",
       "     0.5062144691361659,\n",
       "     0.5393651310828038,\n",
       "     0.6664093033150473,\n",
       "     0.7777105169400343,\n",
       "     0.8009959613497865,\n",
       "     0.9533230804743745,\n",
       "     0.9991299984919568,\n",
       "     0.9999599568182582,\n",
       "     0.999921710831292,\n",
       "     0.9996301463842433,\n",
       "     0.9999643450013916,\n",
       "     0.999998001729779,\n",
       "     0.9999258911858014,\n",
       "     0.9905961921436057,\n",
       "     0.9464280458915072,\n",
       "     0.8720669283635419,\n",
       "     0.7037501802838395,\n",
       "     0.5639507638576793],\n",
       "    [0.7286504123429208,\n",
       "     0.7935208355667005,\n",
       "     0.6916915661060433,\n",
       "     0.5449402006029717,\n",
       "     0.5038095299501625,\n",
       "     0.5021240343253378,\n",
       "     0.5249493555106218,\n",
       "     0.6286948648644267,\n",
       "     0.7901283823832822,\n",
       "     0.9122753897747018,\n",
       "     0.9907179471612594,\n",
       "     0.9998395602996045,\n",
       "     0.9999940557941144,\n",
       "     0.9999951562129521,\n",
       "     0.9999681565933707,\n",
       "     0.9999863702596569,\n",
       "     0.999994662170036,\n",
       "     0.999537058380492,\n",
       "     0.9799559448129279,\n",
       "     0.9586790070811556,\n",
       "     0.9185315279492473,\n",
       "     0.7579919785232663,\n",
       "     0.5797101708786992],\n",
       "    [0.7159569859012553,\n",
       "     0.7790292454052395,\n",
       "     0.6805414494252581,\n",
       "     0.5420642053457244,\n",
       "     0.5034878417461789,\n",
       "     0.5008019933441129,\n",
       "     0.5114327275492667,\n",
       "     0.59549795568263,\n",
       "     0.8367329995526267,\n",
       "     0.9781444667967032,\n",
       "     0.998442901033393,\n",
       "     0.999894780905861,\n",
       "     0.9999742139644534,\n",
       "     0.9999813163620807,\n",
       "     0.9999692684504871,\n",
       "     0.9999723912610173,\n",
       "     0.9999346390558035,\n",
       "     0.9971308684287385,\n",
       "     0.9684785842431165,\n",
       "     0.968412903346324,\n",
       "     0.9540012580844133,\n",
       "     0.8559393088892194,\n",
       "     0.6750981021467036],\n",
       "    [0.6231497901060306,\n",
       "     0.6649591540661982,\n",
       "     0.6014406512935748,\n",
       "     0.5230106598622657,\n",
       "     0.5024133293125729,\n",
       "     0.5030367388139234,\n",
       "     0.5208199477054225,\n",
       "     0.6375444953606465,\n",
       "     0.8962681058459838,\n",
       "     0.9876288773822651,\n",
       "     0.9976025964599239,\n",
       "     0.9991483093656972,\n",
       "     0.9992398545806028,\n",
       "     0.9992783771191195,\n",
       "     0.9994769398872161,\n",
       "     0.9997775024903085,\n",
       "     0.9997331796945883,\n",
       "     0.99588260799176,\n",
       "     0.9634397893612716,\n",
       "     0.965839379611888,\n",
       "     0.969226251313119,\n",
       "     0.9254970043382251,\n",
       "     0.7792102637224606],\n",
       "    [0.5275918630933001,\n",
       "     0.5376052560426879,\n",
       "     0.5230017612347128,\n",
       "     0.5072993653021312,\n",
       "     0.5085987430171439,\n",
       "     0.5282709916270742,\n",
       "     0.5888849152809271,\n",
       "     0.7784548889564493,\n",
       "     0.9546904818701191,\n",
       "     0.9844882685971726,\n",
       "     0.9800531920809253,\n",
       "     0.9811150060186595,\n",
       "     0.990288352902128,\n",
       "     0.9934182629444972,\n",
       "     0.9910732792750754,\n",
       "     0.9947340963578388,\n",
       "     0.9965618837814784,\n",
       "     0.9851611558432479,\n",
       "     0.9332351602821257,\n",
       "     0.945634878779553,\n",
       "     0.9703642127666537,\n",
       "     0.9544985195459129,\n",
       "     0.8451679752511733],\n",
       "    [0.5022739340378007,\n",
       "     0.5034947445599307,\n",
       "     0.5069312837310864,\n",
       "     0.5247658827041523,\n",
       "     0.5595949188263059,\n",
       "     0.6268974532652136,\n",
       "     0.7319271192718139,\n",
       "     0.8809178369657419,\n",
       "     0.9708905295887017,\n",
       "     0.9737437937200217,\n",
       "     0.9305647321877265,\n",
       "     0.9244317950329571,\n",
       "     0.9734910215950735,\n",
       "     0.98686035605299,\n",
       "     0.9563618743721657,\n",
       "     0.9028318995012778,\n",
       "     0.9018580969671437,\n",
       "     0.8618873982276669,\n",
       "     0.8430634631202379,\n",
       "     0.929978684135424,\n",
       "     0.9736267202800448,\n",
       "     0.9674119849285782,\n",
       "     0.8762927626400323],\n",
       "    [0.5001241242142237,\n",
       "     0.5019420039166852,\n",
       "     0.5226570646513254,\n",
       "     0.6025347217761313,\n",
       "     0.6931192284873822,\n",
       "     0.7473649774121098,\n",
       "     0.7985634663767611,\n",
       "     0.8321324526650153,\n",
       "     0.890290544013274,\n",
       "     0.909170646794814,\n",
       "     0.9250433473386576,\n",
       "     0.9523581460477412,\n",
       "     0.9809709661047301,\n",
       "     0.9912321949419959,\n",
       "     0.9498975785403517,\n",
       "     0.7489802642385966,\n",
       "     0.6777729796244465,\n",
       "     0.705562710308875,\n",
       "     0.7925096757433194,\n",
       "     0.9018593392138774,\n",
       "     0.9428785032890367,\n",
       "     0.9098077977372756,\n",
       "     0.769416092936146],\n",
       "    [0.500092053194977,\n",
       "     0.5030483454208717,\n",
       "     0.5371242960788789,\n",
       "     0.6622059192611963,\n",
       "     0.7663518776439824,\n",
       "     0.7451475014525273,\n",
       "     0.7019901853342326,\n",
       "     0.6530399564026328,\n",
       "     0.6594568612550827,\n",
       "     0.7801930242189035,\n",
       "     0.9129421711326985,\n",
       "     0.9476197585597175,\n",
       "     0.9621427281636696,\n",
       "     0.9797914894761894,\n",
       "     0.9445841932659861,\n",
       "     0.8102837477075744,\n",
       "     0.8129630462983264,\n",
       "     0.9044882010473305,\n",
       "     0.959259378837205,\n",
       "     0.9688222583302807,\n",
       "     0.9328125564407254,\n",
       "     0.7851669074874743,\n",
       "     0.6040826921670405],\n",
       "    [0.5000558330854769,\n",
       "     0.5018489294399099,\n",
       "     0.5225218820260256,\n",
       "     0.5999950693022557,\n",
       "     0.6650444956313657,\n",
       "     0.6210995826214075,\n",
       "     0.5606892633385702,\n",
       "     0.5353804338903138,\n",
       "     0.5601021665090624,\n",
       "     0.6663950272771813,\n",
       "     0.7813757040844962,\n",
       "     0.8038421664824406,\n",
       "     0.8175317821932196,\n",
       "     0.9098827005896561,\n",
       "     0.9315572423276084,\n",
       "     0.8897602541213939,\n",
       "     0.9260764875764678,\n",
       "     0.9835636761880141,\n",
       "     0.9966759716223704,\n",
       "     0.9953201921245622,\n",
       "     0.9492410769840104,\n",
       "     0.7062748238521586,\n",
       "     0.538155196060767],\n",
       "    [0.5000124580453532,\n",
       "     0.5004125537088568,\n",
       "     0.5050267874557455,\n",
       "     0.5225432449853854,\n",
       "     0.5374790928091836,\n",
       "     0.5244120958185906,\n",
       "     0.5099603341760708,\n",
       "     0.5253146615001266,\n",
       "     0.605478057430327,\n",
       "     0.6853803347085823,\n",
       "     0.6576437833609834,\n",
       "     0.5952327384513647,\n",
       "     0.6168392629812199,\n",
       "     0.7834926112515952,\n",
       "     0.879026926046391,\n",
       "     0.8466172552555369,\n",
       "     0.8906423809590858,\n",
       "     0.9767724408580271,\n",
       "     0.9953833313136463,\n",
       "     0.992872732799237,\n",
       "     0.9161059306453024,\n",
       "     0.6316011852874499,\n",
       "     0.5146563311977133],\n",
       "    [0.5000010226186359,\n",
       "     0.500033864478257,\n",
       "     0.5004125845892429,\n",
       "     0.5018499520445546,\n",
       "     0.5030608030012715,\n",
       "     0.5019968135333582,\n",
       "     0.5035539529543415,\n",
       "     0.537192698647847,\n",
       "     0.6609628657405553,\n",
       "     0.7515192020869529,\n",
       "     0.6650716086210247,\n",
       "     0.5447178988913255,\n",
       "     0.5348884240566013,\n",
       "     0.6251494189296462,\n",
       "     0.702032477508595,\n",
       "     0.6842192337353442,\n",
       "     0.7786884350099397,\n",
       "     0.9316362009028943,\n",
       "     0.9748196895123141,\n",
       "     0.9617992236844899,\n",
       "     0.8170112301813839,\n",
       "     0.5756066109388454,\n",
       "     0.5064778691023888],\n",
       "    [0.500000030880407,\n",
       "     0.5000010226186359,\n",
       "     0.5000124580453532,\n",
       "     0.5000558330854769,\n",
       "     0.500092053194977,\n",
       "     0.5001116661695613,\n",
       "     0.5018614181937411,\n",
       "     0.5225115211529752,\n",
       "     0.5996119260887202,\n",
       "     0.6606108648001283,\n",
       "     0.5997548866340726,\n",
       "     0.5231149487480896,\n",
       "     0.5074120297030201,\n",
       "     0.5246058071791779,\n",
       "     0.5425409934107908,\n",
       "     0.5525761950088685,\n",
       "     0.6516751062832453,\n",
       "     0.8279632851108123,\n",
       "     0.8995459894117257,\n",
       "     0.8457202698869198,\n",
       "     0.6656420344524293,\n",
       "     0.5319734127386638,\n",
       "     0.5025195728107485],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5000124580453532,\n",
       "     0.5004125537088568,\n",
       "     0.5050257649404489,\n",
       "     0.5225094492433539,\n",
       "     0.5370687703326523,\n",
       "     0.5225105006071693,\n",
       "     0.505060648488115,\n",
       "     0.5008375957472146,\n",
       "     0.5019307309465716,\n",
       "     0.5036233248909822,\n",
       "     0.5088964472874783,\n",
       "     0.5463745126659317,\n",
       "     0.6417043301428341,\n",
       "     0.704119302061353,\n",
       "     0.6441155466989731,\n",
       "     0.547715384864027,\n",
       "     0.5073602089104637,\n",
       "     0.5005189232217093],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5000010226186359,\n",
       "     0.500033864478257,\n",
       "     0.5004125537088568,\n",
       "     0.5018489294399099,\n",
       "     0.5030483454208717,\n",
       "     0.5018489603198947,\n",
       "     0.5004135763267948,\n",
       "     0.5000463225235321,\n",
       "     0.5000578783227221,\n",
       "     0.5001384065965536,\n",
       "     0.5008830158903413,\n",
       "     0.5069342306477369,\n",
       "     0.5260313672364841,\n",
       "     0.5408911702648385,\n",
       "     0.5261097261537053,\n",
       "     0.50697657487379,\n",
       "     0.5008396100984129,\n",
       "     0.500048429521599],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.500000030880407,\n",
       "     0.5000010226186359,\n",
       "     0.5000124580453532,\n",
       "     0.5000558330854769,\n",
       "     0.500092053194977,\n",
       "     0.5000558330854769,\n",
       "     0.5000124580453532,\n",
       "     0.5000010226186359,\n",
       "     0.5000000617608141,\n",
       "     0.5000020452372718,\n",
       "     0.5000463534039389,\n",
       "     0.5004694093689146,\n",
       "     0.501953439169878,\n",
       "     0.5031600072878043,\n",
       "     0.501953439169878,\n",
       "     0.5004694093689146,\n",
       "     0.5000463534039389,\n",
       "     0.5000020761176789],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.500000030880407,\n",
       "     0.5000010226186359,\n",
       "     0.5000124580453532,\n",
       "     0.5000558330854769,\n",
       "     0.500092053194977,\n",
       "     0.5000558330854769,\n",
       "     0.5000124580453532,\n",
       "     0.5000010226186359,\n",
       "     0.500000030880407],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5]]),\n",
       "  ('129832.0-124-2020-2021-6',\n",
       "   [[0.5000376510037478,\n",
       "     0.5000513155630316,\n",
       "     0.500030799525006,\n",
       "     0.500006868504093,\n",
       "     0.5000005808264685,\n",
       "     0.5000005808264685,\n",
       "     0.500006868504093,\n",
       "     0.5000307824996866,\n",
       "     0.5000507517618884,\n",
       "     0.5000307824996866,\n",
       "     0.500006902554732,\n",
       "     0.5000091407340083,\n",
       "     0.5002604589836186,\n",
       "     0.5030748308349099,\n",
       "     0.5138208248204761,\n",
       "     0.52528961375689,\n",
       "     0.5270143635911354,\n",
       "     0.5253292622748482,\n",
       "     0.5140652231987505,\n",
       "     0.504151753806465,\n",
       "     0.5020163281489438,\n",
       "     0.5010656367646495,\n",
       "     0.5002541542823439],\n",
       "    [0.5012468274237646,\n",
       "     0.501699331525283,\n",
       "     0.501019955802531,\n",
       "     0.5002280174026789,\n",
       "     0.500026102835085,\n",
       "     0.5000500168305737,\n",
       "     0.5002782053506891,\n",
       "     0.5010501573463534,\n",
       "     0.5016875296348899,\n",
       "     0.5010199387772823,\n",
       "     0.5002285982290262,\n",
       "     0.5000873749149027,\n",
       "     0.5014946411068465,\n",
       "     0.5147379488086209,\n",
       "     0.5610467941200826,\n",
       "     0.6077356508262747,\n",
       "     0.6147466451489207,\n",
       "     0.6086288693142374,\n",
       "     0.5641594396209093,\n",
       "     0.5280570934715283,\n",
       "     0.5228012397617193,\n",
       "     0.5127525795479344,\n",
       "     0.5030558340496618],\n",
       "    [0.5151848280559614,\n",
       "     0.5206903708030926,\n",
       "     0.5124239849380855,\n",
       "     0.5027964792192139,\n",
       "     0.5004931218797583,\n",
       "     0.501311315771904,\n",
       "     0.5045130622902866,\n",
       "     0.5134922670844404,\n",
       "     0.5207216463503342,\n",
       "     0.5124415510485348,\n",
       "     0.502786351887228,\n",
       "     0.5007468660920998,\n",
       "     0.5074939840795895,\n",
       "     0.5482746160940571,\n",
       "     0.6457240886966769,\n",
       "     0.7193385323087195,\n",
       "     0.7310995896199526,\n",
       "     0.7275732881833223,\n",
       "     0.6622214658039685,\n",
       "     0.6140554337427921,\n",
       "     0.6085836865032093,\n",
       "     0.5601746770809363,\n",
       "     0.5149220749503792],\n",
       "    [0.5676570999162771,\n",
       "     0.5917313625704012,\n",
       "     0.5554897445427385,\n",
       "     0.5129019866172896,\n",
       "     0.5049114603439083,\n",
       "     0.5154239004210315,\n",
       "     0.5349814260362973,\n",
       "     0.5697160679373597,\n",
       "     0.5945909049338624,\n",
       "     0.5559785956105672,\n",
       "     0.5125615836489191,\n",
       "     0.503362063879626,\n",
       "     0.5279606754075437,\n",
       "     0.6355396403454283,\n",
       "     0.7766115794526377,\n",
       "     0.8260620922462893,\n",
       "     0.8383195539845473,\n",
       "     0.8533290998800676,\n",
       "     0.8147054142441809,\n",
       "     0.7633454926065877,\n",
       "     0.7212609736872088,\n",
       "     0.6236607700825089,\n",
       "     0.5383577639752386],\n",
       "    [0.6103957951075925,\n",
       "     0.6484204908513377,\n",
       "     0.5913125699760118,\n",
       "     0.5253021385543721,\n",
       "     0.5285091561872949,\n",
       "     0.5816190471796705,\n",
       "     0.6370248890119751,\n",
       "     0.6699875588031797,\n",
       "     0.6745553793379206,\n",
       "     0.5977561899658242,\n",
       "     0.5224823549578267,\n",
       "     0.5085521044448303,\n",
       "     0.5564455993861612,\n",
       "     0.7163830904601289,\n",
       "     0.8572590170628098,\n",
       "     0.8827390168576102,\n",
       "     0.8955411374114464,\n",
       "     0.9223878859565159,\n",
       "     0.9077455255481085,\n",
       "     0.840152502609662,\n",
       "     0.7516791826888382,\n",
       "     0.6655172348670331,\n",
       "     0.5825824484258648],\n",
       "    [0.5679054265533492,\n",
       "     0.5929648469715568,\n",
       "     0.5609046774837608,\n",
       "     0.5399224887181329,\n",
       "     0.5983034751059318,\n",
       "     0.7052044041090568,\n",
       "     0.7814878851506091,\n",
       "     0.7895839983491597,\n",
       "     0.7252277069465444,\n",
       "     0.6119868918178369,\n",
       "     0.5345002081705545,\n",
       "     0.5280042293572964,\n",
       "     0.6007714949686972,\n",
       "     0.7336317320075971,\n",
       "     0.8177708383818761,\n",
       "     0.8378616214254018,\n",
       "     0.8589062343941308,\n",
       "     0.8920660517765849,\n",
       "     0.8792081207868151,\n",
       "     0.7919448225287634,\n",
       "     0.7058193724946596,\n",
       "     0.6761048667826488,\n",
       "     0.6146196133744597],\n",
       "    [0.5182106288844803,\n",
       "     0.5341444938138916,\n",
       "     0.5468924849566661,\n",
       "     0.5914609243838389,\n",
       "     0.69336710294073,\n",
       "     0.7848722840288521,\n",
       "     0.87188462793046,\n",
       "     0.9208061671668143,\n",
       "     0.8754447142469396,\n",
       "     0.7366132462326326,\n",
       "     0.6253287667322669,\n",
       "     0.6281972226278535,\n",
       "     0.7300211261521825,\n",
       "     0.8330131242451319,\n",
       "     0.8859455293641573,\n",
       "     0.9379878203461394,\n",
       "     0.9588102206229783,\n",
       "     0.9522576230021991,\n",
       "     0.898153599574614,\n",
       "     0.7848734555002255,\n",
       "     0.673734441470231,\n",
       "     0.6181291362016337,\n",
       "     0.5714069472157715],\n",
       "    [0.5147312402403876,\n",
       "     0.5588239981008896,\n",
       "     0.6123404790524292,\n",
       "     0.6554070131131302,\n",
       "     0.7161991962497322,\n",
       "     0.7616350404354414,\n",
       "     0.8858893966598477,\n",
       "     0.9678002803274564,\n",
       "     0.9652350622356484,\n",
       "     0.8934482593925138,\n",
       "     0.81784835128995,\n",
       "     0.8645968357711215,\n",
       "     0.918928906262906,\n",
       "     0.9558523954719103,\n",
       "     0.984133013813136,\n",
       "     0.9975028404428591,\n",
       "     0.999077478346167,\n",
       "     0.9983122412436288,\n",
       "     0.9875145146924761,\n",
       "     0.930872431183974,\n",
       "     0.7855864752173624,\n",
       "     0.6171366305190042,\n",
       "     0.5409844452509694],\n",
       "    [0.5222092166477064,\n",
       "     0.5918371887977839,\n",
       "     0.6583993765051127,\n",
       "     0.647377197779186,\n",
       "     0.640506071815526,\n",
       "     0.6902903047760517,\n",
       "     0.8549855794143435,\n",
       "     0.9639427102461486,\n",
       "     0.9789162942566354,\n",
       "     0.953682157572465,\n",
       "     0.934863419086109,\n",
       "     0.9714963588934573,\n",
       "     0.9856610074201566,\n",
       "     0.9914350862522322,\n",
       "     0.9972537229363219,\n",
       "     0.999673921921475,\n",
       "     0.9998818810679896,\n",
       "     0.9997642390693752,\n",
       "     0.9976200144301222,\n",
       "     0.9828684681493298,\n",
       "     0.9279745678480776,\n",
       "     0.7712066708937507,\n",
       "     0.6077234418833568],\n",
       "    [0.5134421155329757,\n",
       "     0.5557154989843703,\n",
       "     0.5941718004983363,\n",
       "     0.5765124265415147,\n",
       "     0.5878505879309219,\n",
       "     0.7033273881856482,\n",
       "     0.8725690030667306,\n",
       "     0.9601315530558778,\n",
       "     0.9801141505848603,\n",
       "     0.9751757272992498,\n",
       "     0.9686325345246458,\n",
       "     0.9837985798853922,\n",
       "     0.993363786426496,\n",
       "     0.9967972857542322,\n",
       "     0.9983904731703822,\n",
       "     0.9996025179142279,\n",
       "     0.9998059821466367,\n",
       "     0.9996194308774469,\n",
       "     0.9966840634026878,\n",
       "     0.9868860188855977,\n",
       "     0.9743524415602546,\n",
       "     0.8970105892916402,\n",
       "     0.6965190418526641],\n",
       "    [0.5030058020320591,\n",
       "     0.512737250576605,\n",
       "     0.5257797103331856,\n",
       "     0.5554718189948422,\n",
       "     0.6720466809396499,\n",
       "     0.8322306303090024,\n",
       "     0.9327447417372768,\n",
       "     0.971350476482667,\n",
       "     0.9775684521527644,\n",
       "     0.9790532385387851,\n",
       "     0.9828280860628196,\n",
       "     0.9874730547327975,\n",
       "     0.9933967318745387,\n",
       "     0.9974040090988247,\n",
       "     0.9984868365947541,\n",
       "     0.999563672766679,\n",
       "     0.9998692087634626,\n",
       "     0.9997549875404427,\n",
       "     0.9972751108668753,\n",
       "     0.9893055015227132,\n",
       "     0.982030490155927,\n",
       "     0.9228099713212857,\n",
       "     0.7262651601055131],\n",
       "    [0.5002769236439323,\n",
       "     0.5021727335581441,\n",
       "     0.5185619829720547,\n",
       "     0.6108068207899627,\n",
       "     0.8121689186720629,\n",
       "     0.9344820511705395,\n",
       "     0.9729788336260791,\n",
       "     0.9841418334652009,\n",
       "     0.972376657871644,\n",
       "     0.9653817856433837,\n",
       "     0.983214777038,\n",
       "     0.9894764254537883,\n",
       "     0.9903539235053747,\n",
       "     0.9950521372406501,\n",
       "     0.9979029378693262,\n",
       "     0.9996249809392511,\n",
       "     0.9999090118818934,\n",
       "     0.9997787038665727,\n",
       "     0.9986188824551437,\n",
       "     0.9961054206988802,\n",
       "     0.9882772331328191,\n",
       "     0.9380047860253119,\n",
       "     0.7689955028030662],\n",
       "    [0.5000581840670425,\n",
       "     0.5018312249677228,\n",
       "     0.5245339925784552,\n",
       "     0.6386660465774961,\n",
       "     0.8323067376703505,\n",
       "     0.9321717349415011,\n",
       "     0.9689061287162466,\n",
       "     0.9795293635760826,\n",
       "     0.947312019296775,\n",
       "     0.8912073474258183,\n",
       "     0.9444183744026482,\n",
       "     0.9788068643186714,\n",
       "     0.9843506747945521,\n",
       "     0.9931320706561161,\n",
       "     0.9983529156793784,\n",
       "     0.9997231631900911,\n",
       "     0.9998776442749584,\n",
       "     0.9995945226248864,\n",
       "     0.9990623233124039,\n",
       "     0.9987971727532549,\n",
       "     0.9959643496997046,\n",
       "     0.9700789723614571,\n",
       "     0.8342694383134118],\n",
       "    [0.5000307824996866,\n",
       "     0.5011021089188078,\n",
       "     0.5152112727359746,\n",
       "     0.5900859408799162,\n",
       "     0.7458951968404,\n",
       "     0.8645738250410999,\n",
       "     0.9166024580598422,\n",
       "     0.9242923476079598,\n",
       "     0.8460541471671699,\n",
       "     0.7357406859166041,\n",
       "     0.8272068177709289,\n",
       "     0.9471976935683237,\n",
       "     0.9833205622228897,\n",
       "     0.9963161841459148,\n",
       "     0.999283338782579,\n",
       "     0.9997653726983163,\n",
       "     0.9997585686822714,\n",
       "     0.9992033548430389,\n",
       "     0.9986421887449287,\n",
       "     0.9986119790606627,\n",
       "     0.9971686533243931,\n",
       "     0.9784240860018486,\n",
       "     0.843638049801272],\n",
       "    [0.500006868504093,\n",
       "     0.5003238694696344,\n",
       "     0.5059948673588904,\n",
       "     0.5522415561627944,\n",
       "     0.6985585589636922,\n",
       "     0.8262412804284478,\n",
       "     0.8482721206568434,\n",
       "     0.8393330677960625,\n",
       "     0.7911649705276943,\n",
       "     0.7318034498213262,\n",
       "     0.810531312828052,\n",
       "     0.9485724243189815,\n",
       "     0.9887008302946221,\n",
       "     0.9973716707222962,\n",
       "     0.9993808956827264,\n",
       "     0.9996685986860975,\n",
       "     0.999222686648581,\n",
       "     0.9971582410475919,\n",
       "     0.9944416548593545,\n",
       "     0.9905743155113034,\n",
       "     0.9808575127636382,\n",
       "     0.9173817781701507,\n",
       "     0.7222373884624211],\n",
       "    [0.5000005638011491,\n",
       "     0.5001321489412396,\n",
       "     0.5039921432941001,\n",
       "     0.5469523987779051,\n",
       "     0.6999622131335455,\n",
       "     0.8199642882803234,\n",
       "     0.8101480465710791,\n",
       "     0.8086985635767715,\n",
       "     0.8159313405470578,\n",
       "     0.8015548408137053,\n",
       "     0.8819540032227369,\n",
       "     0.9696453955542782,\n",
       "     0.9835281384751942,\n",
       "     0.9862878617036179,\n",
       "     0.9957275058448899,\n",
       "     0.9983398234348672,\n",
       "     0.9950412307187634,\n",
       "     0.9798512190984588,\n",
       "     0.9696635567723658,\n",
       "     0.9535695212730352,\n",
       "     0.9088659222460963,\n",
       "     0.7645128501766002,\n",
       "     0.5868795424536418],\n",
       "    [0.5000000170253195,\n",
       "     0.5000958520952694,\n",
       "     0.5031629050804782,\n",
       "     0.5384729668347256,\n",
       "     0.6677478331864825,\n",
       "     0.7751582300059003,\n",
       "     0.7625687041485135,\n",
       "     0.7540925980931468,\n",
       "     0.7445618967891893,\n",
       "     0.7544127245505137,\n",
       "     0.8886393427813084,\n",
       "     0.9676217493559988,\n",
       "     0.9614510894061481,\n",
       "     0.9225716369686615,\n",
       "     0.9504787945410447,\n",
       "     0.979454851767484,\n",
       "     0.9664489760266982,\n",
       "     0.9223616267216161,\n",
       "     0.9185147008875905,\n",
       "     0.9128048001569774,\n",
       "     0.8728708750823916,\n",
       "     0.7300384267982151,\n",
       "     0.5699847638563678],\n",
       "    [0.5,\n",
       "     0.5000689973042555,\n",
       "     0.5022848780491894,\n",
       "     0.5278385898026907,\n",
       "     0.6232127735927319,\n",
       "     0.7056514243579761,\n",
       "     0.6765900573313021,\n",
       "     0.6352492896723514,\n",
       "     0.6198383104682978,\n",
       "     0.711618456117518,\n",
       "     0.8927921525185428,\n",
       "     0.9559192315235758,\n",
       "     0.9392881806916312,\n",
       "     0.8948602987605763,\n",
       "     0.9117921955552624,\n",
       "     0.9562331380635284,\n",
       "     0.9612344500461559,\n",
       "     0.9339824268257207,\n",
       "     0.9199676960604513,\n",
       "     0.9072583913704823,\n",
       "     0.8508246161854872,\n",
       "     0.7373279540172013,\n",
       "     0.6284025444797706],\n",
       "    [0.5000000170253195,\n",
       "     0.5000587648935032,\n",
       "     0.5019342143424244,\n",
       "     0.5235003370563724,\n",
       "     0.6039707047003698,\n",
       "     0.6693598974172706,\n",
       "     0.6158890706299368,\n",
       "     0.5485949328348002,\n",
       "     0.5622551163442037,\n",
       "     0.7283779634903035,\n",
       "     0.9131807718792503,\n",
       "     0.9588821254879812,\n",
       "     0.9495964390256745,\n",
       "     0.9396623982296376,\n",
       "     0.9638136337755555,\n",
       "     0.985765042775513,\n",
       "     0.991719402380188,\n",
       "     0.9836557387274641,\n",
       "     0.9669469791721381,\n",
       "     0.9506705300968877,\n",
       "     0.8797036943303721,\n",
       "     0.7871814622873893,\n",
       "     0.7497463493674389],\n",
       "    [0.5000005638011491,\n",
       "     0.500050033855893,\n",
       "     0.50126606163332,\n",
       "     0.5136694019601552,\n",
       "     0.5581413759641334,\n",
       "     0.5936224057548194,\n",
       "     0.5577630004475856,\n",
       "     0.5165497782744441,\n",
       "     0.5312620394598565,\n",
       "     0.6608500191773092,\n",
       "     0.8635188115173976,\n",
       "     0.9475305272737331,\n",
       "     0.9551569070769136,\n",
       "     0.9559727217959991,\n",
       "     0.9741598615728833,\n",
       "     0.9872684934430163,\n",
       "     0.9936695256745147,\n",
       "     0.9910496419501448,\n",
       "     0.979517543298388,\n",
       "     0.959946510858697,\n",
       "     0.885091217502501,\n",
       "     0.7904266354599724,\n",
       "     0.7896650913738579],\n",
       "    [0.500006868504093,\n",
       "     0.5002343391295913,\n",
       "     0.5029989337756223,\n",
       "     0.5151917072318629,\n",
       "     0.532877109184112,\n",
       "     0.5329032694604103,\n",
       "     0.5152600602666867,\n",
       "     0.5036306527424779,\n",
       "     0.5098613859709121,\n",
       "     0.5755512122938473,\n",
       "     0.7644809215556997,\n",
       "     0.9032370544399346,\n",
       "     0.914594409551769,\n",
       "     0.9000237196560512,\n",
       "     0.9280717829926567,\n",
       "     0.9564453660700939,\n",
       "     0.974173186455997,\n",
       "     0.9698264218118392,\n",
       "     0.9428080169037159,\n",
       "     0.898027656992361,\n",
       "     0.8104939881027411,\n",
       "     0.7040445085844171,\n",
       "     0.6793208545970375],\n",
       "    [0.5000307824996866,\n",
       "     0.5010199387772823,\n",
       "     0.5124346527652796,\n",
       "     0.5556519790577545,\n",
       "     0.5917302729211348,\n",
       "     0.5570867215601558,\n",
       "     0.5134358153846947,\n",
       "     0.501373151208046,\n",
       "     0.5034892503036118,\n",
       "     0.5391300651399689,\n",
       "     0.6725519608965392,\n",
       "     0.7962135147627466,\n",
       "     0.7736617935929485,\n",
       "     0.7117504600252903,\n",
       "     0.751159473048909,\n",
       "     0.8314777644849264,\n",
       "     0.8769867558987636,\n",
       "     0.8568204730377559,\n",
       "     0.8097615610391761,\n",
       "     0.7663276668586477,\n",
       "     0.6883432378587995,\n",
       "     0.5914762239032856,\n",
       "     0.549021842903501],\n",
       "    [0.5000507517618884,\n",
       "     0.5016806782338449,\n",
       "     0.5204638482707459,\n",
       "     0.5907514815004982,\n",
       "     0.6468630388274572,\n",
       "     0.5907939184218884,\n",
       "     0.5204940502694692,\n",
       "     0.501726888608593,\n",
       "     0.5013385630432958,\n",
       "     0.5157274648747469,\n",
       "     0.5717593615884948,\n",
       "     0.6253016779852841,\n",
       "     0.5956889587478144,\n",
       "     0.5612806646059767,\n",
       "     0.6123513393425389,\n",
       "     0.7022530159029261,\n",
       "     0.7498339942893824,\n",
       "     0.7624848760730968,\n",
       "     0.7668237495228614,\n",
       "     0.733618701253974,\n",
       "     0.6284131647946736,\n",
       "     0.5359092926193992,\n",
       "     0.5075447925734455],\n",
       "    [0.5000307824996866,\n",
       "     0.501019374978478,\n",
       "     0.5124159937654879,\n",
       "     0.5554273319066689,\n",
       "     0.5907448392512106,\n",
       "     0.5554273319066689,\n",
       "     0.5124159937654879,\n",
       "     0.5010268242778501,\n",
       "     0.5002780512458832,\n",
       "     0.5030244889076806,\n",
       "     0.5137007588038305,\n",
       "     0.523317356933347,\n",
       "     0.5171721459936521,\n",
       "     0.5261639834093019,\n",
       "     0.5985325567788983,\n",
       "     0.6813799175358243,\n",
       "     0.7149217394682017,\n",
       "     0.7631847593159595,\n",
       "     0.7731315702998427,\n",
       "     0.6956853140968432,\n",
       "     0.579151411054934,\n",
       "     0.515236407644995,\n",
       "     0.5014303076395317],\n",
       "    [0.500006868504093,\n",
       "     0.5002274536016468,\n",
       "     0.5027709239520755,\n",
       "     0.5124159937654879,\n",
       "     0.5204632854139808,\n",
       "     0.5124159937654879,\n",
       "     0.5027709239520755,\n",
       "     0.5002280344279948,\n",
       "     0.5000261198604043,\n",
       "     0.5002348859053005,\n",
       "     0.5010570428451925,\n",
       "     0.5017935574335242,\n",
       "     0.5021282873212715,\n",
       "     0.5127685594710041,\n",
       "     0.5568024881346192,\n",
       "     0.6047712558164706,\n",
       "     0.6231780655077598,\n",
       "     0.6581332478007745,\n",
       "     0.6558184074938551,\n",
       "     0.5877498738611028,\n",
       "     0.5258920534359659,\n",
       "     0.5038524153267641,\n",
       "     0.5002725539247217],\n",
       "    [0.5000005638011491,\n",
       "     0.5000186705298574,\n",
       "     0.5002274536016468,\n",
       "     0.501019374978478,\n",
       "     0.5016806612087178,\n",
       "     0.501019374978478,\n",
       "     0.5002274536016468,\n",
       "     0.5000186875551769,\n",
       "     0.5000011276022981,\n",
       "     0.500006868504093,\n",
       "     0.5000307824996866,\n",
       "     0.5000576202659011,\n",
       "     0.500258253119416,\n",
       "     0.5027852243199333,\n",
       "     0.5126575962000839,\n",
       "     0.5234871929335102,\n",
       "     0.5276492331879337,\n",
       "     0.5358607946971414,\n",
       "     0.5340945395565567,\n",
       "     0.5168780523017797,\n",
       "     0.5040485103987857,\n",
       "     0.5005130911218747,\n",
       "     0.500063753838582],\n",
       "    [0.5000000170253195,\n",
       "     0.5000005638011491,\n",
       "     0.500006868504093,\n",
       "     0.5000307824996866,\n",
       "     0.5000507517618884,\n",
       "     0.5000307824996866,\n",
       "     0.500006868504093,\n",
       "     0.5000005638011491,\n",
       "     0.5000000170253195,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5000005638011491,\n",
       "     0.5000186705298574,\n",
       "     0.5002280174026789,\n",
       "     0.5010386092280277,\n",
       "     0.5019267991730442,\n",
       "     0.5022667546706313,\n",
       "     0.5029348961443573,\n",
       "     0.5027681392066629,\n",
       "     0.5015255944543815,\n",
       "     0.5012968439308413,\n",
       "     0.5017254340542251,\n",
       "     0.501247972044258],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5000000170253195,\n",
       "     0.5000005638011491,\n",
       "     0.5000068855294125,\n",
       "     0.5000313633261528,\n",
       "     0.5000581840670425,\n",
       "     0.5000684335031171,\n",
       "     0.5000952712688221,\n",
       "     0.5003095516407244,\n",
       "     0.5028085908085532,\n",
       "     0.5124302857463956,\n",
       "     0.5206909336387815,\n",
       "     0.5151848450655782],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5000307824996866,\n",
       "     0.501019374978478,\n",
       "     0.5124159937654879,\n",
       "     0.5554577359202124,\n",
       "     0.5917302729211348,\n",
       "     0.5676570832026899],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5000000170253195,\n",
       "     0.5000005638011491,\n",
       "     0.500006868504093,\n",
       "     0.5000307824996866,\n",
       "     0.5000507517618884,\n",
       "     0.5000307824996866,\n",
       "     0.500006868504093,\n",
       "     0.5000513155630316,\n",
       "     0.5016806782338449,\n",
       "     0.5204632854139808,\n",
       "     0.5907939184218884,\n",
       "     0.6483691148219789,\n",
       "     0.6103881887759451],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5000005638011491,\n",
       "     0.5000186705298574,\n",
       "     0.5002274536016468,\n",
       "     0.501019374978478,\n",
       "     0.5016806612087178,\n",
       "     0.501019374978478,\n",
       "     0.5002274536016468,\n",
       "     0.5000494530294303,\n",
       "     0.5010199387772823,\n",
       "     0.5124159937654879,\n",
       "     0.5554577359202124,\n",
       "     0.5917302729211348,\n",
       "     0.5676570832026899],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.500006868504093,\n",
       "     0.5002274536016468,\n",
       "     0.5027709239520755,\n",
       "     0.5124159937654879,\n",
       "     0.5204632854139808,\n",
       "     0.5124159937654879,\n",
       "     0.5027709239520755,\n",
       "     0.5002343221042755,\n",
       "     0.5002343221042755,\n",
       "     0.5027709239520755,\n",
       "     0.5124228580319302,\n",
       "     0.5206903538069265,\n",
       "     0.5151848280559614],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.500030799525006,\n",
       "     0.5010199387772823,\n",
       "     0.5124228580319302,\n",
       "     0.5554577359202124,\n",
       "     0.5907939184218884,\n",
       "     0.5554577359202124,\n",
       "     0.5124228580319302,\n",
       "     0.5010205025760841,\n",
       "     0.5000494700547495,\n",
       "     0.5002274536016468,\n",
       "     0.5010199387772823,\n",
       "     0.501699331525283,\n",
       "     0.5012468274237646],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5000513155630316,\n",
       "     0.501699331525283,\n",
       "     0.5206903538069265,\n",
       "     0.5917302729211348,\n",
       "     0.6483691148219789,\n",
       "     0.5917302729211348,\n",
       "     0.5206903538069265,\n",
       "     0.5016993485504058,\n",
       "     0.5000518793641746,\n",
       "     0.500006868504093,\n",
       "     0.500030799525006,\n",
       "     0.5000513155630316,\n",
       "     0.5000376510037478],\n",
       "    [0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5000376510037478,\n",
       "     0.5012468274237646,\n",
       "     0.5151848280559614,\n",
       "     0.5676570832026899,\n",
       "     0.6103881887759451,\n",
       "     0.5676570832026899,\n",
       "     0.5151848280559614,\n",
       "     0.5012468274237646,\n",
       "     0.5000376510037478,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5]])])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_test_instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for test instances, to compute MMR and top-k metrics\n",
    "latent_vectors = {}\n",
    "for playerId, teamId, season, heatmaps in filtered_test_instances:\n",
    "    vectors_with_ids = []\n",
    "    for heatmaps_id, heatmap in heatmaps:\n",
    "        # Convert heatmap to tensor and move to the device\n",
    "        hm_tensor = torch.tensor(heatmap, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        bottleneck, _ = autoencoder.encoder(hm_tensor)  # Get the bottleneck representation, ignore the indices_list\n",
    "        vector = bottleneck.detach().cpu().numpy()  # Detach and convert to NumPy array\n",
    "        # Append tuple of heatmapsId and latent vector\n",
    "        vectors_with_ids.append((heatmaps_id, vector))\n",
    "    latent_vectors[(playerId, teamId, season)] = vectors_with_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique (playerId, teamId, season) combinations: 100\n",
      "Total number of latent vectors: 213\n"
     ]
    }
   ],
   "source": [
    "num_keys = len(latent_vectors)\n",
    "print(f\"Number of unique (playerId, teamId, season) combinations: {num_keys}\")\n",
    "total_latent_vectors = sum(len(vectors) for vectors in latent_vectors.values())\n",
    "print(f\"Total number of latent vectors: {total_latent_vectors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to leverage GPU\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "k_values = [1, 3, 5, 10]  # List of k values\n",
    "\n",
    "# Loop through each query vector\n",
    "for (playerId, teamId, season), vectors_with_ids in latent_vectors_torch.items():\n",
    "    for i, (heatmaps_id, query_vector) in enumerate(vectors_with_ids):\n",
    "        similarities = []  # Store similarities for the current query vector\n",
    "\n",
    "        # Compare with all other vectors in the dataset\n",
    "        for (other_playerId, other_teamId, other_season), other_vectors_with_ids in latent_vectors_torch.items():\n",
    "            for other_heatmaps_id, other_vector in other_vectors_with_ids:\n",
    "                if other_heatmaps_id == heatmaps_id:  # Skip self-comparison\n",
    "                    continue\n",
    "                \n",
    "                # Compute cosine similarity\n",
    "                sim = torch.nn.functional.cosine_similarity(\n",
    "                    query_vector.flatten().unsqueeze(0),\n",
    "                    other_vector.flatten().unsqueeze(0)\n",
    "                ).item()\n",
    "                similarities.append((sim, (other_playerId, other_teamId, other_season)))\n",
    "\n",
    "        # Sort by similarity in descending order\n",
    "        similarities.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "        # Evaluate rankings for each k\n",
    "        ranks = {k: [key == (playerId, teamId, season) for _, key in similarities[:k]] for k in k_values}\n",
    "\n",
    "        # Calculate top-k accuracy for each k value\n",
    "        top_k_values = {k: 1 if any(ranks[k]) else 0 for k in k_values}\n",
    "\n",
    "        # Calculate MRR\n",
    "        mrr = 1 / (ranks[10].index(True) + 1) if any(ranks[10]) else 0  # MRR for k=10\n",
    "\n",
    "        # Store the result for the current query\n",
    "        results.append({\n",
    "            'query': (playerId, teamId, season),\n",
    "            'mrr': mrr,\n",
    "            'top_k_values': top_k_values,  # Dictionary with top-k accuracy for each k\n",
    "        })\n",
    "\n",
    "# Calculate averages for each k\n",
    "avg_mrr = np.mean([result['mrr'] for result in results])\n",
    "avg_top_k_values = {k: np.mean([result['top_k_values'][k] for result in results]) for k in k_values}\n",
    "\n",
    "# Output results\n",
    "print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "for k, avg_top_k in avg_top_k_values.items():\n",
    "    print(f\"Average Top-{k} Accuracy: {avg_top_k:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MRR: 0.2524\n",
      "Average Top-1 Accuracy: 0.1362\n",
      "Average Top-3 Accuracy: 0.2911\n",
      "Average Top-5 Accuracy: 0.4178\n",
      "Average Top-10 Accuracy: 0.5822\n"
     ]
    }
   ],
   "source": [
    "# CPU based similarity computation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "results = []\n",
    "k_values = [1, 3, 5, 10]  # List of k values\n",
    "\n",
    "for (playerId, teamId, season), vectors_with_ids in latent_vectors.items():\n",
    "    # Loop over each query vector for the current (playerId, teamId, season)\n",
    "    for i, (heatmaps_id, query_vector) in enumerate(vectors_with_ids):\n",
    "        # Compute similarity between the query vector and all other vectors\n",
    "        similarities = []\n",
    "        for (other_playerId, other_teamId, other_season), other_vectors_with_ids in latent_vectors.items():\n",
    "            for other_heatmaps_id, other_vector in other_vectors_with_ids:\n",
    "                # Skip comparison if the heatmap is the same (i.e., avoid comparing the vector with itself)\n",
    "                if other_heatmaps_id == heatmaps_id:\n",
    "                    continue\n",
    "                # Compute cosine similarity between query vector and other vector\n",
    "                sim = cosine_similarity(query_vector.reshape(1, -1), other_vector.reshape(1, -1))[0][0]\n",
    "                similarities.append((sim, (other_playerId, other_teamId, other_season)))\n",
    "        \n",
    "        # Sort by similarity in descending order\n",
    "        similarities.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "        # Evaluate rankings for each k\n",
    "        ranks = {k: [key == (playerId, teamId, season) for _, key in similarities[:k]] for k in k_values}\n",
    "        \n",
    "        # Calculate top-k accuracy for each k value\n",
    "        top_k_values = {k: 1 if any(ranks[k]) else 0 for k in k_values}\n",
    "\n",
    "        # Calculate MRR\n",
    "        mrr = 1 / (ranks[10].index(True) + 1) if any(ranks[10]) else 0  # MRR for k=10 (you can adjust this as needed)\n",
    "\n",
    "        results.append({\n",
    "            'query': (playerId, teamId, season),\n",
    "            'mrr': mrr,\n",
    "            'top_k_values': top_k_values,  # Dictionary with top-k accuracy for each k\n",
    "        })\n",
    "\n",
    "# Calculate averages for each k\n",
    "avg_mrr = np.mean([result['mrr'] for result in results])\n",
    "avg_top_k_values = {k: np.mean([result['top_k_values'][k] for result in results]) for k in k_values}\n",
    "\n",
    "print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "for k, avg_top_k in avg_top_k_values.items():\n",
    "    print(f\"Average Top-{k} Accuracy: {avg_top_k:.4f}\")\n",
    "\n",
    "# In the comparison with Matteo's results, from top-3 upwards I should expect higher scores since in my method I have more than 1 correct instance retrievable for some players (the ones with more than 2 heatmaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_set = HeatmapCSVLoader(csv_path=\"_Data-split/passes-start-test.csv\")\n",
    "test_loader = DataLoader(test_set, batch_size=16, shuffle=False)\n",
    "\n",
    "# Get a batch of images\n",
    "autoencoder.eval()\n",
    "images, ids, playerIds, teamIds, seasons = next(iter(test_loader))\n",
    "images = images.to(device)  # Move images to the selected device\n",
    "\n",
    "# Reconstruct images\n",
    "with torch.no_grad():\n",
    "    reconstructed = autoencoder(images)\n",
    "\n",
    "# Plot original and reconstructed images (limit to 5 for better visualization)\n",
    "num_examples = min(len(images), 5)  # Display up to 5 examples\n",
    "fig, axes = plt.subplots(2, num_examples, figsize=(15, 5))\n",
    "for i in range(num_examples):\n",
    "    # Original\n",
    "    axes[0, i].imshow(images[i].cpu().squeeze(), cmap='gray')  # Move back to CPU for plotting\n",
    "    if isinstance(ids, (list, tuple)):\n",
    "        axes[0, i].set_title(f\"Original ({ids[i]})\")\n",
    "    else:\n",
    "        axes[0, i].set_title(\"Original\")\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Reconstructed\n",
    "    axes[1, i].imshow(reconstructed[i].cpu().squeeze(), cmap='gray')  # Move back to CPU for plotting\n",
    "    axes[1, i].set_title(\"Reconstructed\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder class for grid search\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels, bottleneck_size, dropout_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Conv Block 1\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_rate)\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "        \n",
    "        # Conv Block 2\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, groups=64),\n",
    "            nn.Conv2d(128, 128, kernel_size=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, groups=128),\n",
    "            nn.Conv2d(128, 128, kernel_size=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_rate)\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "        \n",
    "        # Conv Block 3\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout_rate)\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "        \n",
    "        # Compute flattened size after convolutions and pooling\n",
    "        self.flattened_size = 256 * 4 * 2  # Computed from input size [1, 35, 23]\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, bottleneck_size),\n",
    "            nn.Dropout(p=dropout_rate)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv Block 1\n",
    "        x = self.conv_block1(x)\n",
    "        x, indices1 = self.pool1(x)\n",
    "        \n",
    "        # Conv Block 2\n",
    "        x = self.conv_block2(x)\n",
    "        x, indices2 = self.pool2(x)\n",
    "        \n",
    "        # Conv Block 3\n",
    "        x = self.conv_block3(x)\n",
    "        x, indices3 = self.pool3(x)\n",
    "\n",
    "        # Flatten and bottleneck\n",
    "        x = self.flatten(x)\n",
    "        bottleneck = self.dense(x)\n",
    "        \n",
    "        return bottleneck, [indices1, indices2, indices3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder class for grid search\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_channels, bottleneck_size, dropout_rate):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(bottleneck_size, 256 * 4 * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(256, 4, 2))\n",
    "\n",
    "        self.unpool3 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        self.deconv_block3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        self.deconv_block2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.unpool1 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        self.deconv_block1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, input_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(input_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, indices_list, output_sizes):\n",
    "        # Dense layer and unflatten\n",
    "        x = self.dense(x)\n",
    "        x = self.unflatten(x)\n",
    "\n",
    "        # Unpooling and deconvolution\n",
    "        x = self.unpool3(x, indices_list[2], output_size=output_sizes[2])\n",
    "        x = self.deconv_block3(x)\n",
    "\n",
    "        x = self.unpool2(x, indices_list[1], output_size=output_sizes[1])\n",
    "        x = self.deconv_block2(x)\n",
    "\n",
    "        x = self.unpool1(x, indices_list[0], output_size=output_sizes[0])\n",
    "        x = self.deconv_block1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder with ReLU also in deconvolution layer of block1, to match the convolution layer of the Encoder\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_channels, bottleneck_size, dropout_rate):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(bottleneck_size, 256 * 4 * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(256, 4, 2))\n",
    "\n",
    "        self.unpool3 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        self.deconv_block3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        self.deconv_block2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.unpool1 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        self.deconv_block1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, input_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(input_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, indices_list, output_sizes):\n",
    "        # Dense layer and unflatten\n",
    "        x = self.dense(x)\n",
    "        x = self.unflatten(x)\n",
    "\n",
    "        # Unpooling and deconvolution\n",
    "        x = self.unpool3(x, indices_list[2], output_size=output_sizes[2])\n",
    "        x = self.deconv_block3(x)\n",
    "\n",
    "        x = self.unpool2(x, indices_list[1], output_size=output_sizes[1])\n",
    "        x = self.deconv_block2(x)\n",
    "\n",
    "        x = self.unpool1(x, indices_list[0], output_size=output_sizes[0])\n",
    "        x = self.deconv_block1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder class for grid search\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_channels, bottleneck_size, dropout_rate):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(input_channels, bottleneck_size, dropout_rate)\n",
    "        self.decoder = Decoder(input_channels, bottleneck_size, dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        bottleneck, indices_list = self.encoder(x)\n",
    "\n",
    "        # Verify output_sizes match shapes before pooling\n",
    "        output_sizes = [\n",
    "            torch.Size([x.size(0), 1, 35, 23]),    # Before Pool1\n",
    "            torch.Size([x.size(0), 64, 17, 11]),  # Before Pool2\n",
    "            torch.Size([x.size(0), 128, 8, 5])    # Before Pool3\n",
    "        ]\n",
    "\n",
    "        # Decoder\n",
    "        reconstructed = self.decoder(bottleneck, indices_list, output_sizes)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define EarlyStopping class\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "\n",
    "learning_rates = [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\n",
    "bottleneck_sizes = [16, 32, 64]\n",
    "optimizers = ['Adam', 'AdamW', 'SGD']\n",
    "loss_functions = ['MSELoss'] #'SmoothL1Loss'\n",
    "batch_sizes = [16, 32, 64]\n",
    "weight_decays = [0, 1e-4, 1e-3]\n",
    "dropout_rates = [0.0, 0.1, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with one parameter per type\n",
    "\n",
    "learning_rates = [1e-4]\n",
    "bottleneck_sizes = [64]\n",
    "optimizers = ['Adam']\n",
    "loss_functions = ['MSELoss']\n",
    "batch_sizes = [64]\n",
    "weight_decays = [0.2]\n",
    "dropout_rates = [0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get optimizer\n",
    "\n",
    "def get_optimizer(optimizer_name, model_params, lr, weight_decay):\n",
    "    if optimizer_name == 'Adam':\n",
    "        return optim.Adam(model_params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'AdamW':\n",
    "        return optim.AdamW(model_params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        return optim.SGD(model_params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get loss function\n",
    "\n",
    "def get_loss_function(loss_name):\n",
    "    if loss_name == 'MSELoss':\n",
    "        return nn.MSELoss()\n",
    "    elif loss_name == 'SmoothL1Loss':\n",
    "        return nn.SmoothL1Loss()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss function: {loss_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "results = []\n",
    "\n",
    "# Function to add a result\n",
    "def save_result(hyperparams, metrics):\n",
    "    result = {\n",
    "        \"hyperparameters\": hyperparams,\n",
    "        \"metrics\": metrics,\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = HeatmapCSVLoader(csv_path=\"_Data-split/passes-start-train.csv\")\n",
    "val_set = HeatmapCSVLoader(csv_path=\"_Data-split/passes-start-validation.csv\")\n",
    "#test_set = HeatmapCSVLoader(csv_path=\"_Data-split/passes-start-test.csv\")\n",
    "\n",
    "# kaggle path\n",
    "#train_set = HeatmapCSVLoader(csv_path=\"/kaggle/input/data-test-embedder/train.csv\")\n",
    "#val_set = HeatmapCSVLoader(csv_path=\"/kaggle/input/data-test-embedder/validation.csv\")\n",
    "#test_set = HeatmapCSVLoader(csv_path=\"/kaggle/input/data-test-embedder/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m      3\u001b[0m images, ids, playerIds, teamIds, seasons \u001b[38;5;241m=\u001b[39m val_set[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Unpack images and metadata\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[0;32m      5\u001b[0m input_max \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m      6\u001b[0m input_min \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "val_loader = DataLoader(val_set, batch_size=16)\n",
    "\n",
    "images, ids, playerIds, teamIds, seasons = val_set[0]  # Unpack images and metadata\n",
    "images = images.to(device)\n",
    "input_max = images.max().item()\n",
    "input_min = images.min().item()\n",
    "print(\"Original Input range:\", images[0].min(), images[0].max())  # Debug input range\n",
    "print(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with parameters: \n",
      "Learning Rate: 0.0001, Bottleneck: 64, Optimizer: Adam, Loss Function: MSELoss,\n",
      " Batch Size: 64, Weight Decay: 0.2, Dropout Rate: 0.3\n",
      "Original Input range: 0.0 50.853153228759766\n",
      "Output range: 2.1026171026505835e-08 0.9999995231628418\n",
      "Batch loss: 6.622037410736084\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 4.2807285494106395e-10 1.0\n",
      "Batch loss: 6.870143890380859\n",
      "Original Input range: 0.0 38.09182357788086\n",
      "Output range: 1.9184746633982286e-05 0.9999996423721313\n",
      "Batch loss: 6.178079128265381\n",
      "Original Input range: 0.0 42.41950607299805\n",
      "Output range: 5.19250420438766e-09 0.9999988079071045\n",
      "Batch loss: 5.503143310546875\n",
      "Original Input range: 0.0 50.76498031616211\n",
      "Output range: 2.200146287378857e-09 0.9999958276748657\n",
      "Batch loss: 5.267609119415283\n",
      "Original Input range: 0.0 40.24433517456055\n",
      "Output range: 5.334690285963006e-06 0.9999995231628418\n",
      "Batch loss: 6.106750965118408\n",
      "Original Input range: 0.0 127.84440612792969\n",
      "Output range: 3.7712952760671214e-09 1.0\n",
      "Batch loss: 7.084789276123047\n",
      "Original Input range: 0.0 64.54231262207031\n",
      "Output range: 1.4799053133174311e-05 0.9999892711639404\n",
      "Batch loss: 7.439558506011963\n",
      "Original Input range: 0.0 37.03127670288086\n",
      "Output range: 3.0361684366653208e-06 0.9999998807907104\n",
      "Batch loss: 5.081667423248291\n",
      "Original Input range: 0.0 80.46197509765625\n",
      "Output range: 8.769305837041941e-10 0.9999998807907104\n",
      "Batch loss: 6.01688289642334\n",
      "Original Input range: 0.0 85.22960662841797\n",
      "Output range: 1.172825747630668e-08 0.9999996423721313\n",
      "Batch loss: 6.52730131149292\n",
      "Original Input range: 0.0 51.19465637207031\n",
      "Output range: 2.31808962780633e-06 0.9999988079071045\n",
      "Batch loss: 6.100270748138428\n",
      "Original Input range: 0.0 102.27552795410156\n",
      "Output range: 1.0914915193893648e-08 0.9999998807907104\n",
      "Batch loss: 6.827132701873779\n",
      "Original Input range: 0.0 82.15438842773438\n",
      "Output range: 1.7808069255331738e-11 1.0\n",
      "Batch loss: 6.417174339294434\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 1.452577946814415e-09 1.0\n",
      "Batch loss: 6.348248481750488\n",
      "Original Input range: 0.0 32.91852569580078\n",
      "Output range: 6.045976988389157e-06 0.9999979734420776\n",
      "Batch loss: 5.225132465362549\n",
      "Original Input range: 0.0 40.142005920410156\n",
      "Output range: 3.233744064345956e-05 0.9999994039535522\n",
      "Batch loss: 6.368407726287842\n",
      "Original Input range: 0.0 79.60464477539062\n",
      "Output range: 8.561952199670486e-06 0.9999969005584717\n",
      "Batch loss: 7.785269260406494\n",
      "Original Input range: 0.0 59.92755126953125\n",
      "Output range: 8.952715688792523e-06 0.9999936819076538\n",
      "Batch loss: 5.782020568847656\n",
      "Original Input range: 0.0 54.426517486572266\n",
      "Output range: 5.691731075785356e-06 0.9999973773956299\n",
      "Batch loss: 7.331172466278076\n",
      "Original Input range: 0.0 36.3109130859375\n",
      "Output range: 1.684065296103654e-06 1.0\n",
      "Batch loss: 5.437315464019775\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 1.0977874165263302e-10 1.0\n",
      "Batch loss: 6.670790672302246\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 2.8368759785735165e-08 1.0\n",
      "Batch loss: 5.9191389083862305\n",
      "Original Input range: 0.0 53.051963806152344\n",
      "Output range: 1.2674273852653073e-09 1.0\n",
      "Batch loss: 5.643782138824463\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 1.4565394246615626e-13 1.0\n",
      "Batch loss: 5.93377161026001\n",
      "Original Input range: 0.0 80.46197509765625\n",
      "Output range: 8.434039244775704e-09 1.0\n",
      "Batch loss: 6.7307538986206055\n",
      "Original Input range: 0.0 46.95805358886719\n",
      "Output range: 7.745903474187799e-09 0.9999996423721313\n",
      "Batch loss: 5.7889485359191895\n",
      "Original Input range: 0.0 47.55305099487305\n",
      "Output range: 5.310237156663788e-07 1.0\n",
      "Batch loss: 6.722589015960693\n",
      "Original Input range: 0.0 39.56005096435547\n",
      "Output range: 1.6378704458475113e-05 0.9999914169311523\n",
      "Batch loss: 5.9043731689453125\n",
      "Original Input range: 0.0 85.22960662841797\n",
      "Output range: 5.465801677928539e-07 0.9999958276748657\n",
      "Batch loss: 6.808475971221924\n",
      "Original Input range: 0.0 43.45826721191406\n",
      "Output range: 7.3408173193456605e-06 0.9999996423721313\n",
      "Batch loss: 5.335958957672119\n",
      "Original Input range: 0.0 58.06863784790039\n",
      "Output range: 1.2571673323691357e-05 0.9999933242797852\n",
      "Batch loss: 6.2360358238220215\n",
      "Original Input range: 0.0 53.99897766113281\n",
      "Output range: 1.0634602176651242e-06 0.9999978542327881\n",
      "Batch loss: 6.167534351348877\n",
      "Original Input range: 0.0 40.40686798095703\n",
      "Output range: 6.788573955418542e-06 0.9999961853027344\n",
      "Batch loss: 5.065954208374023\n",
      "Original Input range: 0.0 57.596832275390625\n",
      "Output range: 2.6722136681200936e-06 1.0\n",
      "Batch loss: 5.709940433502197\n",
      "Original Input range: 0.0 159.15589904785156\n",
      "Output range: 3.512172597197605e-08 0.9999984502792358\n",
      "Batch loss: 6.949153900146484\n",
      "Original Input range: 0.0 49.05189514160156\n",
      "Output range: 2.2846988940727897e-06 0.9999991655349731\n",
      "Batch loss: 6.047111988067627\n",
      "Original Input range: 0.0 40.91291427612305\n",
      "Output range: 1.6998200180751155e-06 0.9999970197677612\n",
      "Batch loss: 5.023475170135498\n",
      "Original Input range: 0.0 59.37202835083008\n",
      "Output range: 1.3213482816354372e-05 0.9999885559082031\n",
      "Batch loss: 6.336555004119873\n",
      "Original Input range: 0.0 54.94767379760742\n",
      "Output range: 8.902006243260985e-07 0.9999963045120239\n",
      "Batch loss: 6.029998779296875\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 2.4037282855715603e-05 0.9999697208404541\n",
      "Batch loss: 7.31316614151001\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 1.918482121254783e-05 0.9999793767929077\n",
      "Batch loss: 8.965962409973145\n",
      "Original Input range: 0.0 86.11009979248047\n",
      "Output range: 3.3620668546063825e-05 0.999940037727356\n",
      "Batch loss: 6.463407516479492\n",
      "Original Input range: 0.0 56.19416427612305\n",
      "Output range: 2.936601731562405e-06 0.9999969005584717\n",
      "Batch loss: 5.746169090270996\n",
      "Original Input range: 0.0 37.5518913269043\n",
      "Output range: 0.00016865516954567283 0.9998346567153931\n",
      "Batch loss: 5.750894069671631\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 4.290279775887029e-08 0.9999995231628418\n",
      "Batch loss: 6.721378803253174\n",
      "Original Input range: 0.0 57.333343505859375\n",
      "Output range: 0.0001466611574869603 0.9998088479042053\n",
      "Batch loss: 6.96150541305542\n",
      "Original Input range: 0.0 47.97340393066406\n",
      "Output range: 0.00013362588651943952 0.99996018409729\n",
      "Batch loss: 6.445680618286133\n",
      "Original Input range: 0.0 32.97996520996094\n",
      "Output range: 0.0001321254821959883 0.9999369382858276\n",
      "Batch loss: 6.135671615600586\n",
      "Original Input range: 0.0 53.641319274902344\n",
      "Output range: 0.00021195143926888704 0.999945878982544\n",
      "Batch loss: 5.380978107452393\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 5.2308396334410645e-06 0.9999717473983765\n",
      "Batch loss: 6.1641340255737305\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 2.339592297673221e-09 1.0\n",
      "Batch loss: 6.762720108032227\n",
      "Original Input range: 0.0 85.24281311035156\n",
      "Output range: 4.2330000837864645e-07 1.0\n",
      "Batch loss: 5.400787353515625\n",
      "Original Input range: 0.0 46.85770034790039\n",
      "Output range: 0.0008855166961438954 0.9998857975006104\n",
      "Batch loss: 4.924431800842285\n",
      "Original Input range: 0.0 51.152488708496094\n",
      "Output range: 0.00011314684525132179 0.9999731779098511\n",
      "Batch loss: 5.459636211395264\n",
      "Original Input range: 0.0 48.69768524169922\n",
      "Output range: 0.0011065343860536814 0.9997392296791077\n",
      "Batch loss: 4.899820327758789\n",
      "Original Input range: 0.0 63.922203063964844\n",
      "Output range: 7.288859342224896e-05 0.9999936819076538\n",
      "Batch loss: 5.8989033699035645\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.0004480773932300508 0.9999957084655762\n",
      "Batch loss: 8.233487129211426\n",
      "Original Input range: 0.0 77.83159637451172\n",
      "Output range: 0.00033054040977731347 0.9998655319213867\n",
      "Batch loss: 5.472743034362793\n",
      "Original Input range: 0.0 48.32259750366211\n",
      "Output range: 0.0005977588007226586 0.9999479055404663\n",
      "Batch loss: 4.9571428298950195\n",
      "Original Input range: 0.0 32.866878509521484\n",
      "Output range: 0.0047210888005793095 0.9993768334388733\n",
      "Batch loss: 5.051587104797363\n",
      "Original Input range: 0.0 31.91970443725586\n",
      "Output range: 0.016122983768582344 0.99979168176651\n",
      "Batch loss: 5.1229400634765625\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.0004918033955618739 0.999998927116394\n",
      "Batch loss: 6.494756698608398\n",
      "Original Input range: 0.0 159.15589904785156\n",
      "Output range: 0.00021713638852816075 0.9999732971191406\n",
      "Batch loss: 6.63926887512207\n",
      "Original Input range: 0.0 46.75095748901367\n",
      "Output range: 0.008792081847786903 0.9997574687004089\n",
      "Batch loss: 5.970858573913574\n",
      "Original Input range: 0.0 64.35528564453125\n",
      "Output range: 0.006119645666331053 0.9995007514953613\n",
      "Batch loss: 7.355947971343994\n",
      "Original Input range: 0.0 33.56317901611328\n",
      "Output range: 0.005695499014109373 0.9999402761459351\n",
      "Batch loss: 5.189576625823975\n",
      "Original Input range: 0.0 43.624542236328125\n",
      "Output range: 0.00792734045535326 0.9999419450759888\n",
      "Batch loss: 4.7170305252075195\n",
      "Original Input range: 0.0 83.86809539794922\n",
      "Output range: 0.0005979599663987756 0.9999970197677612\n",
      "Batch loss: 4.512932777404785\n",
      "Original Input range: 0.0 53.052162170410156\n",
      "Output range: 0.005808521062135696 0.9997344613075256\n",
      "Batch loss: 6.083765983581543\n",
      "Original Input range: 0.0 55.424163818359375\n",
      "Output range: 0.0015647360123693943 0.9997283816337585\n",
      "Batch loss: 5.308857440948486\n",
      "Validation Input range: 0.0 39.0533447265625\n",
      "Validation Output range: 0.07891354709863663 0.9950887560844421\n",
      "Validation Input range: 0.0 47.14912414550781\n",
      "Validation Output range: 0.0756719782948494 0.9946099519729614\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.02311304211616516 0.9999651908874512\n",
      "Validation Input range: 0.0 41.433265686035156\n",
      "Validation Output range: 0.09289627522230148 0.9917991161346436\n",
      "Validation Input range: 0.0 54.87662887573242\n",
      "Validation Output range: 0.06963905692100525 0.9922783374786377\n",
      "Validation Input range: 0.0 41.19639587402344\n",
      "Validation Output range: 0.09093638509511948 0.9934384226799011\n",
      "Validation Input range: 0.0 40.4886589050293\n",
      "Validation Output range: 0.10192622244358063 0.9880062937736511\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.02464200370013714 0.9987667798995972\n",
      "Validation Input range: 0.0 79.57794952392578\n",
      "Validation Output range: 0.09073378145694733 0.9909676909446716\n",
      "Validation Input range: 0.0 37.079376220703125\n",
      "Validation Output range: 0.09977860003709793 0.9939513206481934\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.0018336226930841804 0.9999998807907104\n",
      "Validation Input range: 0.0 33.93391418457031\n",
      "Validation Output range: 0.09195999056100845 0.9949306845664978\n",
      "Validation Input range: 0.0 32.81044006347656\n",
      "Validation Output range: 0.09884054213762283 0.988241970539093\n",
      "Validation Input range: 0.0 49.359073638916016\n",
      "Validation Output range: 0.08430972695350647 0.9946670532226562\n",
      "Validation Input range: 0.0 39.49129867553711\n",
      "Validation Output range: 0.0894264504313469 0.9911866784095764\n",
      "Validation Input range: 0.0 43.210540771484375\n",
      "Validation Output range: 0.09203004837036133 0.9916325211524963\n",
      "Validation Input range: 0.0 32.88222122192383\n",
      "Validation Output range: 0.09521737694740295 0.9927603006362915\n",
      "Validation Input range: 0.0 39.19929885864258\n",
      "Validation Output range: 0.09838279336690903 0.9879211187362671\n",
      "Validation Input range: 0.0 52.02884292602539\n",
      "Validation Output range: 0.07962598651647568 0.9903351068496704\n",
      "Validation Input range: 0.0 42.88813781738281\n",
      "Validation Output range: 0.054252151399850845 0.9941272735595703\n",
      "Validation Input range: 0.0 30.61672592163086\n",
      "Validation Output range: 0.10060326009988785 0.9809710383415222\n",
      "Epoch [1/10], Average Batch Training Loss: 6.110571726946763, Average Batch Validation Loss: 5.490835348765056\n",
      "Original Input range: 0.0 127.84440612792969\n",
      "Output range: 0.012092516757547855 0.9995672106742859\n",
      "Batch loss: 6.629919052124023\n",
      "Original Input range: 0.0 85.24281311035156\n",
      "Output range: 0.0007560058729723096 0.999890923500061\n",
      "Batch loss: 5.60975456237793\n",
      "Original Input range: 0.0 48.32259750366211\n",
      "Output range: 0.010047473013401031 0.9988752007484436\n",
      "Batch loss: 4.3521409034729\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.002607457572594285 0.999988317489624\n",
      "Batch loss: 7.821663856506348\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.0009960252791643143 0.9997441172599792\n",
      "Batch loss: 6.933758735656738\n",
      "Original Input range: 0.0 79.5941390991211\n",
      "Output range: 0.00013466710515785962 0.9999767541885376\n",
      "Batch loss: 6.07700777053833\n",
      "Original Input range: 0.0 64.35528564453125\n",
      "Output range: 0.0008376772166229784 0.999128520488739\n",
      "Batch loss: 5.6438398361206055\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.0011284744832664728 0.9986510872840881\n",
      "Batch loss: 5.946715354919434\n",
      "Original Input range: 0.0 58.06863784790039\n",
      "Output range: 0.014581386931240559 0.9973053932189941\n",
      "Batch loss: 6.1151275634765625\n",
      "Original Input range: 0.0 40.91291427612305\n",
      "Output range: 0.015159647911787033 0.9998020529747009\n",
      "Batch loss: 5.222926616668701\n",
      "Original Input range: 0.0 45.650856018066406\n",
      "Output range: 0.022068336606025696 0.9994101524353027\n",
      "Batch loss: 5.08452033996582\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.0028918941970914602 0.9983158111572266\n",
      "Batch loss: 6.730432510375977\n",
      "Original Input range: 0.0 51.13776397705078\n",
      "Output range: 0.0237246286123991 0.9991539716720581\n",
      "Batch loss: 5.3323893547058105\n",
      "Original Input range: 0.0 54.426517486572266\n",
      "Output range: 0.0041315583512187 0.9997807145118713\n",
      "Batch loss: 5.520050525665283\n",
      "Original Input range: 0.0 26.36052131652832\n",
      "Output range: 0.04179535061120987 0.9968681931495667\n",
      "Batch loss: 4.271912097930908\n",
      "Original Input range: 0.0 51.152488708496094\n",
      "Output range: 0.009701867587864399 0.9995192289352417\n",
      "Batch loss: 4.743771076202393\n",
      "Original Input range: 0.0 80.46197509765625\n",
      "Output range: 0.005841600243002176 0.999333918094635\n",
      "Batch loss: 5.236849308013916\n",
      "Original Input range: 0.0 63.922203063964844\n",
      "Output range: 0.029663721099495888 0.9990109205245972\n",
      "Batch loss: 6.254450798034668\n",
      "Original Input range: 0.0 39.49016189575195\n",
      "Output range: 0.03376688435673714 0.9991497993469238\n",
      "Batch loss: 5.5107316970825195\n",
      "Original Input range: 0.0 63.922203063964844\n",
      "Output range: 0.035576123744249344 0.9992920160293579\n",
      "Batch loss: 5.524364471435547\n",
      "Original Input range: 0.0 43.624542236328125\n",
      "Output range: 0.05058946833014488 0.995708703994751\n",
      "Batch loss: 5.315385818481445\n",
      "Original Input range: 0.0 36.687992095947266\n",
      "Output range: 0.031142039224505424 0.998506486415863\n",
      "Batch loss: 5.223869323730469\n",
      "Original Input range: 0.0 86.11009979248047\n",
      "Output range: 0.023445576429367065 0.9984448552131653\n",
      "Batch loss: 6.933194637298584\n",
      "Original Input range: 0.0 82.15438842773438\n",
      "Output range: 0.007572756614536047 0.9960124492645264\n",
      "Batch loss: 4.842777729034424\n",
      "Original Input range: 0.0 38.72101593017578\n",
      "Output range: 0.027154741808772087 0.9981613755226135\n",
      "Batch loss: 5.598947048187256\n",
      "Original Input range: 0.0 53.99897766113281\n",
      "Output range: 0.023716505616903305 0.9988110065460205\n",
      "Batch loss: 6.592459201812744\n",
      "Original Input range: 0.0 50.34861373901367\n",
      "Output range: 0.023126842454075813 0.9978811144828796\n",
      "Batch loss: 6.64506721496582\n",
      "Original Input range: 0.0 48.69768524169922\n",
      "Output range: 0.04662369564175606 0.9995644688606262\n",
      "Batch loss: 5.637019634246826\n",
      "Original Input range: 0.0 80.46197509765625\n",
      "Output range: 0.010532067157328129 0.9989070892333984\n",
      "Batch loss: 6.543361186981201\n",
      "Original Input range: 0.0 64.54231262207031\n",
      "Output range: 0.023684144020080566 0.9981560111045837\n",
      "Batch loss: 6.373302459716797\n",
      "Original Input range: 0.0 50.853153228759766\n",
      "Output range: 0.01885381154716015 0.999101996421814\n",
      "Batch loss: 6.136645793914795\n",
      "Original Input range: 0.0 38.68598175048828\n",
      "Output range: 0.01353495568037033 0.9985130429267883\n",
      "Batch loss: 5.501698970794678\n",
      "Original Input range: 0.0 42.724082946777344\n",
      "Output range: 0.013540305197238922 0.9990748167037964\n",
      "Batch loss: 5.729650974273682\n",
      "Original Input range: 0.0 159.15589904785156\n",
      "Output range: 0.0003003341844305396 0.9998175501823425\n",
      "Batch loss: 6.747075080871582\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.02160487323999405 0.9990561604499817\n",
      "Batch loss: 9.250955581665039\n",
      "Original Input range: 0.0 34.16322708129883\n",
      "Output range: 0.026000013574957848 0.9997037053108215\n",
      "Batch loss: 4.625710487365723\n",
      "Original Input range: 0.0 47.55305099487305\n",
      "Output range: 0.019295448437333107 0.9997913241386414\n",
      "Batch loss: 5.328430652618408\n",
      "Original Input range: 0.0 85.22960662841797\n",
      "Output range: 0.011022507213056087 0.9991037249565125\n",
      "Batch loss: 5.603987693786621\n",
      "Original Input range: 0.0 30.854175567626953\n",
      "Output range: 0.04156375303864479 0.9982275366783142\n",
      "Batch loss: 4.329290390014648\n",
      "Original Input range: 0.0 159.15589904785156\n",
      "Output range: 0.0012595466105267406 0.9999909400939941\n",
      "Batch loss: 6.064362049102783\n",
      "Original Input range: 0.0 45.98609924316406\n",
      "Output range: 0.022515129297971725 0.9972370862960815\n",
      "Batch loss: 5.2989325523376465\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.007973795756697655 0.9983696341514587\n",
      "Batch loss: 6.091623306274414\n",
      "Original Input range: 0.0 66.27521514892578\n",
      "Output range: 0.00866684503853321 0.9966855645179749\n",
      "Batch loss: 5.559447288513184\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.0015602844068780541 0.999251663684845\n",
      "Batch loss: 5.323545932769775\n",
      "Original Input range: 0.0 46.95805358886719\n",
      "Output range: 0.030929453670978546 0.9994687438011169\n",
      "Batch loss: 5.21995210647583\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.002328069182112813 0.999553382396698\n",
      "Batch loss: 5.154141426086426\n",
      "Original Input range: 0.0 79.60464477539062\n",
      "Output range: 0.022980544716119766 0.9977220892906189\n",
      "Batch loss: 5.212652683258057\n",
      "Original Input range: 0.0 53.051963806152344\n",
      "Output range: 0.02344662882387638 0.9972862005233765\n",
      "Batch loss: 5.6227545738220215\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.003302033059298992 0.9997215867042542\n",
      "Batch loss: 6.5198493003845215\n",
      "Original Input range: 0.0 36.512420654296875\n",
      "Output range: 0.039378706365823746 0.9979490637779236\n",
      "Batch loss: 4.873602390289307\n",
      "Original Input range: 0.0 51.13776397705078\n",
      "Output range: 0.03550686314702034 0.994484007358551\n",
      "Batch loss: 5.423075199127197\n",
      "Original Input range: 0.0 54.94767379760742\n",
      "Output range: 0.005092135164886713 0.9996730089187622\n",
      "Batch loss: 4.908034801483154\n",
      "Original Input range: 0.0 59.92755126953125\n",
      "Output range: 0.033219803124666214 0.9979567527770996\n",
      "Batch loss: 6.072608947753906\n",
      "Original Input range: 0.0 59.37202835083008\n",
      "Output range: 0.05351606011390686 0.9966601133346558\n",
      "Batch loss: 5.032184600830078\n",
      "Original Input range: 0.0 53.641319274902344\n",
      "Output range: 0.05556666478514671 0.9949014186859131\n",
      "Batch loss: 5.470219135284424\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.03863966837525368 0.9996824264526367\n",
      "Batch loss: 6.779460430145264\n",
      "Original Input range: 0.0 83.86809539794922\n",
      "Output range: 0.022678911685943604 0.9987969398498535\n",
      "Batch loss: 5.283385276794434\n",
      "Original Input range: 0.0 31.53007698059082\n",
      "Output range: 0.045622605830430984 0.9949398040771484\n",
      "Batch loss: 5.407429218292236\n",
      "Original Input range: 0.0 37.13005065917969\n",
      "Output range: 0.02427554316818714 0.9976403713226318\n",
      "Batch loss: 5.430083751678467\n",
      "Original Input range: 0.0 51.33224868774414\n",
      "Output range: 0.015969788655638695 0.9998823404312134\n",
      "Batch loss: 5.562103748321533\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.022784341126680374 0.9984139204025269\n",
      "Batch loss: 5.391455173492432\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.03746073320508003 0.9947770833969116\n",
      "Batch loss: 8.39854907989502\n",
      "Original Input range: 0.0 102.27552795410156\n",
      "Output range: 0.017716804519295692 0.9977624416351318\n",
      "Batch loss: 6.3253374099731445\n",
      "Original Input range: 0.0 57.596832275390625\n",
      "Output range: 0.03388659656047821 0.9971801042556763\n",
      "Batch loss: 6.049282550811768\n",
      "Original Input range: 0.0 54.78154373168945\n",
      "Output range: 0.03272669389843941 0.9996354579925537\n",
      "Batch loss: 5.374179363250732\n",
      "Original Input range: 0.0 40.561767578125\n",
      "Output range: 0.020011018961668015 0.9989117383956909\n",
      "Batch loss: 5.125093460083008\n",
      "Original Input range: 0.0 42.355018615722656\n",
      "Output range: 0.04009847715497017 0.9981879591941833\n",
      "Batch loss: 4.781960964202881\n",
      "Original Input range: 0.0 40.230987548828125\n",
      "Output range: 0.028713541105389595 0.9937893748283386\n",
      "Batch loss: 5.639265060424805\n",
      "Original Input range: 0.0 43.11163330078125\n",
      "Output range: 0.05465342849493027 0.9995900988578796\n",
      "Batch loss: 5.8172831535339355\n",
      "Original Input range: 0.0 43.055049896240234\n",
      "Output range: 0.026240747421979904 0.9990850687026978\n",
      "Batch loss: 6.399570465087891\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.011753841303288937 0.9996273517608643\n",
      "Batch loss: 6.7225141525268555\n",
      "Validation Input range: 0.0 39.0533447265625\n",
      "Validation Output range: 0.1144886463880539 0.9905809760093689\n",
      "Validation Input range: 0.0 47.14912414550781\n",
      "Validation Output range: 0.12908081710338593 0.9919658899307251\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.06757615506649017 0.9994558691978455\n",
      "Validation Input range: 0.0 41.433265686035156\n",
      "Validation Output range: 0.12301890552043915 0.991042971611023\n",
      "Validation Input range: 0.0 54.87662887573242\n",
      "Validation Output range: 0.08240332454442978 0.9892409443855286\n",
      "Validation Input range: 0.0 41.19639587402344\n",
      "Validation Output range: 0.12210866063833237 0.9873484373092651\n",
      "Validation Input range: 0.0 40.4886589050293\n",
      "Validation Output range: 0.11458296328783035 0.9895776510238647\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.08650415390729904 0.9995129108428955\n",
      "Validation Input range: 0.0 79.57794952392578\n",
      "Validation Output range: 0.12040034681558609 0.9890467524528503\n",
      "Validation Input range: 0.0 37.079376220703125\n",
      "Validation Output range: 0.11519099026918411 0.9919690489768982\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.026484061032533646 0.9999700784683228\n",
      "Validation Input range: 0.0 33.93391418457031\n",
      "Validation Output range: 0.11552047729492188 0.9895304441452026\n",
      "Validation Input range: 0.0 32.81044006347656\n",
      "Validation Output range: 0.11935166269540787 0.9908813238143921\n",
      "Validation Input range: 0.0 49.359073638916016\n",
      "Validation Output range: 0.13876581192016602 0.9923508763313293\n",
      "Validation Input range: 0.0 39.49129867553711\n",
      "Validation Output range: 0.12055793404579163 0.9885426759719849\n",
      "Validation Input range: 0.0 43.210540771484375\n",
      "Validation Output range: 0.1157589703798294 0.9915804266929626\n",
      "Validation Input range: 0.0 32.88222122192383\n",
      "Validation Output range: 0.11307424306869507 0.9914857745170593\n",
      "Validation Input range: 0.0 39.19929885864258\n",
      "Validation Output range: 0.11588401347398758 0.9886534810066223\n",
      "Validation Input range: 0.0 52.02884292602539\n",
      "Validation Output range: 0.10208684951066971 0.9954937696456909\n",
      "Validation Input range: 0.0 42.88813781738281\n",
      "Validation Output range: 0.1070663332939148 0.993976354598999\n",
      "Validation Input range: 0.0 30.61672592163086\n",
      "Validation Output range: 0.1442749798297882 0.9751361608505249\n",
      "Epoch [2/10], Average Batch Training Loss: 5.772606983990737, Average Batch Validation Loss: 5.454810676120577\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.008401378057897091 0.9997026324272156\n",
      "Batch loss: 5.58541202545166\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.009905328042805195 0.9961981177330017\n",
      "Batch loss: 8.82552433013916\n",
      "Original Input range: 0.0 47.26083755493164\n",
      "Output range: 0.01906065084040165 0.9989583492279053\n",
      "Batch loss: 5.4469428062438965\n",
      "Original Input range: 0.0 47.97340393066406\n",
      "Output range: 0.06952328234910965 0.9966511130332947\n",
      "Batch loss: 6.109221935272217\n",
      "Original Input range: 0.0 64.54231262207031\n",
      "Output range: 0.06718534231185913 0.9988142251968384\n",
      "Batch loss: 6.372223377227783\n",
      "Original Input range: 0.0 34.881980895996094\n",
      "Output range: 0.06617531180381775 0.9997518658638\n",
      "Batch loss: 5.614931106567383\n",
      "Original Input range: 0.0 36.37077713012695\n",
      "Output range: 0.08140072971582413 0.9994530081748962\n",
      "Batch loss: 5.308063507080078\n",
      "Original Input range: 0.0 59.37202835083008\n",
      "Output range: 0.015889659523963928 0.9994136095046997\n",
      "Batch loss: 5.595216751098633\n",
      "Original Input range: 0.0 92.99876403808594\n",
      "Output range: 0.04379981383681297 0.9972594976425171\n",
      "Batch loss: 6.489432334899902\n",
      "Original Input range: 0.0 63.922203063964844\n",
      "Output range: 0.04938896745443344 0.9940565824508667\n",
      "Batch loss: 4.942669868469238\n",
      "Original Input range: 0.0 66.27521514892578\n",
      "Output range: 0.016410620883107185 0.9896551370620728\n",
      "Batch loss: 4.940062522888184\n",
      "Original Input range: 0.0 47.27468490600586\n",
      "Output range: 0.07655368000268936 0.9932410717010498\n",
      "Batch loss: 5.141496658325195\n",
      "Original Input range: 0.0 102.27552795410156\n",
      "Output range: 0.01900472491979599 0.9989606142044067\n",
      "Batch loss: 6.56300687789917\n",
      "Original Input range: 0.0 40.561767578125\n",
      "Output range: 0.05857101455330849 0.997025191783905\n",
      "Batch loss: 4.541377067565918\n",
      "Original Input range: 0.0 63.922203063964844\n",
      "Output range: 0.05595368519425392 0.9962061643600464\n",
      "Batch loss: 6.058110237121582\n",
      "Original Input range: 0.0 41.575199127197266\n",
      "Output range: 0.05847538262605667 0.9994508624076843\n",
      "Batch loss: 4.782596111297607\n",
      "Original Input range: 0.0 40.764122009277344\n",
      "Output range: 0.042436446994543076 0.9991305470466614\n",
      "Batch loss: 5.168550968170166\n",
      "Original Input range: 0.0 77.83159637451172\n",
      "Output range: 0.05968585982918739 0.9970729351043701\n",
      "Batch loss: 5.542740821838379\n",
      "Original Input range: 0.0 82.15438842773438\n",
      "Output range: 0.030878037214279175 0.9995238780975342\n",
      "Batch loss: 5.679289817810059\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.008380674757063389 0.9999790191650391\n",
      "Batch loss: 8.577881813049316\n",
      "Original Input range: 0.0 34.89680099487305\n",
      "Output range: 0.047149159014225006 0.9940787553787231\n",
      "Batch loss: 4.876016616821289\n",
      "Original Input range: 0.0 64.35528564453125\n",
      "Output range: 0.06313406676054001 0.9964643716812134\n",
      "Batch loss: 7.1230854988098145\n",
      "Original Input range: 0.0 54.94767379760742\n",
      "Output range: 0.06353454291820526 0.9975910186767578\n",
      "Batch loss: 5.7730913162231445\n",
      "Original Input range: 0.0 54.78154373168945\n",
      "Output range: 0.048063721507787704 0.998526930809021\n",
      "Batch loss: 5.7800703048706055\n",
      "Original Input range: 0.0 83.86809539794922\n",
      "Output range: 0.05212784558534622 0.9975211024284363\n",
      "Batch loss: 5.389195919036865\n",
      "Original Input range: 0.0 34.26239013671875\n",
      "Output range: 0.020749414339661598 0.998536229133606\n",
      "Batch loss: 5.431008815765381\n",
      "Original Input range: 0.0 59.92755126953125\n",
      "Output range: 0.07765429466962814 0.9948590993881226\n",
      "Batch loss: 6.448544025421143\n",
      "Original Input range: 0.0 37.277435302734375\n",
      "Output range: 0.04833449050784111 0.9995224475860596\n",
      "Batch loss: 4.602597236633301\n",
      "Original Input range: 0.0 46.55131149291992\n",
      "Output range: 0.039847131818532944 0.9994596838951111\n",
      "Batch loss: 5.862626075744629\n",
      "Original Input range: 0.0 54.426517486572266\n",
      "Output range: 0.069907046854496 0.9978851675987244\n",
      "Batch loss: 6.252695560455322\n",
      "Original Input range: 0.0 85.22960662841797\n",
      "Output range: 0.07100576162338257 0.9987670183181763\n",
      "Batch loss: 6.78132438659668\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.05298930034041405 0.9991117119789124\n",
      "Batch loss: 7.842013835906982\n",
      "Original Input range: 0.0 54.8901252746582\n",
      "Output range: 0.04255617409944534 0.9942352175712585\n",
      "Batch loss: 5.92018985748291\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.010802707634866238 0.9971141815185547\n",
      "Batch loss: 7.0373663902282715\n",
      "Original Input range: 0.0 86.11009979248047\n",
      "Output range: 0.03385786712169647 0.9898627996444702\n",
      "Batch loss: 5.460393905639648\n",
      "Original Input range: 0.0 80.46197509765625\n",
      "Output range: 0.01146125327795744 0.9998732805252075\n",
      "Batch loss: 5.996662616729736\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.030005138367414474 0.9971593618392944\n",
      "Batch loss: 6.562920570373535\n",
      "Original Input range: 0.0 159.15589904785156\n",
      "Output range: 0.009523398242890835 0.9969246983528137\n",
      "Batch loss: 6.5696258544921875\n",
      "Original Input range: 0.0 159.15589904785156\n",
      "Output range: 0.004865780007094145 0.9957984089851379\n",
      "Batch loss: 7.366698741912842\n",
      "Original Input range: 0.0 41.01715087890625\n",
      "Output range: 0.033708710223436356 0.998687207698822\n",
      "Batch loss: 4.940394878387451\n",
      "Original Input range: 0.0 33.312889099121094\n",
      "Output range: 0.021219633519649506 0.9996501207351685\n",
      "Batch loss: 4.1570143699646\n",
      "Original Input range: 0.0 51.19465637207031\n",
      "Output range: 0.08661457896232605 0.9977200627326965\n",
      "Batch loss: 5.391739368438721\n",
      "Original Input range: 0.0 54.78456115722656\n",
      "Output range: 0.056286539882421494 0.9968628883361816\n",
      "Batch loss: 4.866649150848389\n",
      "Original Input range: 0.0 79.60464477539062\n",
      "Output range: 0.07203388214111328 0.9904140830039978\n",
      "Batch loss: 6.490638732910156\n",
      "Original Input range: 0.0 127.84440612792969\n",
      "Output range: 0.04865645989775658 0.997844934463501\n",
      "Batch loss: 6.33509635925293\n",
      "Original Input range: 0.0 43.624542236328125\n",
      "Output range: 0.06490723043680191 0.9945008754730225\n",
      "Batch loss: 5.490835666656494\n",
      "Original Input range: 0.0 46.95805358886719\n",
      "Output range: 0.07292456179857254 0.9922468662261963\n",
      "Batch loss: 4.874144077301025\n",
      "Original Input range: 0.0 85.24281311035156\n",
      "Output range: 0.08102189749479294 0.998281717300415\n",
      "Batch loss: 5.86669397354126\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.036053936928510666 0.9994001388549805\n",
      "Batch loss: 5.831208229064941\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.007677476853132248 0.9969236254692078\n",
      "Batch loss: 6.19783353805542\n",
      "Original Input range: 0.0 40.514862060546875\n",
      "Output range: 0.05123450979590416 0.9954450130462646\n",
      "Batch loss: 5.195153713226318\n",
      "Original Input range: 0.0 50.34861373901367\n",
      "Output range: 0.017650993540883064 0.9981676340103149\n",
      "Batch loss: 4.732516288757324\n",
      "Original Input range: 0.0 85.22960662841797\n",
      "Output range: 0.026811478659510612 0.9979719519615173\n",
      "Batch loss: 5.4957990646362305\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.008863053284585476 0.9996682405471802\n",
      "Batch loss: 5.9346842765808105\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.044667165726423264 0.9996455907821655\n",
      "Batch loss: 5.989703178405762\n",
      "Original Input range: 0.0 53.051963806152344\n",
      "Output range: 0.06801532953977585 0.9994352459907532\n",
      "Batch loss: 5.329257965087891\n",
      "Original Input range: 0.0 51.13776397705078\n",
      "Output range: 0.04853283613920212 0.9988208413124084\n",
      "Batch loss: 4.820460319519043\n",
      "Original Input range: 0.0 36.04962158203125\n",
      "Output range: 0.03777865692973137 0.998009979724884\n",
      "Batch loss: 4.900108337402344\n",
      "Original Input range: 0.0 45.650856018066406\n",
      "Output range: 0.09144675731658936 0.9926171898841858\n",
      "Batch loss: 5.846604347229004\n",
      "Original Input range: 0.0 41.53719711303711\n",
      "Output range: 0.10497814416885376 0.996147871017456\n",
      "Batch loss: 5.035370826721191\n",
      "Original Input range: 0.0 32.888370513916016\n",
      "Output range: 0.0756063312292099 0.9989472031593323\n",
      "Batch loss: 4.322639465332031\n",
      "Original Input range: 0.0 51.517547607421875\n",
      "Output range: 0.08064960688352585 0.9968311190605164\n",
      "Batch loss: 5.3756794929504395\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.040591198951005936 0.998472273349762\n",
      "Batch loss: 7.788393497467041\n",
      "Original Input range: 0.0 50.47058868408203\n",
      "Output range: 0.048722829669713974 0.9984793066978455\n",
      "Batch loss: 5.85221529006958\n",
      "Original Input range: 0.0 53.99897766113281\n",
      "Output range: 0.02352214977145195 0.9967395663261414\n",
      "Batch loss: 4.629310131072998\n",
      "Original Input range: 0.0 38.68598175048828\n",
      "Output range: 0.08991353958845139 0.9973323345184326\n",
      "Batch loss: 4.357979774475098\n",
      "Original Input range: 0.0 48.06983947753906\n",
      "Output range: 0.050591204315423965 0.9886841177940369\n",
      "Batch loss: 5.052099227905273\n",
      "Original Input range: 0.0 42.27656555175781\n",
      "Output range: 0.06837329268455505 0.9959486126899719\n",
      "Batch loss: 4.906805992126465\n",
      "Original Input range: 0.0 79.5941390991211\n",
      "Output range: 0.03122565895318985 0.999982476234436\n",
      "Batch loss: 5.385038375854492\n",
      "Original Input range: 0.0 40.5273323059082\n",
      "Output range: 0.07534455507993698 0.9897398948669434\n",
      "Batch loss: 5.199947834014893\n",
      "Original Input range: 0.0 59.99742126464844\n",
      "Output range: 0.013098512776196003 0.9993083477020264\n",
      "Batch loss: 5.436995029449463\n",
      "Validation Input range: 0.0 39.0533447265625\n",
      "Validation Output range: 0.1429305374622345 0.9931313395500183\n",
      "Validation Input range: 0.0 47.14912414550781\n",
      "Validation Output range: 0.14889776706695557 0.9885218143463135\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.12803266942501068 0.9974949359893799\n",
      "Validation Input range: 0.0 41.433265686035156\n",
      "Validation Output range: 0.14500738680362701 0.986179530620575\n",
      "Validation Input range: 0.0 54.87662887573242\n",
      "Validation Output range: 0.14838147163391113 0.9889044165611267\n",
      "Validation Input range: 0.0 41.19639587402344\n",
      "Validation Output range: 0.13652662932872772 0.988375186920166\n",
      "Validation Input range: 0.0 40.4886589050293\n",
      "Validation Output range: 0.13779371976852417 0.9903104901313782\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.09224182367324829 0.9984304308891296\n",
      "Validation Input range: 0.0 79.57794952392578\n",
      "Validation Output range: 0.12332749366760254 0.9901852607727051\n",
      "Validation Input range: 0.0 37.079376220703125\n",
      "Validation Output range: 0.1590224653482437 0.9865854978561401\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.06145695596933365 0.9995456337928772\n",
      "Validation Input range: 0.0 33.93391418457031\n",
      "Validation Output range: 0.14408713579177856 0.986862063407898\n",
      "Validation Input range: 0.0 32.81044006347656\n",
      "Validation Output range: 0.12373674660921097 0.9848458766937256\n",
      "Validation Input range: 0.0 49.359073638916016\n",
      "Validation Output range: 0.1590258777141571 0.9876677393913269\n",
      "Validation Input range: 0.0 39.49129867553711\n",
      "Validation Output range: 0.15000969171524048 0.9884722232818604\n",
      "Validation Input range: 0.0 43.210540771484375\n",
      "Validation Output range: 0.14453236758708954 0.9912657141685486\n",
      "Validation Input range: 0.0 32.88222122192383\n",
      "Validation Output range: 0.13657383620738983 0.9879328608512878\n",
      "Validation Input range: 0.0 39.19929885864258\n",
      "Validation Output range: 0.1236305981874466 0.9828160405158997\n",
      "Validation Input range: 0.0 52.02884292602539\n",
      "Validation Output range: 0.14264899492263794 0.9846786260604858\n",
      "Validation Input range: 0.0 42.88813781738281\n",
      "Validation Output range: 0.12762436270713806 0.9898681640625\n",
      "Validation Input range: 0.0 30.61672592163086\n",
      "Validation Output range: 0.16131635010242462 0.9773648381233215\n",
      "Epoch [3/10], Average Batch Training Loss: 5.72380125690514, Average Batch Validation Loss: 5.401797078904652\n",
      "Original Input range: 0.0 63.922203063964844\n",
      "Output range: 0.041115500032901764 0.9930510520935059\n",
      "Batch loss: 4.690862655639648\n",
      "Original Input range: 0.0 41.807594299316406\n",
      "Output range: 0.057539574801921844 0.9968090653419495\n",
      "Batch loss: 5.359330177307129\n",
      "Original Input range: 0.0 51.13776397705078\n",
      "Output range: 0.06844350695610046 0.9991982579231262\n",
      "Batch loss: 5.570714473724365\n",
      "Original Input range: 0.0 39.22265625\n",
      "Output range: 0.05008728802204132 0.9994321465492249\n",
      "Batch loss: 4.5850138664245605\n",
      "Original Input range: 0.0 54.78456115722656\n",
      "Output range: 0.036253854632377625 0.9995250701904297\n",
      "Batch loss: 5.22266149520874\n",
      "Original Input range: 0.0 77.83159637451172\n",
      "Output range: 0.08469343930482864 0.9948868155479431\n",
      "Batch loss: 5.163390636444092\n",
      "Original Input range: 0.0 47.27468490600586\n",
      "Output range: 0.05430027097463608 0.9920952916145325\n",
      "Batch loss: 5.908698081970215\n",
      "Original Input range: 0.0 34.614356994628906\n",
      "Output range: 0.07353540509939194 0.9937971830368042\n",
      "Batch loss: 4.945100784301758\n",
      "Original Input range: 0.0 59.37202835083008\n",
      "Output range: 0.06838486343622208 0.9916958212852478\n",
      "Batch loss: 6.3085246086120605\n",
      "Original Input range: 0.0 82.15438842773438\n",
      "Output range: 0.09920896589756012 0.996411144733429\n",
      "Batch loss: 6.0649919509887695\n",
      "Original Input range: 0.0 59.99742126464844\n",
      "Output range: 0.04881327971816063 0.9988389611244202\n",
      "Batch loss: 5.370730400085449\n",
      "Original Input range: 0.0 85.24281311035156\n",
      "Output range: 0.05113542824983597 0.9974727034568787\n",
      "Batch loss: 7.243571758270264\n",
      "Original Input range: 0.0 63.922203063964844\n",
      "Output range: 0.05738726630806923 0.9888534545898438\n",
      "Batch loss: 5.6684465408325195\n",
      "Original Input range: 0.0 53.051963806152344\n",
      "Output range: 0.016305262222886086 0.9924128651618958\n",
      "Batch loss: 4.87726354598999\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.028529701754450798 0.9941186904907227\n",
      "Batch loss: 6.421921253204346\n",
      "Original Input range: 0.0 38.00634002685547\n",
      "Output range: 0.09176064282655716 0.9972808361053467\n",
      "Batch loss: 5.497883319854736\n",
      "Original Input range: 0.0 43.56495666503906\n",
      "Output range: 0.058434635400772095 0.9996446371078491\n",
      "Batch loss: 4.61319637298584\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.1048329621553421 0.9964995384216309\n",
      "Batch loss: 7.561004161834717\n",
      "Original Input range: 0.0 43.82943344116211\n",
      "Output range: 0.09641031175851822 0.9961178302764893\n",
      "Batch loss: 4.99881649017334\n",
      "Original Input range: 0.0 64.35528564453125\n",
      "Output range: 0.09278228133916855 0.9955196380615234\n",
      "Batch loss: 5.930601119995117\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.056603364646434784 0.990715503692627\n",
      "Batch loss: 7.672565460205078\n",
      "Original Input range: 0.0 55.424163818359375\n",
      "Output range: 0.06679490208625793 0.9976311922073364\n",
      "Batch loss: 6.069679260253906\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.03716066852211952 0.999142050743103\n",
      "Batch loss: 10.840362548828125\n",
      "Original Input range: 0.0 54.8901252746582\n",
      "Output range: 0.06265588849782944 0.9970358610153198\n",
      "Batch loss: 5.235323429107666\n",
      "Original Input range: 0.0 54.94767379760742\n",
      "Output range: 0.023582885041832924 0.9991914629936218\n",
      "Batch loss: 4.818816661834717\n",
      "Original Input range: 0.0 42.51327133178711\n",
      "Output range: 0.05087082087993622 0.9956176280975342\n",
      "Batch loss: 5.386342525482178\n",
      "Original Input range: 0.0 49.05189514160156\n",
      "Output range: 0.07410471886396408 0.9941727519035339\n",
      "Batch loss: 6.572912216186523\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.08457282930612564 0.9985186457633972\n",
      "Batch loss: 8.073372840881348\n",
      "Original Input range: 0.0 79.5941390991211\n",
      "Output range: 0.036325424909591675 0.999058187007904\n",
      "Batch loss: 6.851909637451172\n",
      "Original Input range: 0.0 43.055049896240234\n",
      "Output range: 0.041916657239198685 0.9984031319618225\n",
      "Batch loss: 4.648808002471924\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.036973074078559875 0.9970155954360962\n",
      "Batch loss: 6.581317901611328\n",
      "Original Input range: 0.0 85.22960662841797\n",
      "Output range: 0.09480550140142441 0.9932647943496704\n",
      "Batch loss: 6.396937370300293\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.027051394805312157 0.9999077320098877\n",
      "Batch loss: 6.076934814453125\n",
      "Original Input range: 0.0 39.37679672241211\n",
      "Output range: 0.08977679163217545 0.9909493327140808\n",
      "Batch loss: 4.4644646644592285\n",
      "Original Input range: 0.0 51.19465637207031\n",
      "Output range: 0.09316960722208023 0.9959841966629028\n",
      "Batch loss: 6.020777702331543\n",
      "Original Input range: 0.0 33.93821716308594\n",
      "Output range: 0.030091075226664543 0.9984493255615234\n",
      "Batch loss: 3.9767227172851562\n",
      "Original Input range: 0.0 40.230987548828125\n",
      "Output range: 0.07310029119253159 0.9967578053474426\n",
      "Batch loss: 4.897234916687012\n",
      "Original Input range: 0.0 68.46208953857422\n",
      "Output range: 0.0541544072329998 0.9927018880844116\n",
      "Batch loss: 5.004240989685059\n",
      "Original Input range: 0.0 43.31812286376953\n",
      "Output range: 0.05026083439588547 0.9963661432266235\n",
      "Batch loss: 5.501984596252441\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.0746421068906784 0.9899333119392395\n",
      "Batch loss: 5.704739093780518\n",
      "Original Input range: 0.0 51.517547607421875\n",
      "Output range: 0.05349377170205116 0.9959432482719421\n",
      "Batch loss: 4.792158126831055\n",
      "Original Input range: 0.0 53.99897766113281\n",
      "Output range: 0.07218106836080551 0.9923934936523438\n",
      "Batch loss: 6.221860885620117\n",
      "Original Input range: 0.0 127.84440612792969\n",
      "Output range: 0.108881875872612 0.99319988489151\n",
      "Batch loss: 6.095407009124756\n",
      "Original Input range: 0.0 80.46197509765625\n",
      "Output range: 0.07694437354803085 0.9981537461280823\n",
      "Batch loss: 4.9424896240234375\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.03583990037441254 0.9978904128074646\n",
      "Batch loss: 5.798342704772949\n",
      "Original Input range: 0.0 79.60464477539062\n",
      "Output range: 0.05381876230239868 0.9949373006820679\n",
      "Batch loss: 5.922021865844727\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.06062263995409012 0.9954417943954468\n",
      "Batch loss: 6.46697998046875\n",
      "Original Input range: 0.0 36.687992095947266\n",
      "Output range: 0.0744728147983551 0.989997148513794\n",
      "Batch loss: 4.4013190269470215\n",
      "Original Input range: 0.0 43.41630554199219\n",
      "Output range: 0.08806474506855011 0.9934982061386108\n",
      "Batch loss: 4.629020690917969\n",
      "Original Input range: 0.0 51.33224868774414\n",
      "Output range: 0.06852152198553085 0.9985049962997437\n",
      "Batch loss: 5.541759490966797\n",
      "Original Input range: 0.0 39.20207977294922\n",
      "Output range: 0.08406101912260056 0.9980677962303162\n",
      "Batch loss: 5.285397529602051\n",
      "Original Input range: 0.0 59.92755126953125\n",
      "Output range: 0.07687751203775406 0.9984344840049744\n",
      "Batch loss: 6.142741680145264\n",
      "Original Input range: 0.0 56.19416427612305\n",
      "Output range: 0.05970041826367378 0.997475802898407\n",
      "Batch loss: 6.18323278427124\n",
      "Original Input range: 0.0 41.5326042175293\n",
      "Output range: 0.038828425109386444 0.9896256327629089\n",
      "Batch loss: 5.526395797729492\n",
      "Original Input range: 0.0 38.21017837524414\n",
      "Output range: 0.06214200705289841 0.9958427548408508\n",
      "Batch loss: 5.410708427429199\n",
      "Original Input range: 0.0 159.15589904785156\n",
      "Output range: 0.007123974617570639 0.9999978542327881\n",
      "Batch loss: 5.702342510223389\n",
      "Original Input range: 0.0 92.99876403808594\n",
      "Output range: 0.10173812508583069 0.9917910099029541\n",
      "Batch loss: 6.410674095153809\n",
      "Original Input range: 0.0 46.85770034790039\n",
      "Output range: 0.05413908138871193 0.9920703768730164\n",
      "Batch loss: 4.7764892578125\n",
      "Original Input range: 0.0 42.5487174987793\n",
      "Output range: 0.07740425318479538 0.9925270080566406\n",
      "Batch loss: 4.843804359436035\n",
      "Original Input range: 0.0 48.06983947753906\n",
      "Output range: 0.11446892470121384 0.9984791874885559\n",
      "Batch loss: 5.483007907867432\n",
      "Original Input range: 0.0 28.245132446289062\n",
      "Output range: 0.0991033986210823 0.993530809879303\n",
      "Batch loss: 4.6613240242004395\n",
      "Original Input range: 0.0 80.46197509765625\n",
      "Output range: 0.08471575379371643 0.9988447427749634\n",
      "Batch loss: 6.100944519042969\n",
      "Original Input range: 0.0 48.32259750366211\n",
      "Output range: 0.09312940388917923 0.9943143725395203\n",
      "Batch loss: 5.502217769622803\n",
      "Original Input range: 0.0 159.15589904785156\n",
      "Output range: 0.03671177476644516 0.9979873895645142\n",
      "Batch loss: 6.519433498382568\n",
      "Original Input range: 0.0 83.86809539794922\n",
      "Output range: 0.08857652544975281 0.9978019595146179\n",
      "Batch loss: 5.298033714294434\n",
      "Original Input range: 0.0 42.724082946777344\n",
      "Output range: 0.060122814029455185 0.9935842156410217\n",
      "Batch loss: 5.198991298675537\n",
      "Original Input range: 0.0 102.27552795410156\n",
      "Output range: 0.057331640273332596 0.997761607170105\n",
      "Batch loss: 5.020634651184082\n",
      "Original Input range: 0.0 37.49324035644531\n",
      "Output range: 0.07180618494749069 0.9942743182182312\n",
      "Batch loss: 5.412564754486084\n",
      "Original Input range: 0.0 39.80232238769531\n",
      "Output range: 0.04781227186322212 0.9848843812942505\n",
      "Batch loss: 4.535284042358398\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.12534239888191223 0.9918327331542969\n",
      "Batch loss: 6.522218227386475\n",
      "Original Input range: 0.0 46.55131149291992\n",
      "Output range: 0.11520741879940033 0.9945789575576782\n",
      "Batch loss: 5.7948503494262695\n",
      "Validation Input range: 0.0 39.0533447265625\n",
      "Validation Output range: 0.18853729963302612 0.9904904365539551\n",
      "Validation Input range: 0.0 47.14912414550781\n",
      "Validation Output range: 0.17430557310581207 0.975385844707489\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.11743758618831635 0.9990376234054565\n",
      "Validation Input range: 0.0 41.433265686035156\n",
      "Validation Output range: 0.16681963205337524 0.9794379472732544\n",
      "Validation Input range: 0.0 54.87662887573242\n",
      "Validation Output range: 0.1648886650800705 0.9965921640396118\n",
      "Validation Input range: 0.0 41.19639587402344\n",
      "Validation Output range: 0.1836753785610199 0.9767415523529053\n",
      "Validation Input range: 0.0 40.4886589050293\n",
      "Validation Output range: 0.10685192793607712 0.9788574576377869\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.1804218590259552 0.9984452128410339\n",
      "Validation Input range: 0.0 79.57794952392578\n",
      "Validation Output range: 0.1781693547964096 0.9928968548774719\n",
      "Validation Input range: 0.0 37.079376220703125\n",
      "Validation Output range: 0.18685325980186462 0.9766057729721069\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.13261204957962036 0.9993994235992432\n",
      "Validation Input range: 0.0 33.93391418457031\n",
      "Validation Output range: 0.1820770651102066 0.974037766456604\n",
      "Validation Input range: 0.0 32.81044006347656\n",
      "Validation Output range: 0.1668318808078766 0.9800146818161011\n",
      "Validation Input range: 0.0 49.359073638916016\n",
      "Validation Output range: 0.1652020364999771 0.981234610080719\n",
      "Validation Input range: 0.0 39.49129867553711\n",
      "Validation Output range: 0.18655872344970703 0.9807366132736206\n",
      "Validation Input range: 0.0 43.210540771484375\n",
      "Validation Output range: 0.1625189632177353 0.9803771376609802\n",
      "Validation Input range: 0.0 32.88222122192383\n",
      "Validation Output range: 0.1492612361907959 0.9802114963531494\n",
      "Validation Input range: 0.0 39.19929885864258\n",
      "Validation Output range: 0.167991504073143 0.9808644652366638\n",
      "Validation Input range: 0.0 52.02884292602539\n",
      "Validation Output range: 0.1690322756767273 0.9808433651924133\n",
      "Validation Input range: 0.0 42.88813781738281\n",
      "Validation Output range: 0.1800306737422943 0.9796203970909119\n",
      "Validation Input range: 0.0 30.61672592163086\n",
      "Validation Output range: 0.18195529282093048 0.9685885906219482\n",
      "Epoch [4/10], Average Batch Training Loss: 5.689307347149916, Average Batch Validation Loss: 5.389440195901053\n",
      "Original Input range: 0.0 85.22960662841797\n",
      "Output range: 0.09770625084638596 0.9995216131210327\n",
      "Batch loss: 5.257554054260254\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.0357840359210968 0.9999696016311646\n",
      "Batch loss: 5.603678226470947\n",
      "Original Input range: 0.0 43.055049896240234\n",
      "Output range: 0.05219215899705887 0.9994342923164368\n",
      "Batch loss: 5.026169776916504\n",
      "Original Input range: 0.0 51.19465637207031\n",
      "Output range: 0.10265865176916122 0.995747983455658\n",
      "Batch loss: 5.693637371063232\n",
      "Original Input range: 0.0 159.15589904785156\n",
      "Output range: 0.03626391664147377 0.9989168643951416\n",
      "Batch loss: 6.789522171020508\n",
      "Original Input range: 0.0 28.01096534729004\n",
      "Output range: 0.050760526210069656 0.9951540231704712\n",
      "Batch loss: 4.431605339050293\n",
      "Original Input range: 0.0 45.650856018066406\n",
      "Output range: 0.057277169078588486 0.9953038692474365\n",
      "Batch loss: 4.582665920257568\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.11857111006975174 0.9877930283546448\n",
      "Batch loss: 7.420763969421387\n",
      "Original Input range: 0.0 42.355018615722656\n",
      "Output range: 0.1028556302189827 0.9937044978141785\n",
      "Batch loss: 4.830221176147461\n",
      "Original Input range: 0.0 82.15438842773438\n",
      "Output range: 0.14005741477012634 0.9863616228103638\n",
      "Batch loss: 5.874892711639404\n",
      "Original Input range: 0.0 43.45826721191406\n",
      "Output range: 0.0651465356349945 0.9957356452941895\n",
      "Batch loss: 3.9361844062805176\n",
      "Original Input range: 0.0 85.24281311035156\n",
      "Output range: 0.078616663813591 0.991396963596344\n",
      "Batch loss: 5.340307235717773\n",
      "Original Input range: 0.0 31.382654190063477\n",
      "Output range: 0.06507481634616852 0.9857487082481384\n",
      "Batch loss: 4.56719970703125\n",
      "Original Input range: 0.0 53.051963806152344\n",
      "Output range: 0.09304947406053543 0.9937572479248047\n",
      "Batch loss: 5.2010650634765625\n",
      "Original Input range: 0.0 47.97340393066406\n",
      "Output range: 0.09981890767812729 0.9935898780822754\n",
      "Batch loss: 5.162327289581299\n",
      "Original Input range: 0.0 48.69768524169922\n",
      "Output range: 0.07369739562273026 0.9931020736694336\n",
      "Batch loss: 5.667306900024414\n",
      "Original Input range: 0.0 127.84440612792969\n",
      "Output range: 0.07966465502977371 0.9929460883140564\n",
      "Batch loss: 5.698331356048584\n",
      "Original Input range: 0.0 41.575199127197266\n",
      "Output range: 0.11047375202178955 0.9893683791160583\n",
      "Batch loss: 5.976312637329102\n",
      "Original Input range: 0.0 80.46197509765625\n",
      "Output range: 0.10911423712968826 0.9971089959144592\n",
      "Batch loss: 7.142201900482178\n",
      "Original Input range: 0.0 48.06983947753906\n",
      "Output range: 0.10251542925834656 0.9956181049346924\n",
      "Batch loss: 5.982423782348633\n",
      "Original Input range: 0.0 58.06863784790039\n",
      "Output range: 0.046602752059698105 0.9949524402618408\n",
      "Batch loss: 4.681506633758545\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.0785997286438942 0.9969256520271301\n",
      "Batch loss: 7.6798505783081055\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.10090582817792892 0.9970439076423645\n",
      "Batch loss: 7.205474853515625\n",
      "Original Input range: 0.0 51.152488708496094\n",
      "Output range: 0.05582404509186745 0.9935805797576904\n",
      "Batch loss: 5.2026286125183105\n",
      "Original Input range: 0.0 37.340721130371094\n",
      "Output range: 0.04033584147691727 0.9953948855400085\n",
      "Batch loss: 4.4320969581604\n",
      "Original Input range: 0.0 86.11009979248047\n",
      "Output range: 0.057373788207769394 0.9992363452911377\n",
      "Batch loss: 6.243045806884766\n",
      "Original Input range: 0.0 63.922203063964844\n",
      "Output range: 0.046756561845541 0.9967406392097473\n",
      "Batch loss: 6.13217306137085\n",
      "Original Input range: 0.0 34.61096954345703\n",
      "Output range: 0.08669440448284149 0.9880224466323853\n",
      "Batch loss: 4.31588888168335\n",
      "Original Input range: 0.0 36.1522102355957\n",
      "Output range: 0.06882417947053909 0.9851924180984497\n",
      "Batch loss: 4.818606376647949\n",
      "Original Input range: 0.0 43.11163330078125\n",
      "Output range: 0.11920379847288132 0.9829802513122559\n",
      "Batch loss: 5.122087478637695\n",
      "Original Input range: 0.0 39.80232238769531\n",
      "Output range: 0.11225204914808273 0.9962761402130127\n",
      "Batch loss: 5.729745864868164\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.1308867484331131 0.9878088235855103\n",
      "Batch loss: 5.83250093460083\n",
      "Original Input range: 0.0 50.47058868408203\n",
      "Output range: 0.12090485543012619 0.9944223761558533\n",
      "Batch loss: 5.376986026763916\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.12095241248607635 0.9898439645767212\n",
      "Batch loss: 6.674752712249756\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.05707602575421333 0.9883335828781128\n",
      "Batch loss: 7.084717273712158\n",
      "Original Input range: 0.0 45.98609924316406\n",
      "Output range: 0.14127986133098602 0.9893707036972046\n",
      "Batch loss: 5.29742431640625\n",
      "Original Input range: 0.0 79.60464477539062\n",
      "Output range: 0.10037055611610413 0.9953083395957947\n",
      "Batch loss: 6.2238664627075195\n",
      "Original Input range: 0.0 79.5941390991211\n",
      "Output range: 0.04929585009813309 0.9988239407539368\n",
      "Batch loss: 5.596029281616211\n",
      "Original Input range: 0.0 59.92755126953125\n",
      "Output range: 0.10487335175275803 0.9964653253555298\n",
      "Batch loss: 5.044046401977539\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.1282581239938736 0.9982837438583374\n",
      "Batch loss: 8.452078819274902\n",
      "Original Input range: 0.0 59.37202835083008\n",
      "Output range: 0.07340267300605774 0.9854985475540161\n",
      "Batch loss: 5.240277290344238\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.053131408989429474 0.993505597114563\n",
      "Batch loss: 7.872045993804932\n",
      "Original Input range: 0.0 64.54231262207031\n",
      "Output range: 0.09703241288661957 0.9935433864593506\n",
      "Batch loss: 5.790177822113037\n",
      "Original Input range: 0.0 42.51327133178711\n",
      "Output range: 0.05374450981616974 0.9917003512382507\n",
      "Batch loss: 6.115604400634766\n",
      "Original Input range: 0.0 54.78154373168945\n",
      "Output range: 0.08994884043931961 0.9947691559791565\n",
      "Batch loss: 4.122400760650635\n",
      "Original Input range: 0.0 37.59745788574219\n",
      "Output range: 0.14545029401779175 0.9923303723335266\n",
      "Batch loss: 5.139273166656494\n",
      "Original Input range: 0.0 42.62110137939453\n",
      "Output range: 0.04878629744052887 0.9909781217575073\n",
      "Batch loss: 5.366915225982666\n",
      "Original Input range: 0.0 102.27552795410156\n",
      "Output range: 0.038440074771642685 0.992932915687561\n",
      "Batch loss: 5.370115756988525\n",
      "Original Input range: 0.0 47.55305099487305\n",
      "Output range: 0.08658908307552338 0.991783618927002\n",
      "Batch loss: 4.807404518127441\n",
      "Original Input range: 0.0 50.853153228759766\n",
      "Output range: 0.05253026634454727 0.9951938986778259\n",
      "Batch loss: 4.249180316925049\n",
      "Original Input range: 0.0 53.99897766113281\n",
      "Output range: 0.07418803125619888 0.9997640252113342\n",
      "Batch loss: 5.158016681671143\n",
      "Original Input range: 0.0 45.17311096191406\n",
      "Output range: 0.09702135622501373 0.9837902188301086\n",
      "Batch loss: 4.662827491760254\n",
      "Original Input range: 0.0 51.33224868774414\n",
      "Output range: 0.08789906650781631 0.985906720161438\n",
      "Batch loss: 5.6844658851623535\n",
      "Original Input range: 0.0 92.99876403808594\n",
      "Output range: 0.06652028858661652 0.9840322136878967\n",
      "Batch loss: 6.160758018493652\n",
      "Original Input range: 0.0 80.46197509765625\n",
      "Output range: 0.058015771210193634 0.9987077713012695\n",
      "Batch loss: 6.070140361785889\n",
      "Original Input range: 0.0 42.655887603759766\n",
      "Output range: 0.11360468715429306 0.9933752417564392\n",
      "Batch loss: 5.207623481750488\n",
      "Original Input range: 0.0 35.959510803222656\n",
      "Output range: 0.1332857608795166 0.995194137096405\n",
      "Batch loss: 5.230566024780273\n",
      "Original Input range: 0.0 68.46208953857422\n",
      "Output range: 0.09390130639076233 0.9954943656921387\n",
      "Batch loss: 5.3652753829956055\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.11179215461015701 0.9947821497917175\n",
      "Batch loss: 8.6856107711792\n",
      "Original Input range: 0.0 33.11359405517578\n",
      "Output range: 0.06970247626304626 0.9854796528816223\n",
      "Batch loss: 5.145820140838623\n",
      "Original Input range: 0.0 66.27521514892578\n",
      "Output range: 0.12876814603805542 0.9956307411193848\n",
      "Batch loss: 5.733022212982178\n",
      "Original Input range: 0.0 43.31812286376953\n",
      "Output range: 0.1832764744758606 0.9919649362564087\n",
      "Batch loss: 6.17040491104126\n",
      "Original Input range: 0.0 85.22960662841797\n",
      "Output range: 0.09114273637533188 0.9967705011367798\n",
      "Batch loss: 6.5797529220581055\n",
      "Original Input range: 0.0 159.15589904785156\n",
      "Output range: 0.1128440722823143 0.9963510036468506\n",
      "Batch loss: 6.358858585357666\n",
      "Original Input range: 0.0 46.85770034790039\n",
      "Output range: 0.08144944906234741 0.9850242137908936\n",
      "Batch loss: 5.241507530212402\n",
      "Original Input range: 0.0 32.270538330078125\n",
      "Output range: 0.14880497753620148 0.9874551296234131\n",
      "Batch loss: 5.419572353363037\n",
      "Original Input range: 0.0 40.561767578125\n",
      "Output range: 0.12475234270095825 0.9889447093009949\n",
      "Batch loss: 4.790245532989502\n",
      "Original Input range: 0.0 56.19416427612305\n",
      "Output range: 0.015293932519853115 0.9950529932975769\n",
      "Batch loss: 5.746260166168213\n",
      "Original Input range: 0.0 77.83159637451172\n",
      "Output range: 0.15041810274124146 0.9918621778488159\n",
      "Batch loss: 5.721848487854004\n",
      "Original Input range: 0.0 47.26083755493164\n",
      "Output range: 0.13811439275741577 0.9862769246101379\n",
      "Batch loss: 5.995645999908447\n",
      "Original Input range: 0.0 42.724082946777344\n",
      "Output range: 0.03248659148812294 0.9861477017402649\n",
      "Batch loss: 5.300236225128174\n",
      "Validation Input range: 0.0 39.0533447265625\n",
      "Validation Output range: 0.17984172701835632 0.9771438241004944\n",
      "Validation Input range: 0.0 47.14912414550781\n",
      "Validation Output range: 0.20021480321884155 0.9659428596496582\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.1222691684961319 0.9916770458221436\n",
      "Validation Input range: 0.0 41.433265686035156\n",
      "Validation Output range: 0.1585572212934494 0.9698421359062195\n",
      "Validation Input range: 0.0 54.87662887573242\n",
      "Validation Output range: 0.1875729113817215 0.9804863929748535\n",
      "Validation Input range: 0.0 41.19639587402344\n",
      "Validation Output range: 0.1841973215341568 0.9680988788604736\n",
      "Validation Input range: 0.0 40.4886589050293\n",
      "Validation Output range: 0.11500751972198486 0.9668566584587097\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.19148387014865875 0.9894634485244751\n",
      "Validation Input range: 0.0 79.57794952392578\n",
      "Validation Output range: 0.1658870428800583 0.9792713522911072\n",
      "Validation Input range: 0.0 37.079376220703125\n",
      "Validation Output range: 0.15112170577049255 0.9710463285446167\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.1246967688202858 0.9977484345436096\n",
      "Validation Input range: 0.0 33.93391418457031\n",
      "Validation Output range: 0.1322035789489746 0.9728882312774658\n",
      "Validation Input range: 0.0 32.81044006347656\n",
      "Validation Output range: 0.13651542365550995 0.9681527614593506\n",
      "Validation Input range: 0.0 49.359073638916016\n",
      "Validation Output range: 0.16678054630756378 0.9721569418907166\n",
      "Validation Input range: 0.0 39.49129867553711\n",
      "Validation Output range: 0.19930750131607056 0.9702404737472534\n",
      "Validation Input range: 0.0 43.210540771484375\n",
      "Validation Output range: 0.10693076252937317 0.9788728952407837\n",
      "Validation Input range: 0.0 32.88222122192383\n",
      "Validation Output range: 0.17490817606449127 0.9710088968276978\n",
      "Validation Input range: 0.0 39.19929885864258\n",
      "Validation Output range: 0.16621647775173187 0.9687215089797974\n",
      "Validation Input range: 0.0 52.02884292602539\n",
      "Validation Output range: 0.19371995329856873 0.968988835811615\n",
      "Validation Input range: 0.0 42.88813781738281\n",
      "Validation Output range: 0.15159381926059723 0.9714742302894592\n",
      "Validation Input range: 0.0 30.61672592163086\n",
      "Validation Output range: 0.19007308781147003 0.9651764035224915\n",
      "Epoch [5/10], Average Batch Training Loss: 5.65960186971745, Average Batch Validation Loss: 5.37389706429981\n",
      "Original Input range: 0.0 34.950706481933594\n",
      "Output range: 0.11553650349378586 0.9940462112426758\n",
      "Batch loss: 5.0888285636901855\n",
      "Original Input range: 0.0 40.514862060546875\n",
      "Output range: 0.1302247941493988 0.9883236885070801\n",
      "Batch loss: 5.351346492767334\n",
      "Original Input range: 0.0 159.15589904785156\n",
      "Output range: 0.042740598320961 0.9987159967422485\n",
      "Batch loss: 6.646049976348877\n",
      "Original Input range: 0.0 54.8901252746582\n",
      "Output range: 0.04352512210607529 0.9870133996009827\n",
      "Batch loss: 5.513360500335693\n",
      "Original Input range: 0.0 38.7529182434082\n",
      "Output range: 0.15703676640987396 0.9863929152488708\n",
      "Batch loss: 5.315728664398193\n",
      "Original Input range: 0.0 85.22960662841797\n",
      "Output range: 0.05059745907783508 0.9930443167686462\n",
      "Batch loss: 6.158731460571289\n",
      "Original Input range: 0.0 68.46208953857422\n",
      "Output range: 0.10961762815713882 0.9885214567184448\n",
      "Batch loss: 5.5323686599731445\n",
      "Original Input range: 0.0 43.624542236328125\n",
      "Output range: 0.08452438563108444 0.9858518838882446\n",
      "Batch loss: 4.450400352478027\n",
      "Original Input range: 0.0 127.84440612792969\n",
      "Output range: 0.09664572775363922 0.9963505268096924\n",
      "Batch loss: 6.273644924163818\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.08744363486766815 0.9803999662399292\n",
      "Batch loss: 5.660254001617432\n",
      "Original Input range: 0.0 51.517547607421875\n",
      "Output range: 0.1225220188498497 0.9855672121047974\n",
      "Batch loss: 5.328291893005371\n",
      "Original Input range: 0.0 54.94767379760742\n",
      "Output range: 0.15678760409355164 0.9908457398414612\n",
      "Batch loss: 6.858755588531494\n",
      "Original Input range: 0.0 42.355018615722656\n",
      "Output range: 0.15391682088375092 0.9912125468254089\n",
      "Batch loss: 5.864886283874512\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.07187362015247345 0.993513286113739\n",
      "Batch loss: 6.245348930358887\n",
      "Original Input range: 0.0 47.90782928466797\n",
      "Output range: 0.14074409008026123 0.9855666160583496\n",
      "Batch loss: 5.365199089050293\n",
      "Original Input range: 0.0 92.99876403808594\n",
      "Output range: 0.12330611795186996 0.9868996143341064\n",
      "Batch loss: 5.91632080078125\n",
      "Original Input range: 0.0 42.614803314208984\n",
      "Output range: 0.14549855887889862 0.9918661713600159\n",
      "Batch loss: 5.778407573699951\n",
      "Original Input range: 0.0 56.19416427612305\n",
      "Output range: 0.12938572466373444 0.9897862672805786\n",
      "Batch loss: 4.951757431030273\n",
      "Original Input range: 0.0 66.27521514892578\n",
      "Output range: 0.12421008944511414 0.9956228137016296\n",
      "Batch loss: 5.529642581939697\n",
      "Original Input range: 0.0 50.853153228759766\n",
      "Output range: 0.051516979932785034 0.9837872385978699\n",
      "Batch loss: 4.704316139221191\n",
      "Original Input range: 0.0 59.37202835083008\n",
      "Output range: 0.15739545226097107 0.9907490611076355\n",
      "Batch loss: 5.000905513763428\n",
      "Original Input range: 0.0 54.78154373168945\n",
      "Output range: 0.1313965916633606 0.9842206239700317\n",
      "Batch loss: 5.930757522583008\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.0634550005197525 0.9947832226753235\n",
      "Batch loss: 6.801361560821533\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.024175619706511497 0.9982435703277588\n",
      "Batch loss: 6.516623020172119\n",
      "Original Input range: 0.0 57.596832275390625\n",
      "Output range: 0.14011695981025696 0.9869686961174011\n",
      "Batch loss: 6.068634033203125\n",
      "Original Input range: 0.0 48.32259750366211\n",
      "Output range: 0.16160303354263306 0.9883328080177307\n",
      "Batch loss: 5.911677360534668\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.12853923439979553 0.9867931008338928\n",
      "Batch loss: 6.113053321838379\n",
      "Original Input range: 0.0 43.055049896240234\n",
      "Output range: 0.1027243360877037 0.9830154776573181\n",
      "Batch loss: 5.346682548522949\n",
      "Original Input range: 0.0 159.15589904785156\n",
      "Output range: 0.12868304550647736 0.9932605624198914\n",
      "Batch loss: 8.166496276855469\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.08861570805311203 0.9956955909729004\n",
      "Batch loss: 6.737280368804932\n",
      "Original Input range: 0.0 83.86809539794922\n",
      "Output range: 0.07953508198261261 0.9939382076263428\n",
      "Batch loss: 6.281829833984375\n",
      "Original Input range: 0.0 42.724082946777344\n",
      "Output range: 0.0956602692604065 0.9859662652015686\n",
      "Batch loss: 5.554503917694092\n",
      "Original Input range: 0.0 54.78456115722656\n",
      "Output range: 0.04612121731042862 0.9874557256698608\n",
      "Batch loss: 4.821785926818848\n",
      "Original Input range: 0.0 36.67008972167969\n",
      "Output range: 0.05447224900126457 0.9850170612335205\n",
      "Batch loss: 4.8871259689331055\n",
      "Original Input range: 0.0 35.781036376953125\n",
      "Output range: 0.0917007252573967 0.9899999499320984\n",
      "Batch loss: 4.911754608154297\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.028074802830815315 0.9835783243179321\n",
      "Batch loss: 6.15591287612915\n",
      "Original Input range: 0.0 43.45826721191406\n",
      "Output range: 0.1313585788011551 0.990346372127533\n",
      "Batch loss: 5.689857006072998\n",
      "Original Input range: 0.0 30.69662094116211\n",
      "Output range: 0.069723941385746 0.983576238155365\n",
      "Batch loss: 4.200867652893066\n",
      "Original Input range: 0.0 51.2689323425293\n",
      "Output range: 0.060880307108163834 0.9815713763237\n",
      "Batch loss: 5.475712299346924\n",
      "Original Input range: 0.0 82.15438842773438\n",
      "Output range: 0.13553833961486816 0.9928777813911438\n",
      "Batch loss: 6.451879501342773\n",
      "Original Input range: 0.0 42.62110137939453\n",
      "Output range: 0.1545010358095169 0.9871534705162048\n",
      "Batch loss: 5.541428565979004\n",
      "Original Input range: 0.0 38.52928161621094\n",
      "Output range: 0.14394575357437134 0.9893108606338501\n",
      "Batch loss: 4.983799457550049\n",
      "Original Input range: 0.0 39.49016189575195\n",
      "Output range: 0.12712538242340088 0.9972042441368103\n",
      "Batch loss: 4.4426774978637695\n",
      "Original Input range: 0.0 36.04282760620117\n",
      "Output range: 0.13591578602790833 0.9881815314292908\n",
      "Batch loss: 4.73741340637207\n",
      "Original Input range: 0.0 58.06863784790039\n",
      "Output range: 0.13382366299629211 0.9898746013641357\n",
      "Batch loss: 4.979903221130371\n",
      "Original Input range: 0.0 51.13776397705078\n",
      "Output range: 0.15762197971343994 0.9864563941955566\n",
      "Batch loss: 6.060580730438232\n",
      "Original Input range: 0.0 64.35528564453125\n",
      "Output range: 0.09024350345134735 0.9914687871932983\n",
      "Batch loss: 5.062687873840332\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.07894878834486008 0.9981800317764282\n",
      "Batch loss: 7.4279022216796875\n",
      "Original Input range: 0.0 38.68598175048828\n",
      "Output range: 0.06460540741682053 0.9882321953773499\n",
      "Batch loss: 4.892725467681885\n",
      "Original Input range: 0.0 53.052162170410156\n",
      "Output range: 0.02871648781001568 0.9933156371116638\n",
      "Batch loss: 5.591530799865723\n",
      "Original Input range: 0.0 79.60464477539062\n",
      "Output range: 0.03564172238111496 0.9870935082435608\n",
      "Batch loss: 6.388884544372559\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.10289695113897324 0.986409068107605\n",
      "Batch loss: 7.132045745849609\n",
      "Original Input range: 0.0 46.85770034790039\n",
      "Output range: 0.1492139995098114 0.9864248037338257\n",
      "Batch loss: 5.571462154388428\n",
      "Original Input range: 0.0 45.98609924316406\n",
      "Output range: 0.1377989798784256 0.9853699207305908\n",
      "Batch loss: 4.969039440155029\n",
      "Original Input range: 0.0 50.34861373901367\n",
      "Output range: 0.1425340324640274 0.9888308644294739\n",
      "Batch loss: 5.450681209564209\n",
      "Original Input range: 0.0 40.24433517456055\n",
      "Output range: 0.044108856469392776 0.9926746487617493\n",
      "Batch loss: 3.910604238510132\n",
      "Original Input range: 0.0 53.051963806152344\n",
      "Output range: 0.09421016275882721 0.9951423406600952\n",
      "Batch loss: 5.1061577796936035\n",
      "Original Input range: 0.0 55.424163818359375\n",
      "Output range: 0.13882757723331451 0.9918333292007446\n",
      "Batch loss: 6.093100547790527\n",
      "Original Input range: 0.0 85.22960662841797\n",
      "Output range: 0.06326530128717422 0.9927054047584534\n",
      "Batch loss: 5.186549663543701\n",
      "Original Input range: 0.0 86.11009979248047\n",
      "Output range: 0.1178603544831276 0.9926252961158752\n",
      "Batch loss: 6.091203689575195\n",
      "Original Input range: 0.0 37.13005065917969\n",
      "Output range: 0.10445642471313477 0.98206627368927\n",
      "Batch loss: 4.58458948135376\n",
      "Original Input range: 0.0 43.11163330078125\n",
      "Output range: 0.0917184054851532 0.9819568395614624\n",
      "Batch loss: 5.62790060043335\n",
      "Original Input range: 0.0 80.46197509765625\n",
      "Output range: 0.11006081104278564 0.9887175559997559\n",
      "Batch loss: 6.460565567016602\n",
      "Original Input range: 0.0 85.24281311035156\n",
      "Output range: 0.16764569282531738 0.9923155307769775\n",
      "Batch loss: 5.374290943145752\n",
      "Original Input range: 0.0 36.20566177368164\n",
      "Output range: 0.1048215925693512 0.9930126070976257\n",
      "Batch loss: 5.271021366119385\n",
      "Original Input range: 0.0 51.19465637207031\n",
      "Output range: 0.15667961537837982 0.9871394038200378\n",
      "Batch loss: 5.705721855163574\n",
      "Original Input range: 0.0 80.46197509765625\n",
      "Output range: 0.068467877805233 0.9959219694137573\n",
      "Batch loss: 6.239062309265137\n",
      "Original Input range: 0.0 47.26083755493164\n",
      "Output range: 0.02458968386054039 0.9930996894836426\n",
      "Batch loss: 4.556523323059082\n",
      "Original Input range: 0.0 59.99742126464844\n",
      "Output range: 0.03611107915639877 0.9822652339935303\n",
      "Batch loss: 6.183452606201172\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.11036426573991776 0.9916039109230042\n",
      "Batch loss: 6.923025608062744\n",
      "Original Input range: 0.0 31.55365753173828\n",
      "Output range: 0.027509113773703575 0.982079267501831\n",
      "Batch loss: 4.4300537109375\n",
      "Validation Input range: 0.0 39.0533447265625\n",
      "Validation Output range: 0.20297202467918396 0.9811784625053406\n",
      "Validation Input range: 0.0 47.14912414550781\n",
      "Validation Output range: 0.17988058924674988 0.9824652671813965\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.1397489607334137 0.9910997748374939\n",
      "Validation Input range: 0.0 41.433265686035156\n",
      "Validation Output range: 0.19138827919960022 0.9823808670043945\n",
      "Validation Input range: 0.0 54.87662887573242\n",
      "Validation Output range: 0.10739109665155411 0.9857935309410095\n",
      "Validation Input range: 0.0 41.19639587402344\n",
      "Validation Output range: 0.14400264620780945 0.9810377955436707\n",
      "Validation Input range: 0.0 40.4886589050293\n",
      "Validation Output range: 0.05004142224788666 0.9789882898330688\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.19640597701072693 0.9943385720252991\n",
      "Validation Input range: 0.0 79.57794952392578\n",
      "Validation Output range: 0.18912042677402496 0.986441433429718\n",
      "Validation Input range: 0.0 37.079376220703125\n",
      "Validation Output range: 0.173121839761734 0.9809882640838623\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.17456597089767456 0.9962446093559265\n",
      "Validation Input range: 0.0 33.93391418457031\n",
      "Validation Output range: 0.20104189217090607 0.9821614027023315\n",
      "Validation Input range: 0.0 32.81044006347656\n",
      "Validation Output range: 0.20311060547828674 0.9770150184631348\n",
      "Validation Input range: 0.0 49.359073638916016\n",
      "Validation Output range: 0.150130033493042 0.9830815196037292\n",
      "Validation Input range: 0.0 39.49129867553711\n",
      "Validation Output range: 0.15534622967243195 0.9834882616996765\n",
      "Validation Input range: 0.0 43.210540771484375\n",
      "Validation Output range: 0.01740233041346073 0.9817688465118408\n",
      "Validation Input range: 0.0 32.88222122192383\n",
      "Validation Output range: 0.10353539884090424 0.9809532761573792\n",
      "Validation Input range: 0.0 39.19929885864258\n",
      "Validation Output range: 0.19968406856060028 0.9842590093612671\n",
      "Validation Input range: 0.0 52.02884292602539\n",
      "Validation Output range: 0.08981659263372421 0.9835883378982544\n",
      "Validation Input range: 0.0 42.88813781738281\n",
      "Validation Output range: 0.13004212081432343 0.983940601348877\n",
      "Validation Input range: 0.0 30.61672592163086\n",
      "Validation Output range: 0.21787843108177185 0.9736088514328003\n",
      "Epoch [6/10], Average Batch Training Loss: 5.641336995111385, Average Batch Validation Loss: 5.329660960606167\n",
      "Original Input range: 0.0 80.46197509765625\n",
      "Output range: 0.14221379160881042 0.9959897398948669\n",
      "Batch loss: 5.895969390869141\n",
      "Original Input range: 0.0 37.1517219543457\n",
      "Output range: 0.02400025725364685 0.9907584190368652\n",
      "Batch loss: 4.60850715637207\n",
      "Original Input range: 0.0 64.54231262207031\n",
      "Output range: 0.15595705807209015 0.9865469932556152\n",
      "Batch loss: 5.6269450187683105\n",
      "Original Input range: 0.0 29.35586929321289\n",
      "Output range: 0.06424705684185028 0.9892968535423279\n",
      "Batch loss: 4.252584934234619\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.10414586961269379 0.9981405735015869\n",
      "Batch loss: 7.191113471984863\n",
      "Original Input range: 0.0 43.46803283691406\n",
      "Output range: 0.11480206251144409 0.9858936071395874\n",
      "Batch loss: 5.151801109313965\n",
      "Original Input range: 0.0 31.672121047973633\n",
      "Output range: 0.10167776048183441 0.972284734249115\n",
      "Batch loss: 4.377530097961426\n",
      "Original Input range: 0.0 55.424163818359375\n",
      "Output range: 0.06891515105962753 0.9791203737258911\n",
      "Batch loss: 5.0916361808776855\n",
      "Original Input range: 0.0 42.51327133178711\n",
      "Output range: 0.05553939566016197 0.9880045056343079\n",
      "Batch loss: 5.067651748657227\n",
      "Original Input range: 0.0 38.55706024169922\n",
      "Output range: 0.029534313827753067 0.9939123392105103\n",
      "Batch loss: 3.7263143062591553\n",
      "Original Input range: 0.0 159.15589904785156\n",
      "Output range: 0.14285708963871002 0.9942986965179443\n",
      "Batch loss: 7.68375825881958\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.10635417699813843 0.9981663823127747\n",
      "Batch loss: 9.101459503173828\n",
      "Original Input range: 0.0 43.11163330078125\n",
      "Output range: 0.07740002870559692 0.9836658835411072\n",
      "Batch loss: 5.122138023376465\n",
      "Original Input range: 0.0 47.90782928466797\n",
      "Output range: 0.12076576054096222 0.9866706728935242\n",
      "Batch loss: 5.434719562530518\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.11827094107866287 0.9940358400344849\n",
      "Batch loss: 9.196614265441895\n",
      "Original Input range: 0.0 48.69768524169922\n",
      "Output range: 0.13347239792346954 0.990036129951477\n",
      "Batch loss: 5.475710868835449\n",
      "Original Input range: 0.0 53.99897766113281\n",
      "Output range: 0.1286385953426361 0.9922507405281067\n",
      "Batch loss: 5.631112098693848\n",
      "Original Input range: 0.0 51.33224868774414\n",
      "Output range: 0.0840919017791748 0.9835606813430786\n",
      "Batch loss: 5.16694974899292\n",
      "Original Input range: 0.0 56.19416427612305\n",
      "Output range: 0.14201697707176208 0.9910796284675598\n",
      "Batch loss: 5.767368793487549\n",
      "Original Input range: 0.0 59.37202835083008\n",
      "Output range: 0.049310825765132904 0.9919481873512268\n",
      "Batch loss: 5.880801200866699\n",
      "Original Input range: 0.0 79.5941390991211\n",
      "Output range: 0.11329296231269836 0.9891111254692078\n",
      "Batch loss: 5.353530406951904\n",
      "Original Input range: 0.0 42.19498062133789\n",
      "Output range: 0.06353271752595901 0.9896687865257263\n",
      "Batch loss: 5.184919834136963\n",
      "Original Input range: 0.0 66.27521514892578\n",
      "Output range: 0.07415870577096939 0.9946082830429077\n",
      "Batch loss: 5.259439468383789\n",
      "Original Input range: 0.0 36.2550048828125\n",
      "Output range: 0.13166260719299316 0.976675271987915\n",
      "Batch loss: 5.2944560050964355\n",
      "Original Input range: 0.0 59.92755126953125\n",
      "Output range: 0.13734567165374756 0.9849646687507629\n",
      "Batch loss: 6.006704807281494\n",
      "Original Input range: 0.0 57.333343505859375\n",
      "Output range: 0.02740168571472168 0.9895064234733582\n",
      "Batch loss: 4.398261547088623\n",
      "Original Input range: 0.0 36.67008972167969\n",
      "Output range: 0.11868222802877426 0.987986147403717\n",
      "Batch loss: 4.203738689422607\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.01996471919119358 0.9983978867530823\n",
      "Batch loss: 7.592428684234619\n",
      "Original Input range: 0.0 85.24281311035156\n",
      "Output range: 0.15887679159641266 0.9874523282051086\n",
      "Batch loss: 7.443129539489746\n",
      "Original Input range: 0.0 53.051963806152344\n",
      "Output range: 0.03891422972083092 0.9864168167114258\n",
      "Batch loss: 4.984604358673096\n",
      "Original Input range: 0.0 85.22960662841797\n",
      "Output range: 0.12589314579963684 0.9953356385231018\n",
      "Batch loss: 5.2978668212890625\n",
      "Original Input range: 0.0 46.55131149291992\n",
      "Output range: 0.11535631865262985 0.9867621064186096\n",
      "Batch loss: 5.846531867980957\n",
      "Original Input range: 0.0 50.47058868408203\n",
      "Output range: 0.0950208231806755 0.9974775910377502\n",
      "Batch loss: 4.961738586425781\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.10757800191640854 0.9980688691139221\n",
      "Batch loss: 6.984988689422607\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.17193648219108582 0.9878338575363159\n",
      "Batch loss: 5.361969947814941\n",
      "Original Input range: 0.0 33.312889099121094\n",
      "Output range: 0.14123769104480743 0.9863518476486206\n",
      "Batch loss: 4.677209377288818\n",
      "Original Input range: 0.0 40.764122009277344\n",
      "Output range: 0.16184993088245392 0.9830896854400635\n",
      "Batch loss: 5.777458190917969\n",
      "Original Input range: 0.0 41.01715087890625\n",
      "Output range: 0.16686289012432098 0.9848034381866455\n",
      "Batch loss: 5.838123321533203\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.11568602174520493 0.9876420497894287\n",
      "Batch loss: 5.199882507324219\n",
      "Original Input range: 0.0 51.19465637207031\n",
      "Output range: 0.17983151972293854 0.9909071922302246\n",
      "Batch loss: 5.763439655303955\n",
      "Original Input range: 0.0 33.668190002441406\n",
      "Output range: 0.07020983099937439 0.9957278966903687\n",
      "Batch loss: 4.316120624542236\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.17893214523792267 0.9812541007995605\n",
      "Batch loss: 6.080282688140869\n",
      "Original Input range: 0.0 127.84440612792969\n",
      "Output range: 0.1902441829442978 0.9929987192153931\n",
      "Batch loss: 6.582566261291504\n",
      "Original Input range: 0.0 51.13776397705078\n",
      "Output range: 0.11921239644289017 0.985524594783783\n",
      "Batch loss: 5.735951900482178\n",
      "Original Input range: 0.0 51.517547607421875\n",
      "Output range: 0.1536305993795395 0.9924174547195435\n",
      "Batch loss: 4.979110240936279\n",
      "Original Input range: 0.0 30.76824188232422\n",
      "Output range: 0.06770425289869308 0.9876025319099426\n",
      "Batch loss: 4.674316883087158\n",
      "Original Input range: 0.0 47.27468490600586\n",
      "Output range: 0.15413036942481995 0.9797959327697754\n",
      "Batch loss: 5.217813491821289\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.07157988101243973 0.9938275218009949\n",
      "Batch loss: 5.323464870452881\n",
      "Original Input range: 0.0 63.922203063964844\n",
      "Output range: 0.16228972375392914 0.9894917011260986\n",
      "Batch loss: 5.703736782073975\n",
      "Original Input range: 0.0 82.15438842773438\n",
      "Output range: 0.1563628762960434 0.9925116300582886\n",
      "Batch loss: 7.275674343109131\n",
      "Original Input range: 0.0 43.45826721191406\n",
      "Output range: 0.12355393171310425 0.9925757646560669\n",
      "Batch loss: 6.573909282684326\n",
      "Original Input range: 0.0 43.09564971923828\n",
      "Output range: 0.13025136291980743 0.9882314801216125\n",
      "Batch loss: 5.163405418395996\n",
      "Original Input range: 0.0 102.27552795410156\n",
      "Output range: 0.08905292302370071 0.9939706325531006\n",
      "Batch loss: 5.582560062408447\n",
      "Original Input range: 0.0 54.78456115722656\n",
      "Output range: 0.12133883684873581 0.9856874346733093\n",
      "Batch loss: 5.225614070892334\n",
      "Original Input range: 0.0 43.31812286376953\n",
      "Output range: 0.15260149538516998 0.9905627965927124\n",
      "Batch loss: 4.822287559509277\n",
      "Original Input range: 0.0 54.94767379760742\n",
      "Output range: 0.1111202985048294 0.9867826700210571\n",
      "Batch loss: 5.781495094299316\n",
      "Original Input range: 0.0 42.355018615722656\n",
      "Output range: 0.11792278289794922 0.9912703037261963\n",
      "Batch loss: 5.410789489746094\n",
      "Original Input range: 0.0 64.35528564453125\n",
      "Output range: 0.1670183390378952 0.9879224896430969\n",
      "Batch loss: 6.301602363586426\n",
      "Original Input range: 0.0 32.9843635559082\n",
      "Output range: 0.14401568472385406 0.9871150255203247\n",
      "Batch loss: 5.252430438995361\n",
      "Original Input range: 0.0 45.17311096191406\n",
      "Output range: 0.14338935911655426 0.9824251532554626\n",
      "Batch loss: 6.017691135406494\n",
      "Original Input range: 0.0 57.596832275390625\n",
      "Output range: 0.15066835284233093 0.9888160228729248\n",
      "Batch loss: 6.169604778289795\n",
      "Original Input range: 0.0 58.06863784790039\n",
      "Output range: 0.17057424783706665 0.9845339059829712\n",
      "Batch loss: 5.688453674316406\n",
      "Original Input range: 0.0 40.142005920410156\n",
      "Output range: 0.1036757156252861 0.9914842247962952\n",
      "Batch loss: 5.139626979827881\n",
      "Original Input range: 0.0 48.32259750366211\n",
      "Output range: 0.1066838875412941 0.9903552532196045\n",
      "Batch loss: 4.785354137420654\n",
      "Original Input range: 0.0 41.78765106201172\n",
      "Output range: 0.021209752187132835 0.9810034036636353\n",
      "Batch loss: 5.474342346191406\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.16857191920280457 0.9823353886604309\n",
      "Batch loss: 6.050056457519531\n",
      "Original Input range: 0.0 92.99876403808594\n",
      "Output range: 0.11534415930509567 0.9919179081916809\n",
      "Batch loss: 4.683107376098633\n",
      "Original Input range: 0.0 42.62110137939453\n",
      "Output range: 0.12180642783641815 0.9837514758110046\n",
      "Batch loss: 5.1660566329956055\n",
      "Original Input range: 0.0 43.47346115112305\n",
      "Output range: 0.012150890193879604 0.9863091707229614\n",
      "Batch loss: 5.185377597808838\n",
      "Original Input range: 0.0 86.11009979248047\n",
      "Output range: 0.07863148301839828 0.9845527410507202\n",
      "Batch loss: 7.610064506530762\n",
      "Original Input range: 0.0 34.411346435546875\n",
      "Output range: 0.10326919704675674 0.9852042198181152\n",
      "Batch loss: 5.070542812347412\n",
      "Validation Input range: 0.0 39.0533447265625\n",
      "Validation Output range: 0.1861593872308731 0.9814221858978271\n",
      "Validation Input range: 0.0 47.14912414550781\n",
      "Validation Output range: 0.1983024775981903 0.975663423538208\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.1665254384279251 0.9898666143417358\n",
      "Validation Input range: 0.0 41.433265686035156\n",
      "Validation Output range: 0.18550686538219452 0.9762469530105591\n",
      "Validation Input range: 0.0 54.87662887573242\n",
      "Validation Output range: 0.17105446755886078 0.9872398972511292\n",
      "Validation Input range: 0.0 41.19639587402344\n",
      "Validation Output range: 0.18908706307411194 0.9774824976921082\n",
      "Validation Input range: 0.0 40.4886589050293\n",
      "Validation Output range: 0.029489606618881226 0.9745519757270813\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.1845102161169052 0.985275387763977\n",
      "Validation Input range: 0.0 79.57794952392578\n",
      "Validation Output range: 0.1538039743900299 0.9947392344474792\n",
      "Validation Input range: 0.0 37.079376220703125\n",
      "Validation Output range: 0.17939148843288422 0.9732330441474915\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.13520489633083344 0.9964858293533325\n",
      "Validation Input range: 0.0 33.93391418457031\n",
      "Validation Output range: 0.17855125665664673 0.973340630531311\n",
      "Validation Input range: 0.0 32.81044006347656\n",
      "Validation Output range: 0.17386797070503235 0.9740344882011414\n",
      "Validation Input range: 0.0 49.359073638916016\n",
      "Validation Output range: 0.17542824149131775 0.9768255949020386\n",
      "Validation Input range: 0.0 39.49129867553711\n",
      "Validation Output range: 0.1860674023628235 0.9802299737930298\n",
      "Validation Input range: 0.0 43.210540771484375\n",
      "Validation Output range: 0.04692636430263519 0.984820544719696\n",
      "Validation Input range: 0.0 32.88222122192383\n",
      "Validation Output range: 0.20803582668304443 0.976531982421875\n",
      "Validation Input range: 0.0 39.19929885864258\n",
      "Validation Output range: 0.19892342388629913 0.9739970564842224\n",
      "Validation Input range: 0.0 52.02884292602539\n",
      "Validation Output range: 0.18735277652740479 0.9796094298362732\n",
      "Validation Input range: 0.0 42.88813781738281\n",
      "Validation Output range: 0.18448565900325775 0.9761301875114441\n",
      "Validation Input range: 0.0 30.61672592163086\n",
      "Validation Output range: 0.1998426914215088 0.9631701111793518\n",
      "Epoch [7/10], Average Batch Training Loss: 5.632739695025162, Average Batch Validation Loss: 5.328935021445865\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.027844522148370743 0.9987325072288513\n",
      "Batch loss: 5.6569037437438965\n",
      "Original Input range: 0.0 54.78154373168945\n",
      "Output range: 0.10976956784725189 0.9905445575714111\n",
      "Batch loss: 5.527901649475098\n",
      "Original Input range: 0.0 42.724082946777344\n",
      "Output range: 0.15529437363147736 0.9810324311256409\n",
      "Batch loss: 4.693851470947266\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.11210019141435623 0.9929783940315247\n",
      "Batch loss: 8.880082130432129\n",
      "Original Input range: 0.0 58.06863784790039\n",
      "Output range: 0.08608295023441315 0.9872510433197021\n",
      "Batch loss: 5.962317943572998\n",
      "Original Input range: 0.0 43.405128479003906\n",
      "Output range: 0.11473695933818817 0.9886836409568787\n",
      "Batch loss: 5.717082500457764\n",
      "Original Input range: 0.0 53.641319274902344\n",
      "Output range: 0.030298126861453056 0.9896054267883301\n",
      "Batch loss: 4.897969722747803\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.07559824734926224 0.9887953996658325\n",
      "Batch loss: 4.454268455505371\n",
      "Original Input range: 0.0 85.22960662841797\n",
      "Output range: 0.17457477748394012 0.990574300289154\n",
      "Batch loss: 5.107363224029541\n",
      "Original Input range: 0.0 102.27552795410156\n",
      "Output range: 0.04186287149786949 0.9940476417541504\n",
      "Batch loss: 5.858098983764648\n",
      "Original Input range: 0.0 57.596832275390625\n",
      "Output range: 0.12444738298654556 0.9885104894638062\n",
      "Batch loss: 5.360300540924072\n",
      "Original Input range: 0.0 159.15589904785156\n",
      "Output range: 0.11019808799028397 0.9914147257804871\n",
      "Batch loss: 7.017977237701416\n",
      "Original Input range: 0.0 42.62110137939453\n",
      "Output range: 0.13977371156215668 0.9860203862190247\n",
      "Batch loss: 4.620865821838379\n",
      "Original Input range: 0.0 127.84440612792969\n",
      "Output range: 0.08355232328176498 0.9867836236953735\n",
      "Batch loss: 7.13474178314209\n",
      "Original Input range: 0.0 46.85770034790039\n",
      "Output range: 0.05214736983180046 0.9897897839546204\n",
      "Batch loss: 4.455454349517822\n",
      "Original Input range: 0.0 43.47346115112305\n",
      "Output range: 0.10205188393592834 0.9865868091583252\n",
      "Batch loss: 4.9319047927856445\n",
      "Original Input range: 0.0 40.13356018066406\n",
      "Output range: 0.12362890690565109 0.9833375811576843\n",
      "Batch loss: 4.816496849060059\n",
      "Original Input range: 0.0 82.15438842773438\n",
      "Output range: 0.08567698299884796 0.9947579503059387\n",
      "Batch loss: 5.710879325866699\n",
      "Original Input range: 0.0 37.85612869262695\n",
      "Output range: 0.06337551027536392 0.9809132218360901\n",
      "Batch loss: 4.973809242248535\n",
      "Original Input range: 0.0 47.55305099487305\n",
      "Output range: 0.09530163556337357 0.989168643951416\n",
      "Batch loss: 5.41804838180542\n",
      "Original Input range: 0.0 50.853153228759766\n",
      "Output range: 0.08737391978502274 0.9917033910751343\n",
      "Batch loss: 4.577531337738037\n",
      "Original Input range: 0.0 37.5518913269043\n",
      "Output range: 0.1587654948234558 0.9925641417503357\n",
      "Batch loss: 4.73268461227417\n",
      "Original Input range: 0.0 35.959510803222656\n",
      "Output range: 0.12191591411828995 0.9863269329071045\n",
      "Batch loss: 4.168545246124268\n",
      "Original Input range: 0.0 56.19416427612305\n",
      "Output range: 0.1891150176525116 0.9788585305213928\n",
      "Batch loss: 5.5955376625061035\n",
      "Original Input range: 0.0 66.27521514892578\n",
      "Output range: 0.09204893559217453 0.9779449105262756\n",
      "Batch loss: 5.855718612670898\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.12075154483318329 0.9844576716423035\n",
      "Batch loss: 7.7480082511901855\n",
      "Original Input range: 0.0 39.37679672241211\n",
      "Output range: 0.14961856603622437 0.9835494756698608\n",
      "Batch loss: 5.397632122039795\n",
      "Original Input range: 0.0 48.06983947753906\n",
      "Output range: 0.16232414543628693 0.9915185570716858\n",
      "Batch loss: 5.757359981536865\n",
      "Original Input range: 0.0 43.82943344116211\n",
      "Output range: 0.0886455774307251 0.9898279905319214\n",
      "Batch loss: 4.258626937866211\n",
      "Original Input range: 0.0 43.46803283691406\n",
      "Output range: 0.062438949942588806 0.9861180186271667\n",
      "Batch loss: 5.966633319854736\n",
      "Original Input range: 0.0 48.69768524169922\n",
      "Output range: 0.1258980929851532 0.9851497411727905\n",
      "Batch loss: 5.469701290130615\n",
      "Original Input range: 0.0 43.624542236328125\n",
      "Output range: 0.16684289276599884 0.9871876239776611\n",
      "Batch loss: 5.725089073181152\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.14679943025112152 0.9835330247879028\n",
      "Batch loss: 8.081145286560059\n",
      "Original Input range: 0.0 40.66942596435547\n",
      "Output range: 0.10208136588335037 0.987348198890686\n",
      "Batch loss: 4.9384942054748535\n",
      "Original Input range: 0.0 41.78765106201172\n",
      "Output range: 0.11625668406486511 0.9886488318443298\n",
      "Batch loss: 5.261651515960693\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.04590857774019241 0.9985982775688171\n",
      "Batch loss: 8.887807846069336\n",
      "Original Input range: 0.0 47.26083755493164\n",
      "Output range: 0.12156950682401657 0.9925892353057861\n",
      "Batch loss: 4.122042179107666\n",
      "Original Input range: 0.0 77.83159637451172\n",
      "Output range: 0.09005461633205414 0.9887803792953491\n",
      "Batch loss: 5.377613067626953\n",
      "Original Input range: 0.0 63.922203063964844\n",
      "Output range: 0.08438199758529663 0.9796725511550903\n",
      "Batch loss: 4.558475971221924\n",
      "Original Input range: 0.0 51.152488708496094\n",
      "Output range: 0.18675757944583893 0.9842889904975891\n",
      "Batch loss: 5.48876428604126\n",
      "Original Input range: 0.0 86.11009979248047\n",
      "Output range: 0.04368731752038002 0.9973195195198059\n",
      "Batch loss: 6.054061412811279\n",
      "Original Input range: 0.0 43.055049896240234\n",
      "Output range: 0.14004196226596832 0.9922549724578857\n",
      "Batch loss: 5.53372859954834\n",
      "Original Input range: 0.0 37.37712860107422\n",
      "Output range: 0.1276969164609909 0.9801499843597412\n",
      "Batch loss: 4.962626934051514\n",
      "Original Input range: 0.0 46.55131149291992\n",
      "Output range: 0.0055236173793673515 0.9953773021697998\n",
      "Batch loss: 5.416370391845703\n",
      "Original Input range: 0.0 51.19465637207031\n",
      "Output range: 0.06006327643990517 0.9825837016105652\n",
      "Batch loss: 5.856881618499756\n",
      "Original Input range: 0.0 80.46197509765625\n",
      "Output range: 0.15262369811534882 0.9958540201187134\n",
      "Batch loss: 6.158171653747559\n",
      "Original Input range: 0.0 51.13776397705078\n",
      "Output range: 0.13791154325008392 0.9886168837547302\n",
      "Batch loss: 6.498775959014893\n",
      "Original Input range: 0.0 83.86809539794922\n",
      "Output range: 0.11463598161935806 0.9912407994270325\n",
      "Batch loss: 6.628151893615723\n",
      "Original Input range: 0.0 64.35528564453125\n",
      "Output range: 0.1436099112033844 0.9839008450508118\n",
      "Batch loss: 6.111098766326904\n",
      "Original Input range: 0.0 47.97340393066406\n",
      "Output range: 0.10444068163633347 0.9860148429870605\n",
      "Batch loss: 6.297633171081543\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.1599084436893463 0.9920139908790588\n",
      "Batch loss: 7.667099952697754\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.12004031240940094 0.9931081533432007\n",
      "Batch loss: 6.713657855987549\n",
      "Original Input range: 0.0 57.333343505859375\n",
      "Output range: 0.12636838853359222 0.9864283204078674\n",
      "Batch loss: 5.391733646392822\n",
      "Original Input range: 0.0 59.92755126953125\n",
      "Output range: 0.15619249641895294 0.9862826466560364\n",
      "Batch loss: 5.21525239944458\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.14844714105129242 0.992618978023529\n",
      "Batch loss: 6.6956892013549805\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.15855197608470917 0.9901673793792725\n",
      "Batch loss: 5.8759989738464355\n",
      "Original Input range: 0.0 36.010013580322266\n",
      "Output range: 0.12490461766719818 0.9928646087646484\n",
      "Batch loss: 5.005699634552002\n",
      "Original Input range: 0.0 63.922203063964844\n",
      "Output range: 0.12822023034095764 0.9879688024520874\n",
      "Batch loss: 5.6812028884887695\n",
      "Original Input range: 0.0 39.83431625366211\n",
      "Output range: 0.16843536496162415 0.9774470329284668\n",
      "Batch loss: 4.9204607009887695\n",
      "Original Input range: 0.0 34.614356994628906\n",
      "Output range: 0.18859760463237762 0.9822106957435608\n",
      "Batch loss: 4.657345771789551\n",
      "Original Input range: 0.0 30.273244857788086\n",
      "Output range: 0.08419108390808105 0.9813756942749023\n",
      "Batch loss: 4.447270393371582\n",
      "Original Input range: 0.0 54.426517486572266\n",
      "Output range: 0.14620816707611084 0.9916608929634094\n",
      "Batch loss: 5.060151100158691\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.1541784107685089 0.9974950551986694\n",
      "Batch loss: 6.283177852630615\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.017812713980674744 0.9924665093421936\n",
      "Batch loss: 7.4725022315979\n",
      "Original Input range: 0.0 50.76498031616211\n",
      "Output range: 0.12177790701389313 0.9921779036521912\n",
      "Batch loss: 5.643526554107666\n",
      "Original Input range: 0.0 45.98609924316406\n",
      "Output range: 0.19216686487197876 0.9811246395111084\n",
      "Batch loss: 5.521259784698486\n",
      "Original Input range: 0.0 46.75095748901367\n",
      "Output range: 0.18288227915763855 0.9859333634376526\n",
      "Batch loss: 5.775018215179443\n",
      "Original Input range: 0.0 31.401899337768555\n",
      "Output range: 0.11581495404243469 0.9820929765701294\n",
      "Batch loss: 3.8165664672851562\n",
      "Original Input range: 0.0 40.40686798095703\n",
      "Output range: 0.14874021708965302 0.9820314645767212\n",
      "Batch loss: 4.6653642654418945\n",
      "Original Input range: 0.0 53.99897766113281\n",
      "Output range: 0.18343722820281982 0.9841377139091492\n",
      "Batch loss: 4.9748101234436035\n",
      "Original Input range: 0.0 49.05189514160156\n",
      "Output range: 0.05107242986559868 0.9883502721786499\n",
      "Batch loss: 5.254016399383545\n",
      "Validation Input range: 0.0 39.0533447265625\n",
      "Validation Output range: 0.22416791319847107 0.9799525141716003\n",
      "Validation Input range: 0.0 47.14912414550781\n",
      "Validation Output range: 0.1882818192243576 0.9806564450263977\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.1881616711616516 0.9775702953338623\n",
      "Validation Input range: 0.0 41.433265686035156\n",
      "Validation Output range: 0.23534928262233734 0.9776712656021118\n",
      "Validation Input range: 0.0 54.87662887573242\n",
      "Validation Output range: 0.14629411697387695 0.9759279489517212\n",
      "Validation Input range: 0.0 41.19639587402344\n",
      "Validation Output range: 0.15275128185749054 0.9771019816398621\n",
      "Validation Input range: 0.0 40.4886589050293\n",
      "Validation Output range: 0.047775574028491974 0.9785395860671997\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.18863163888454437 0.9800686836242676\n",
      "Validation Input range: 0.0 79.57794952392578\n",
      "Validation Output range: 0.2025117725133896 0.9753174781799316\n",
      "Validation Input range: 0.0 37.079376220703125\n",
      "Validation Output range: 0.22064127027988434 0.9771690964698792\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.15202108025550842 0.9873944520950317\n",
      "Validation Input range: 0.0 33.93391418457031\n",
      "Validation Output range: 0.22415171563625336 0.9775847792625427\n",
      "Validation Input range: 0.0 32.81044006347656\n",
      "Validation Output range: 0.20348383486270905 0.9762356877326965\n",
      "Validation Input range: 0.0 49.359073638916016\n",
      "Validation Output range: 0.22964070737361908 0.9794248342514038\n",
      "Validation Input range: 0.0 39.49129867553711\n",
      "Validation Output range: 0.19591446220874786 0.9755826592445374\n",
      "Validation Input range: 0.0 43.210540771484375\n",
      "Validation Output range: 0.010041152127087116 0.9784975051879883\n",
      "Validation Input range: 0.0 32.88222122192383\n",
      "Validation Output range: 0.19505392014980316 0.9766545295715332\n",
      "Validation Input range: 0.0 39.19929885864258\n",
      "Validation Output range: 0.22945387661457062 0.98165363073349\n",
      "Validation Input range: 0.0 52.02884292602539\n",
      "Validation Output range: 0.10494257509708405 0.9792911410331726\n",
      "Validation Input range: 0.0 42.88813781738281\n",
      "Validation Output range: 0.20275592803955078 0.9771010279655457\n",
      "Validation Input range: 0.0 30.61672592163086\n",
      "Validation Output range: 0.224983811378479 0.9733328819274902\n",
      "Epoch [8/10], Average Batch Training Loss: 5.625643461522921, Average Batch Validation Loss: 5.352473247618902\n",
      "Original Input range: 0.0 85.22960662841797\n",
      "Output range: 0.12808004021644592 0.99052494764328\n",
      "Batch loss: 5.237721920013428\n",
      "Original Input range: 0.0 51.33224868774414\n",
      "Output range: 0.1192423552274704 0.9877107739448547\n",
      "Batch loss: 5.696801662445068\n",
      "Original Input range: 0.0 50.853153228759766\n",
      "Output range: 0.1705520749092102 0.9856283664703369\n",
      "Batch loss: 5.411382675170898\n",
      "Original Input range: 0.0 86.11009979248047\n",
      "Output range: 0.18112871050834656 0.9797347187995911\n",
      "Batch loss: 5.813190460205078\n",
      "Original Input range: 0.0 51.517547607421875\n",
      "Output range: 0.17519649863243103 0.985047459602356\n",
      "Batch loss: 5.775451183319092\n",
      "Original Input range: 0.0 83.86809539794922\n",
      "Output range: 0.11862127482891083 0.993571400642395\n",
      "Batch loss: 5.777537822723389\n",
      "Original Input range: 0.0 48.32259750366211\n",
      "Output range: 0.00052456563571468 0.979649007320404\n",
      "Batch loss: 5.614260196685791\n",
      "Original Input range: 0.0 38.6131706237793\n",
      "Output range: 0.09515079855918884 0.9769715666770935\n",
      "Batch loss: 5.418002128601074\n",
      "Original Input range: 0.0 127.84440612792969\n",
      "Output range: 0.1058492511510849 0.989382803440094\n",
      "Batch loss: 5.587575435638428\n",
      "Original Input range: 0.0 38.30424118041992\n",
      "Output range: 0.18623122572898865 0.9877610206604004\n",
      "Batch loss: 5.299520015716553\n",
      "Original Input range: 0.0 64.35528564453125\n",
      "Output range: 0.08488962799310684 0.9824626445770264\n",
      "Batch loss: 4.892299175262451\n",
      "Original Input range: 0.0 41.78765106201172\n",
      "Output range: 0.06663838773965836 0.9860694408416748\n",
      "Batch loss: 5.076153755187988\n",
      "Original Input range: 0.0 28.936805725097656\n",
      "Output range: 0.13382013142108917 0.9843757748603821\n",
      "Batch loss: 4.460958957672119\n",
      "Original Input range: 0.0 85.24281311035156\n",
      "Output range: 0.14526130259037018 0.9894341826438904\n",
      "Batch loss: 5.927823066711426\n",
      "Original Input range: 0.0 159.15589904785156\n",
      "Output range: 0.12000378221273422 0.9934479594230652\n",
      "Batch loss: 6.332291603088379\n",
      "Original Input range: 0.0 41.53719711303711\n",
      "Output range: 0.12756481766700745 0.9858231544494629\n",
      "Batch loss: 5.350100040435791\n",
      "Original Input range: 0.0 51.152488708496094\n",
      "Output range: 0.0777897909283638 0.9886606931686401\n",
      "Batch loss: 4.90910530090332\n",
      "Original Input range: 0.0 54.94767379760742\n",
      "Output range: 0.09860924631357193 0.9847358465194702\n",
      "Batch loss: 4.838305950164795\n",
      "Original Input range: 0.0 40.142005920410156\n",
      "Output range: 0.09597020596265793 0.9808863997459412\n",
      "Batch loss: 4.841799736022949\n",
      "Original Input range: 0.0 64.54231262207031\n",
      "Output range: 0.1608068197965622 0.9907845258712769\n",
      "Batch loss: 4.776501178741455\n",
      "Original Input range: 0.0 63.922203063964844\n",
      "Output range: 0.09850486367940903 0.989452600479126\n",
      "Batch loss: 5.705129146575928\n",
      "Original Input range: 0.0 41.5326042175293\n",
      "Output range: 0.15784823894500732 0.9865826964378357\n",
      "Batch loss: 6.373392105102539\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.14440256357192993 0.9984731078147888\n",
      "Batch loss: 6.946911334991455\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.11296691000461578 0.9871549606323242\n",
      "Batch loss: 7.206772804260254\n",
      "Original Input range: 0.0 63.922203063964844\n",
      "Output range: 0.12661027908325195 0.9934262037277222\n",
      "Batch loss: 5.657401084899902\n",
      "Original Input range: 0.0 32.18483352661133\n",
      "Output range: 0.04878367856144905 0.9924513697624207\n",
      "Batch loss: 4.132725715637207\n",
      "Original Input range: 0.0 46.55131149291992\n",
      "Output range: 0.0870729461312294 0.9794685244560242\n",
      "Batch loss: 4.714524745941162\n",
      "Original Input range: 0.0 59.99742126464844\n",
      "Output range: 0.18452930450439453 0.9832674860954285\n",
      "Batch loss: 5.460506916046143\n",
      "Original Input range: 0.0 42.41950607299805\n",
      "Output range: 0.0875200405716896 0.9881066083908081\n",
      "Batch loss: 5.461727142333984\n",
      "Original Input range: 0.0 102.27552795410156\n",
      "Output range: 0.11553126573562622 0.9921873807907104\n",
      "Batch loss: 5.488934516906738\n",
      "Original Input range: 0.0 80.46197509765625\n",
      "Output range: 0.12757331132888794 0.9907410144805908\n",
      "Batch loss: 5.8716020584106445\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.20253978669643402 0.985175609588623\n",
      "Batch loss: 8.34175968170166\n",
      "Original Input range: 0.0 79.60464477539062\n",
      "Output range: 0.14481902122497559 0.9910491704940796\n",
      "Batch loss: 6.171901702880859\n",
      "Original Input range: 0.0 50.47058868408203\n",
      "Output range: 0.09514686465263367 0.9997453093528748\n",
      "Batch loss: 4.740167617797852\n",
      "Original Input range: 0.0 41.575199127197266\n",
      "Output range: 0.058650851249694824 0.9678515195846558\n",
      "Batch loss: 4.51561975479126\n",
      "Original Input range: 0.0 54.426517486572266\n",
      "Output range: 0.08783885836601257 0.972841739654541\n",
      "Batch loss: 5.849941730499268\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.08999330550432205 0.9889966249465942\n",
      "Batch loss: 8.155323028564453\n",
      "Original Input range: 0.0 42.5487174987793\n",
      "Output range: 0.11263802647590637 0.9864266514778137\n",
      "Batch loss: 5.244436740875244\n",
      "Original Input range: 0.0 30.954355239868164\n",
      "Output range: 0.10800226032733917 0.9805144667625427\n",
      "Batch loss: 5.061466217041016\n",
      "Original Input range: 0.0 43.11163330078125\n",
      "Output range: 0.16598597168922424 0.9812920093536377\n",
      "Batch loss: 6.099629878997803\n",
      "Original Input range: 0.0 46.321128845214844\n",
      "Output range: 0.01325935684144497 0.9872683882713318\n",
      "Batch loss: 6.113834381103516\n",
      "Original Input range: 0.0 79.5941390991211\n",
      "Output range: 0.17252527177333832 0.9896358847618103\n",
      "Batch loss: 5.771298885345459\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.13089711964130402 0.9860002398490906\n",
      "Batch loss: 5.802896976470947\n",
      "Original Input range: 0.0 82.15438842773438\n",
      "Output range: 0.08188032358884811 0.981109619140625\n",
      "Batch loss: 5.643867492675781\n",
      "Original Input range: 0.0 51.2689323425293\n",
      "Output range: 0.14601822197437286 0.9865154027938843\n",
      "Batch loss: 5.629274368286133\n",
      "Original Input range: 0.0 43.45826721191406\n",
      "Output range: 0.1830524504184723 0.9856341481208801\n",
      "Batch loss: 5.721030235290527\n",
      "Original Input range: 0.0 43.896060943603516\n",
      "Output range: 0.13068825006484985 0.9819931983947754\n",
      "Batch loss: 6.043644428253174\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.06913509219884872 0.9869836568832397\n",
      "Batch loss: 5.392625331878662\n",
      "Original Input range: 0.0 55.424163818359375\n",
      "Output range: 0.13265231251716614 0.9830819964408875\n",
      "Batch loss: 4.754037380218506\n",
      "Original Input range: 0.0 77.83159637451172\n",
      "Output range: 0.1643356829881668 0.9870162606239319\n",
      "Batch loss: 6.185049533843994\n",
      "Original Input range: 0.0 159.15589904785156\n",
      "Output range: 0.08466542512178421 0.9892996549606323\n",
      "Batch loss: 6.825699329376221\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.14077194035053253 0.9831027984619141\n",
      "Batch loss: 5.9195098876953125\n",
      "Original Input range: 0.0 33.08015441894531\n",
      "Output range: 0.10678524523973465 0.9774657487869263\n",
      "Batch loss: 4.6778717041015625\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.1432243287563324 0.9866438508033752\n",
      "Batch loss: 5.365017414093018\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.18754887580871582 0.9822054505348206\n",
      "Batch loss: 8.562074661254883\n",
      "Original Input range: 0.0 66.27521514892578\n",
      "Output range: 0.12415648251771927 0.9900087118148804\n",
      "Batch loss: 5.2884440422058105\n",
      "Original Input range: 0.0 51.13776397705078\n",
      "Output range: 0.047688551247119904 0.9910851120948792\n",
      "Batch loss: 4.085984230041504\n",
      "Original Input range: 0.0 57.596832275390625\n",
      "Output range: 0.09681308269500732 0.9813376665115356\n",
      "Batch loss: 5.816091060638428\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.16714957356452942 0.9825050234794617\n",
      "Batch loss: 8.107715606689453\n",
      "Original Input range: 0.0 42.27656555175781\n",
      "Output range: 0.10522162169218063 0.9915618300437927\n",
      "Batch loss: 4.822900772094727\n",
      "Original Input range: 0.0 59.37202835083008\n",
      "Output range: 0.15508514642715454 0.9830348491668701\n",
      "Batch loss: 5.444097995758057\n",
      "Original Input range: 0.0 54.78154373168945\n",
      "Output range: 0.17408038675785065 0.9796094298362732\n",
      "Batch loss: 4.792391300201416\n",
      "Original Input range: 0.0 53.052162170410156\n",
      "Output range: 0.15525056421756744 0.9872426986694336\n",
      "Batch loss: 5.876192092895508\n",
      "Original Input range: 0.0 46.75095748901367\n",
      "Output range: 0.018033219501376152 0.9878793358802795\n",
      "Batch loss: 5.113484859466553\n",
      "Original Input range: 0.0 47.97340393066406\n",
      "Output range: 0.12944014370441437 0.9856835007667542\n",
      "Batch loss: 5.192695140838623\n",
      "Original Input range: 0.0 53.051963806152344\n",
      "Output range: 0.16936153173446655 0.984341561794281\n",
      "Batch loss: 5.719454765319824\n",
      "Original Input range: 0.0 92.99876403808594\n",
      "Output range: 0.15184877812862396 0.9798058867454529\n",
      "Batch loss: 6.647576332092285\n",
      "Original Input range: 0.0 36.526973724365234\n",
      "Output range: 0.057735271751880646 0.9792854189872742\n",
      "Batch loss: 4.386528491973877\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.20003528892993927 0.9856260418891907\n",
      "Batch loss: 6.725247859954834\n",
      "Original Input range: 0.0 39.37679672241211\n",
      "Output range: 0.11881796270608902 0.9911574125289917\n",
      "Batch loss: 4.489978790283203\n",
      "Original Input range: 0.0 36.512420654296875\n",
      "Output range: 0.08801489323377609 0.996933102607727\n",
      "Batch loss: 3.9451534748077393\n",
      "Validation Input range: 0.0 39.0533447265625\n",
      "Validation Output range: 0.12598282098770142 0.9890021085739136\n",
      "Validation Input range: 0.0 47.14912414550781\n",
      "Validation Output range: 0.1058783233165741 0.987669050693512\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.11525652557611465 0.991441547870636\n",
      "Validation Input range: 0.0 41.433265686035156\n",
      "Validation Output range: 0.10503541678190231 0.988857090473175\n",
      "Validation Input range: 0.0 54.87662887573242\n",
      "Validation Output range: 0.14279724657535553 0.9891113638877869\n",
      "Validation Input range: 0.0 41.19639587402344\n",
      "Validation Output range: 0.1332968771457672 0.9905665516853333\n",
      "Validation Input range: 0.0 40.4886589050293\n",
      "Validation Output range: 0.01616540364921093 0.9890971183776855\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.12333959341049194 0.9894561171531677\n",
      "Validation Input range: 0.0 79.57794952392578\n",
      "Validation Output range: 0.15858805179595947 0.9887685775756836\n",
      "Validation Input range: 0.0 37.079376220703125\n",
      "Validation Output range: 0.12980619072914124 0.98887699842453\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.15419109165668488 0.9918274879455566\n",
      "Validation Input range: 0.0 33.93391418457031\n",
      "Validation Output range: 0.1461639106273651 0.9885900616645813\n",
      "Validation Input range: 0.0 32.81044006347656\n",
      "Validation Output range: 0.1397249698638916 0.9863276481628418\n",
      "Validation Input range: 0.0 49.359073638916016\n",
      "Validation Output range: 0.11074556410312653 0.9890415072441101\n",
      "Validation Input range: 0.0 39.49129867553711\n",
      "Validation Output range: 0.10847748070955276 0.9889089465141296\n",
      "Validation Input range: 0.0 43.210540771484375\n",
      "Validation Output range: 0.011309546418488026 0.9887400269508362\n",
      "Validation Input range: 0.0 32.88222122192383\n",
      "Validation Output range: 0.11055004596710205 0.987832248210907\n",
      "Validation Input range: 0.0 39.19929885864258\n",
      "Validation Output range: 0.12208009511232376 0.9890850186347961\n",
      "Validation Input range: 0.0 52.02884292602539\n",
      "Validation Output range: 0.08019659668207169 0.9883370995521545\n",
      "Validation Input range: 0.0 42.88813781738281\n",
      "Validation Output range: 0.11799599230289459 0.9881286025047302\n",
      "Validation Input range: 0.0 30.61672592163086\n",
      "Validation Output range: 0.1462440937757492 0.9867008328437805\n",
      "Epoch [9/10], Average Batch Training Loss: 5.621103169212879, Average Batch Validation Loss: 5.293933005560012\n",
      "Original Input range: 0.0 127.84440612792969\n",
      "Output range: 0.13162454962730408 0.9814687967300415\n",
      "Batch loss: 6.1708455085754395\n",
      "Original Input range: 0.0 31.02065658569336\n",
      "Output range: 0.0912020206451416 0.9773916006088257\n",
      "Batch loss: 4.071922302246094\n",
      "Original Input range: 0.0 56.19416427612305\n",
      "Output range: 0.15904086828231812 0.9840048551559448\n",
      "Batch loss: 5.845841407775879\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.13309593498706818 0.9881941080093384\n",
      "Batch loss: 8.301651000976562\n",
      "Original Input range: 0.0 42.655887603759766\n",
      "Output range: 0.16664566099643707 0.9870147705078125\n",
      "Batch loss: 5.67838716506958\n",
      "Original Input range: 0.0 92.99876403808594\n",
      "Output range: 0.1586301326751709 0.9846310019493103\n",
      "Batch loss: 6.152797222137451\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.15120668709278107 0.9821133017539978\n",
      "Batch loss: 5.87405252456665\n",
      "Original Input range: 0.0 43.46803283691406\n",
      "Output range: 0.16498681902885437 0.9843584299087524\n",
      "Batch loss: 4.313111305236816\n",
      "Original Input range: 0.0 43.624542236328125\n",
      "Output range: 0.09142649918794632 0.9851068258285522\n",
      "Batch loss: 4.423763275146484\n",
      "Original Input range: 0.0 31.254064559936523\n",
      "Output range: 0.1403086632490158 0.9835303425788879\n",
      "Batch loss: 4.358707904815674\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.2024383544921875 0.9919821619987488\n",
      "Batch loss: 6.232607364654541\n",
      "Original Input range: 0.0 43.82943344116211\n",
      "Output range: 0.014500411227345467 0.9849637150764465\n",
      "Batch loss: 4.877292633056641\n",
      "Original Input range: 0.0 40.5273323059082\n",
      "Output range: 0.03928406536579132 0.9861576557159424\n",
      "Batch loss: 5.1482415199279785\n",
      "Original Input range: 0.0 59.99742126464844\n",
      "Output range: 0.19904805719852448 0.9831363558769226\n",
      "Batch loss: 6.112945079803467\n",
      "Original Input range: 0.0 51.33224868774414\n",
      "Output range: 0.028297673910856247 0.9894725680351257\n",
      "Batch loss: 5.245494365692139\n",
      "Original Input range: 0.0 42.355018615722656\n",
      "Output range: 0.17067627608776093 0.9850752353668213\n",
      "Batch loss: 4.49155330657959\n",
      "Original Input range: 0.0 85.22960662841797\n",
      "Output range: 0.13715022802352905 0.990391731262207\n",
      "Batch loss: 6.6359100341796875\n",
      "Original Input range: 0.0 50.76498031616211\n",
      "Output range: 0.14917241036891937 0.9863609075546265\n",
      "Batch loss: 5.057552814483643\n",
      "Original Input range: 0.0 43.47346115112305\n",
      "Output range: 0.17125843465328217 0.9866722822189331\n",
      "Batch loss: 5.121785640716553\n",
      "Original Input range: 0.0 57.333343505859375\n",
      "Output range: 0.18041016161441803 0.9862344264984131\n",
      "Batch loss: 5.470669269561768\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.1840566098690033 0.9911463856697083\n",
      "Batch loss: 8.142797470092773\n",
      "Original Input range: 0.0 54.8901252746582\n",
      "Output range: 0.11711766570806503 0.9879330992698669\n",
      "Batch loss: 5.181155204772949\n",
      "Original Input range: 0.0 59.37202835083008\n",
      "Output range: 0.1881176084280014 0.9832430481910706\n",
      "Batch loss: 5.936537265777588\n",
      "Original Input range: 0.0 42.5487174987793\n",
      "Output range: 0.17292046546936035 0.9773982167243958\n",
      "Batch loss: 5.401500701904297\n",
      "Original Input range: 0.0 38.72101593017578\n",
      "Output range: 0.06417025625705719 0.9829692840576172\n",
      "Batch loss: 4.49221134185791\n",
      "Original Input range: 0.0 255.68881225585938\n",
      "Output range: 0.1145598292350769 0.9972273707389832\n",
      "Batch loss: 7.271203994750977\n",
      "Original Input range: 0.0 54.426517486572266\n",
      "Output range: 0.1189734935760498 0.9811276197433472\n",
      "Batch loss: 4.723137855529785\n",
      "Original Input range: 0.0 51.13776397705078\n",
      "Output range: 0.11427944898605347 0.9840410947799683\n",
      "Batch loss: 5.11228609085083\n",
      "Original Input range: 0.0 80.46197509765625\n",
      "Output range: 0.18616025149822235 0.9938651919364929\n",
      "Batch loss: 5.98128080368042\n",
      "Original Input range: 0.0 50.34861373901367\n",
      "Output range: 0.1681676059961319 0.9903638958930969\n",
      "Batch loss: 5.334454536437988\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.1852300614118576 0.9856435060501099\n",
      "Batch loss: 6.319927215576172\n",
      "Original Input range: 0.0 27.721778869628906\n",
      "Output range: 0.14025187492370605 0.9882203340530396\n",
      "Batch loss: 3.98728084564209\n",
      "Original Input range: 0.0 54.78456115722656\n",
      "Output range: 0.03576977178454399 0.9764897227287292\n",
      "Batch loss: 4.950107574462891\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.18477821350097656 0.9883495569229126\n",
      "Batch loss: 6.926034927368164\n",
      "Original Input range: 0.0 33.668190002441406\n",
      "Output range: 0.1732456386089325 0.9859644174575806\n",
      "Batch loss: 5.078392028808594\n",
      "Original Input range: 0.0 38.87540054321289\n",
      "Output range: 0.17327015101909637 0.9835202097892761\n",
      "Batch loss: 5.297756195068359\n",
      "Original Input range: 0.0 102.27552795410156\n",
      "Output range: 0.14866919815540314 0.9863221645355225\n",
      "Batch loss: 5.6303887367248535\n",
      "Original Input range: 0.0 47.90782928466797\n",
      "Output range: 0.19650983810424805 0.9771106243133545\n",
      "Batch loss: 6.093568801879883\n",
      "Original Input range: 0.0 45.98609924316406\n",
      "Output range: 0.03140638396143913 0.9801537990570068\n",
      "Batch loss: 5.298743724822998\n",
      "Original Input range: 0.0 41.78765106201172\n",
      "Output range: 0.21076864004135132 0.9829868078231812\n",
      "Batch loss: 5.155645847320557\n",
      "Original Input range: 0.0 50.47058868408203\n",
      "Output range: 0.12804284691810608 0.9914131164550781\n",
      "Batch loss: 4.8462629318237305\n",
      "Original Input range: 0.0 38.6131706237793\n",
      "Output range: 0.08548510074615479 0.9884151220321655\n",
      "Batch loss: 4.863114833831787\n",
      "Original Input range: 0.0 53.052162170410156\n",
      "Output range: 0.17307975888252258 0.9895323514938354\n",
      "Batch loss: 5.265896320343018\n",
      "Original Input range: 0.0 48.32259750366211\n",
      "Output range: 0.18080177903175354 0.9851381778717041\n",
      "Batch loss: 5.760646820068359\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.16904515027999878 0.9837107062339783\n",
      "Batch loss: 6.6969404220581055\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.08825501799583435 0.9924144148826599\n",
      "Batch loss: 5.8496270179748535\n",
      "Original Input range: 0.0 85.22960662841797\n",
      "Output range: 0.13226468861103058 0.9902094602584839\n",
      "Batch loss: 6.021196365356445\n",
      "Original Input range: 0.0 82.15438842773438\n",
      "Output range: 0.14355234801769257 0.9910940527915955\n",
      "Batch loss: 6.404432773590088\n",
      "Original Input range: 0.0 40.91291427612305\n",
      "Output range: 0.13050906360149384 0.9842538833618164\n",
      "Batch loss: 4.550992012023926\n",
      "Original Input range: 0.0 77.83159637451172\n",
      "Output range: 0.13869822025299072 0.9863536357879639\n",
      "Batch loss: 6.009212970733643\n",
      "Original Input range: 0.0 64.54231262207031\n",
      "Output range: 0.15326403081417084 0.9859322309494019\n",
      "Batch loss: 5.269988536834717\n",
      "Original Input range: 0.0 42.62110137939453\n",
      "Output range: 0.09479960799217224 0.981842041015625\n",
      "Batch loss: 4.58090877532959\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.10560762882232666 0.9956552982330322\n",
      "Batch loss: 6.875731468200684\n",
      "Original Input range: 0.0 53.99897766113281\n",
      "Output range: 0.2119917869567871 0.9847326874732971\n",
      "Batch loss: 6.163036823272705\n",
      "Original Input range: 0.0 160.9239501953125\n",
      "Output range: 0.11718952655792236 0.9905005097389221\n",
      "Batch loss: 7.823237419128418\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.11675557494163513 0.9927577972412109\n",
      "Batch loss: 5.886688232421875\n",
      "Original Input range: 0.0 68.46208953857422\n",
      "Output range: 0.09421445429325104 0.9838046431541443\n",
      "Batch loss: 6.0637736320495605\n",
      "Original Input range: 0.0 36.435359954833984\n",
      "Output range: 0.09796052426099777 0.9827314019203186\n",
      "Batch loss: 4.601363182067871\n",
      "Original Input range: 0.0 30.584999084472656\n",
      "Output range: 0.19448891282081604 0.9829069972038269\n",
      "Batch loss: 4.613153457641602\n",
      "Original Input range: 0.0 53.641319274902344\n",
      "Output range: 0.20392867922782898 0.9788550734519958\n",
      "Batch loss: 6.839640140533447\n",
      "Original Input range: 0.0 54.78154373168945\n",
      "Output range: 0.056854452937841415 0.9934759736061096\n",
      "Batch loss: 5.337641716003418\n",
      "Original Input range: 0.0 159.15589904785156\n",
      "Output range: 0.16031984984874725 0.9873994588851929\n",
      "Batch loss: 6.984559535980225\n",
      "Original Input range: 0.0 86.11009979248047\n",
      "Output range: 0.11615002900362015 0.9810628294944763\n",
      "Batch loss: 6.363586902618408\n",
      "Original Input range: 0.0 79.57794952392578\n",
      "Output range: 0.10742897540330887 0.9870359301567078\n",
      "Batch loss: 5.439408779144287\n",
      "Original Input range: 0.0 79.60464477539062\n",
      "Output range: 0.14201566576957703 0.9919288158416748\n",
      "Batch loss: 5.699309825897217\n",
      "Original Input range: 0.0 36.1522102355957\n",
      "Output range: 0.104169562458992 0.9861419796943665\n",
      "Batch loss: 4.920918941497803\n",
      "Original Input range: 0.0 43.055049896240234\n",
      "Output range: 0.04074013978242874 0.9846565127372742\n",
      "Batch loss: 4.510770797729492\n",
      "Original Input range: 0.0 80.46197509765625\n",
      "Output range: 0.15421727299690247 0.9920742511749268\n",
      "Batch loss: 5.547298431396484\n",
      "Original Input range: 0.0 37.768489837646484\n",
      "Output range: 0.1424577385187149 0.987087607383728\n",
      "Batch loss: 5.397071361541748\n",
      "Original Input range: 0.0 63.922203063964844\n",
      "Output range: 0.192968487739563 0.9883641004562378\n",
      "Batch loss: 6.409229278564453\n",
      "Original Input range: 0.0 50.853153228759766\n",
      "Output range: 0.18164420127868652 0.9913110733032227\n",
      "Batch loss: 5.322473049163818\n",
      "Validation Input range: 0.0 39.0533447265625\n",
      "Validation Output range: 0.09021846950054169 0.9867628216743469\n",
      "Validation Input range: 0.0 47.14912414550781\n",
      "Validation Output range: 0.0669722855091095 0.9913129210472107\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.08077620714902878 0.9908024072647095\n",
      "Validation Input range: 0.0 41.433265686035156\n",
      "Validation Output range: 0.06419889628887177 0.9891073107719421\n",
      "Validation Input range: 0.0 54.87662887573242\n",
      "Validation Output range: 0.06549189239740372 0.988688051700592\n",
      "Validation Input range: 0.0 41.19639587402344\n",
      "Validation Output range: 0.07210252434015274 0.988476037979126\n",
      "Validation Input range: 0.0 40.4886589050293\n",
      "Validation Output range: 0.0029592840000987053 0.9877819418907166\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.07605306804180145 0.9890461564064026\n",
      "Validation Input range: 0.0 79.57794952392578\n",
      "Validation Output range: 0.11453035473823547 0.9889340996742249\n",
      "Validation Input range: 0.0 37.079376220703125\n",
      "Validation Output range: 0.09181667119264603 0.9880796074867249\n",
      "Validation Input range: 0.0 159.15589904785156\n",
      "Validation Output range: 0.12868118286132812 0.997633695602417\n",
      "Validation Input range: 0.0 33.93391418457031\n",
      "Validation Output range: 0.09789847582578659 0.9853531718254089\n",
      "Validation Input range: 0.0 32.81044006347656\n",
      "Validation Output range: 0.09864339232444763 0.9891136884689331\n",
      "Validation Input range: 0.0 49.359073638916016\n",
      "Validation Output range: 0.08416412025690079 0.9882211089134216\n",
      "Validation Input range: 0.0 39.49129867553711\n",
      "Validation Output range: 0.0995168462395668 0.9875556826591492\n",
      "Validation Input range: 0.0 43.210540771484375\n",
      "Validation Output range: 0.005622587166726589 0.9879796504974365\n",
      "Validation Input range: 0.0 32.88222122192383\n",
      "Validation Output range: 0.06304729729890823 0.9879392385482788\n",
      "Validation Input range: 0.0 39.19929885864258\n",
      "Validation Output range: 0.08810063451528549 0.9890256524085999\n",
      "Validation Input range: 0.0 52.02884292602539\n",
      "Validation Output range: 0.03517469763755798 0.9877992868423462\n",
      "Validation Input range: 0.0 42.88813781738281\n",
      "Validation Output range: 0.056782010942697525 0.9889172315597534\n",
      "Validation Input range: 0.0 30.61672592163086\n",
      "Validation Output range: 0.13504524528980255 0.9790514707565308\n",
      "Epoch [10/10], Average Batch Training Loss: 5.617150078357105, Average Batch Validation Loss: 5.2784943126496815\n",
      "Number of unique (playerId, teamId, season) combinations: 100\n",
      "Total number of latent vectors: 213\n",
      "Average MRR: 0.0175\n",
      "Average Top-1 Accuracy: 0.0094\n",
      "Average Top-3 Accuracy: 0.0235\n",
      "Average Top-5 Accuracy: 0.0235\n",
      "Average Top-10 Accuracy: 0.0469\n",
      "Execution time: 270.44 seconds\n"
     ]
    }
   ],
   "source": [
    "# Training with grid search and evaluation on MSE and Retrieval Task\n",
    "# Here the goal is to find the best hyperparameters to then use them to train the final (large) model\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "k_values = [1, 3, 5, 10]  # List of k values\n",
    "\n",
    "# Do this once before the main loop, since hm_tensor is the same in all the runs\n",
    "for playerId, teamId, season, heatmaps in filtered_test_instances:\n",
    "        vectors_with_ids = []\n",
    "        for heatmaps_id, heatmap in heatmaps:\n",
    "            # Convert heatmap to tensor and move to the device\n",
    "            hm_tensor = torch.tensor(heatmap, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "results = []\n",
    "num_epochs = 10\n",
    "\n",
    "start_time = time.time()\n",
    "# Grid Search Loop\n",
    "for lr, bottleneck, optimizer_name, loss_name, batch_size, weight_decay, dropout_rate in product(\n",
    "    learning_rates, bottleneck_sizes, optimizers, loss_functions, batch_sizes, weight_decays, dropout_rates\n",
    "):\n",
    "    # Prepare DataLoader\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    autoencoder = Autoencoder(input_channels=1, bottleneck_size=bottleneck, dropout_rate=dropout_rate).to(device)\n",
    "    \n",
    "    # Get optimizer and loss function\n",
    "    optimizer = get_optimizer(optimizer_name, autoencoder.parameters(), lr, weight_decay)\n",
    "    criterion = get_loss_function(loss_name)\n",
    "\n",
    "    # skipping creating latent vectors for training and validation data, since here I am just looking for the best hyperparameters\n",
    "    # I may need to do this step when I'll train the final model and I will need only the training latent vectors, since the validation set will not exist (already found the best hyperparameters)\n",
    "    # latent_vectors_train = {}  # Store latent vectors for training data\n",
    "\n",
    "    # Train model\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    print(f\"Training model with parameters: \\nLearning Rate: {lr}, Bottleneck: {bottleneck}, Optimizer: {optimizer_name}, Loss Function: {loss_name},\\n Batch Size: {batch_size}, Weight Decay: {weight_decay}, Dropout Rate: {dropout_rate}\")\n",
    "    for epoch in range(num_epochs):\n",
    "        autoencoder.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            images, ids, playerIds, teamIds, seasons = batch  # Unpack images and metadata\n",
    "            images = images.to(device)\n",
    "            input_max = images.max().item()\n",
    "            input_min = images.min().item()\n",
    "            print(\"Original Input range:\", input_min, input_max)  # Debug input range\n",
    "            #forward pass \n",
    "            reconstructed = autoencoder(images)\n",
    "            output_max = reconstructed.max().item()\n",
    "            output_min = reconstructed.min().item()\n",
    "            print(\"Output range:\", output_min, output_max)  # Debug output range\n",
    "            \n",
    "            #reconstructed = reconstructed * (input_max / output_max) # rescale in the input range\n",
    "            #print(\"Output range rescaled:\", reconstructed.min().item(), reconstructed.max().item())  # Debug output range\n",
    "            # maybe here I could rescale input and output in [0,1], before computing the loss\n",
    "            # so I don't run into the vanishing gradient problem and at the same time the loss doesn't explode\n",
    "            #images = (images - input_min) / (input_max - input_min) # [0, 1] scaling\n",
    "            #print(\"[0,1] Input range:\", images.min().item(), images.max().item())  # Debug input range\n",
    "            #output_max = reconstructed.max().item() #compute new values after rescaling\n",
    "            #output_min = reconstructed.min().item() #compute new values after rescaling\n",
    "            #reconstructed = (reconstructed - output_min) / (output_max - output_min) # [0, 1] scaling\n",
    "            #print(\"[0,1] Output range:\", reconstructed.min().item(), reconstructed.max().item())  # Debug output range\n",
    "            loss = criterion(reconstructed, images) \n",
    "            print(\"Batch loss:\", loss.item())  # Debug batch loss\n",
    "            #backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        #print(f\"Epoch [{epoch+1}/{num_epochs}], Average Batch Training Loss: {train_loss}\")\n",
    "        \n",
    "        # Validate model\n",
    "        autoencoder.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images, ids, playerIds, teamIds, seasons = batch # Unpack images and metadata\n",
    "                images = images.to(device)\n",
    "                input_max = images.max().item()\n",
    "                input_min = images.min().item()\n",
    "                print(\"Validation Input range:\", input_min, input_max)  # Debug input range\n",
    "                reconstructed = autoencoder(images)\n",
    "                output_max = reconstructed.max().item()\n",
    "                output_min = reconstructed.min().item()\n",
    "                print(\"Validation Output range:\", output_min, output_max)  # Debug output range\n",
    "                #reconstructed = (reconstructed - output_min) / (output_max - output_min) # [0, 1] scaling  \n",
    "                #reconstructed = reconstructed * (input_max / output_max) # rescale in the input range\n",
    "                #print(\"Validation Output range rescaled:\", reconstructed.min().item(), reconstructed.max().item())  # Debug output range\n",
    "                #images = (images - input_min) / (input_max - input_min) # [0, 1] scaling\n",
    "                #print(\"Input range [0,1]:\", images.min().item(), images.max().item())  # Debug input range\n",
    "                #output_max = reconstructed.max().item() #compute new values after rescaling\n",
    "                #output_min = reconstructed.min().item() #compute new values after rescaling\n",
    "                #reconstructed = (reconstructed - output_min) / (output_max - output_min) # [0, 1] scaling\n",
    "                #print(\"Output range [0,1]:\", reconstructed.min().item(), reconstructed.max().item())  # Debug output range\n",
    "                val_loss += criterion(reconstructed, images).item() \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "\n",
    "        # Check early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Batch Training Loss: {train_loss}, Average Batch Validation Loss: {val_loss}\")\n",
    "\n",
    "    # Create embeddings for test instances, to compute MMR and top-k metrics\n",
    "    latent_vectors = {}\n",
    "    for playerId, teamId, season, heatmaps in filtered_test_instances:\n",
    "        vectors_with_ids = []\n",
    "        for heatmaps_id, heatmap in heatmaps:\n",
    "            # Convert heatmap to tensor and move to the device\n",
    "            #hm_tensor = torch.tensor(heatmap, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            embedding, _ = autoencoder.encoder(hm_tensor)  # Get the bottleneck representation, ignore the indices_list\n",
    "            vector = embedding.detach().cpu().numpy()  # Detach and convert to NumPy array\n",
    "            # Append tuple of heatmapsId and latent vector\n",
    "            vectors_with_ids.append((heatmaps_id, vector))\n",
    "        latent_vectors[(playerId, teamId, season)] = vectors_with_ids\n",
    "\n",
    "    num_keys = len(latent_vectors)\n",
    "    print(f\"Number of unique (playerId, teamId, season) combinations: {num_keys}\")\n",
    "    total_latent_vectors = sum(len(vectors) for vectors in latent_vectors.values())\n",
    "    print(f\"Total number of latent vectors: {total_latent_vectors}\")\n",
    "\n",
    "    data_retrieval_results = []\n",
    "\n",
    "    for (playerId, teamId, season), vectors_with_ids in latent_vectors.items():\n",
    "        # Loop over each query vector for the current (playerId, teamId, season)\n",
    "        for i, (heatmaps_id, query_vector) in enumerate(vectors_with_ids):\n",
    "            # Compute similarity between the query vector and all other vectors\n",
    "            similarities = []\n",
    "            for (other_playerId, other_teamId, other_season), other_vectors_with_ids in latent_vectors.items():\n",
    "                for other_heatmaps_id, other_vector in other_vectors_with_ids:\n",
    "                    # Skip comparison if the heatmap is the same (i.e., avoid comparing the vector with itself)\n",
    "                    if other_heatmaps_id == heatmaps_id:\n",
    "                        continue\n",
    "                    # Compute cosine similarity between query vector and other vector\n",
    "                    sim = cosine_similarity(query_vector.reshape(1, -1), other_vector.reshape(1, -1))[0][0]\n",
    "                    similarities.append((sim, (other_playerId, other_teamId, other_season)))\n",
    "            \n",
    "            # Sort by similarity in descending order\n",
    "            similarities.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "            # Evaluate rankings for each k\n",
    "            ranks = {k: [key == (playerId, teamId, season) for _, key in similarities[:k]] for k in k_values}\n",
    "            \n",
    "            # Calculate top-k accuracy for each k value\n",
    "            top_k_values = {k: 1 if any(ranks[k]) else 0 for k in k_values}\n",
    "\n",
    "            # Calculate MRR\n",
    "            mrr = 1 / (ranks[10].index(True) + 1) if any(ranks[10]) else 0  # MRR for k=10 \n",
    "\n",
    "            data_retrieval_results.append({\n",
    "                'query': (playerId, teamId, season),\n",
    "                'mrr': mrr,\n",
    "                'top_k_values': top_k_values,  # Dictionary with top-k accuracy for each k\n",
    "            })\n",
    "\n",
    "    # Calculate averages for each k\n",
    "    avg_mrr = np.mean([retrieval_result['mrr'] for retrieval_result in data_retrieval_results])\n",
    "    avg_top_k_values = {k: np.mean([retrieval_result['top_k_values'][k] for retrieval_result in data_retrieval_results]) for k in k_values}\n",
    "\n",
    "    # Save result for the current configuration\n",
    "    hyperparams = {\n",
    "        \"learning_rate\": lr,\n",
    "        \"bottleneck_size\": bottleneck,\n",
    "        \"optimizer\": optimizer_name,\n",
    "        \"loss_function\": loss_name,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"dropout_rate\": dropout_rate\n",
    "    }\n",
    "    metrics = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"epochs_run\": epoch + 1,\n",
    "        \"avg_mrr\": avg_mrr\n",
    "    }\n",
    "\n",
    "    print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "    # Add avg_top_k values for each k to the metrics\n",
    "    for k, avg_top_k in avg_top_k_values.items():\n",
    "        metrics[f\"avg_top_{k}\"] = avg_top_k\n",
    "        print(f\"Average Top-{k} Accuracy: {avg_top_k:.4f}\")\n",
    "\n",
    "    save_result(hyperparams, metrics)\n",
    "    \n",
    "    # Save results to file after every configuration\n",
    "    with open(\"results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_set = HeatmapCSVLoader(csv_path=\"_Data-split/passes-start-test.csv\")\n",
    "test_loader = DataLoader(test_set, batch_size=16, shuffle=False)\n",
    "\n",
    "# Get a batch of images\n",
    "autoencoder.eval()\n",
    "images, ids, playerIds, teamIds, seasons = next(iter(test_loader))\n",
    "images = images.to(device)  # Move images to the selected device\n",
    "\n",
    "# Reconstruct images\n",
    "with torch.no_grad():\n",
    "    reconstructed = autoencoder(images)\n",
    "\n",
    "# Plot original and reconstructed images (limit to 5 for better visualization)\n",
    "num_examples = min(len(images), 5)  # Display up to 5 examples\n",
    "fig, axes = plt.subplots(2, num_examples, figsize=(15, 5))\n",
    "for i in range(num_examples):\n",
    "    # Original\n",
    "    axes[0, i].imshow(images[i].cpu().squeeze(), cmap='gray')  # Move back to CPU for plotting\n",
    "    if isinstance(ids, (list, tuple)):\n",
    "        axes[0, i].set_title(f\"Original ({ids[i]})\")\n",
    "    else:\n",
    "        axes[0, i].set_title(\"Original\")\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Reconstructed\n",
    "    axes[1, i].imshow(reconstructed[i].cpu().squeeze(), cmap='gray')  # Move back to CPU for plotting\n",
    "    axes[1, i].set_title(\"Reconstructed\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Grid Search\n",
    "results = []\n",
    "num_epochs = 15\n",
    "for lr, bottleneck, optimizer_name, loss_name, batch_size, weight_decay, dropout_rate in product(\n",
    "    learning_rates, bottleneck_sizes, optimizers, loss_functions, batch_sizes, weight_decays, dropout_rates\n",
    "):\n",
    "    # Prepare DataLoader\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    autoencoder = Autoencoder(input_channels=1, bottleneck_size=bottleneck, dropout_rate=dropout_rate).to(device)\n",
    "    \n",
    "    # Get optimizer and loss function\n",
    "    optimizer = get_optimizer(optimizer_name, autoencoder.parameters(), lr, weight_decay)\n",
    "    criterion = get_loss_function(loss_name)\n",
    "    \n",
    "    # Train model\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    print(f\"Training model with parameters: \\nLearning Rate: {lr}, Bottleneck: {bottleneck}, Optimizer: {optimizer_name}, Loss Function: {loss_name},\\n Batch Size: {batch_size}, Weight Decay: {weight_decay}, Dropout Rate: {dropout_rate}\")\n",
    "    for epoch in range(num_epochs):\n",
    "        autoencoder.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            images, _ = batch  # Unpack images and ignore heatmap IDs\n",
    "            images = images.to(device)\n",
    "            \n",
    "            #forward pass \n",
    "            reconstructed = autoencoder(images)\n",
    "            loss = criterion(reconstructed, images)\n",
    "            #backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        #print(f\"Epoch [{epoch+1}/{num_epochs}], Average Batch Training Loss: {train_loss}\")\n",
    "        \n",
    "        # Validate model\n",
    "        autoencoder.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images, ids = batch  # Unpack images and heatmap IDs\n",
    "                images = images.to(device)\n",
    "                reconstructed = autoencoder(images)\n",
    "                val_loss += criterion(reconstructed, images).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "\n",
    "        # Check early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Batch Training Loss: {train_loss}, Average Batch Validation Loss: {val_loss}\")\n",
    "\n",
    "    # Save result for the current configuration\n",
    "    hyperparams = {\n",
    "        \"learning_rate\": lr,\n",
    "        \"bottleneck_size\": bottleneck,\n",
    "        \"optimizer\": optimizer_name,\n",
    "        \"loss_function\": loss_name,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"dropout_rate\": dropout_rate\n",
    "    }\n",
    "    metrics = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"epochs_run\": epoch + 1\n",
    "    }\n",
    "    save_result(hyperparams, metrics)\n",
    "    \n",
    "    # Save results to file after every configuration\n",
    "    with open(\"results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best hyperparameters\n",
    "best_config = min(results, key=lambda x: x['val_loss'])\n",
    "print(f\"Best configuration: {best_config}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NBA_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
