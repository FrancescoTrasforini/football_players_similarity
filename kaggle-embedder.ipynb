{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10325089,"sourceType":"datasetVersion","datasetId":6392875},{"sourceId":10327286,"sourceType":"datasetVersion","datasetId":6394433},{"sourceId":10333207,"sourceType":"datasetVersion","datasetId":6398199}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Create train, validation and test splits\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# Specify the columns you want to read\ncolumns_to_read = ['heatmapsId', 'playerId', 'teamId', 'season', 'passes-start-heatmap']\n\n# Read the CSV file with only the specified columns\ndata = pd.read_csv('_normalized-heatmaps/normalized-heatmaps-chunk-1.csv', usecols=columns_to_read)\n\n# Split into train, validation, and test sets\ntrain_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)  # 70% train\nval_data, test_data = train_test_split(temp_data, test_size=0.33, random_state=42)  # 20% val, 10% test\n\n# Save to CSV\ntrain_data.to_csv(\"_Data-split/passes-start-train.csv\", index=False)\nval_data.to_csv(\"_Data-split/passes-start-validation.csv\", index=False)\ntest_data.to_csv(\"_Data-split/passes-start-test.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test with normalization [0,1]\n\nimport pandas as pd\nimport numpy as np\nimport json\nfrom sklearn.model_selection import train_test_split\n\n# Specify the columns you want to read\ncolumns_to_read = ['heatmapsId', 'playerId', 'teamId', 'season', 'passes-start-heatmap']\n\n# Read the CSV file with only the specified columns\ndata = pd.read_csv('_normalized-heatmaps/normalized-heatmaps-chunk-1.csv', usecols=columns_to_read)\n\n# Normalize the heatmap values\ndef normalize_heatmap(heatmap_str, range_min=-1, range_max=1):\n    # Convert the heatmap string to a numpy array\n    heatmap = np.array(eval(heatmap_str))  # Assuming the heatmap is stored as a string of lists\n\n    # Normalize to the desired range\n    heatmap_min, heatmap_max = heatmap.min(), heatmap.max()\n    #print(f\"Min: {heatmap_min}\\nMax: {heatmap_max}\")\n    normalized_heatmap = ((heatmap - heatmap_min) / (heatmap_max - heatmap_min))  # [0, 1] scaling\n    \n    return normalized_heatmap.tolist()  # Convert back to list\n\n# Apply normalization and save as JSON strings\ndata['passes-start-heatmap'] = data['passes-start-heatmap'].apply(\n    lambda x: json.dumps(normalize_heatmap(x))\n)\n\n# Split into train, validation, and test sets\ntrain_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)  # 70% train\nval_data, test_data = train_test_split(temp_data, test_size=0.33, random_state=42)  # 20% val, 10% test\n\n# Save to CSV\ntrain_data.to_csv(\"_Data-split/passes-start-train.csv\", index=False)\nval_data.to_csv(\"_Data-split/passes-start-validation.csv\", index=False)\ntest_data.to_csv(\"_Data-split/passes-start-test.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# normalizing data through a sigmoid function\n# seems to work quite well as an embedder\n\nimport pandas as pd\nimport numpy as np\nimport json\nfrom sklearn.model_selection import train_test_split\n\n# Specify the columns you want to read\ncolumns_to_read = ['heatmapsId', 'playerId', 'teamId', 'season', 'passes-start-heatmap']\n\n# Initialize an empty list to store data from each file\nall_data = []\n\n# Loop through files and read data\nfor k in range(1, 6):  # Loop through files 1 to 5\n    file_path = f'_normalized-heatmaps/normalized-heatmaps-chunk-{k}.csv'\n    data_chunk = pd.read_csv(file_path, usecols=columns_to_read)\n    all_data.append(data_chunk)\n\n# Concatenate all the data into a single DataFrame\ndata = pd.concat(all_data, ignore_index=True)\n\n# Apply sigmoid to the heatmap values\ndef sigmoid_heatmap(heatmap_str):\n    # Convert the heatmap string to a numpy array\n    heatmap = np.array(eval(heatmap_str))  # Assuming the heatmap is stored as a string of lists\n\n    # Apply sigmoid\n    sigmoid_heatmap = 1 / (1 + np.exp(-heatmap))  # Sigmoid function\n    \n    return sigmoid_heatmap.tolist()  # Convert back to list\n\n# Apply sigmoid transformation and save as JSON strings\ndata['passes-start-heatmap'] = data['passes-start-heatmap'].apply(\n    lambda x: json.dumps(sigmoid_heatmap(x))\n)\n\n# Split into train, validation, and test sets\ntrain_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)  # 70% train\nval_data, test_data = train_test_split(temp_data, test_size=0.33, random_state=42)  # 20% val, 10% test\n\n# Save to CSV\ntrain_data.to_csv(\"_Data-split/passes-start-train.csv\", index=False)\nval_data.to_csv(\"_Data-split/passes-start-validation.csv\", index=False)\ntest_data.to_csv(\"_Data-split/passes-start-test.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:24:12.294787Z","iopub.execute_input":"2024-12-30T10:24:12.295140Z","iopub.status.idle":"2024-12-30T10:24:15.159827Z","shell.execute_reply.started":"2024-12-30T10:24:12.295104Z","shell.execute_reply":"2024-12-30T10:24:15.158872Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.4.1+cu121\nCUDA available: True\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport json\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim as optim\nfrom itertools import product\nimport matplotlib.pyplot as plt\n\nclass HeatmapCSVLoader(Dataset):\n    def __init__(self, csv_path):\n        self.data = pd.read_csv(csv_path)\n        self.heatmaps = self.data[\"passes-start-heatmap\"]\n        self.ids = self.data[\"heatmapsId\"]\n        self.playerIds = self.data[\"playerId\"]\n        self.teamIds = self.data[\"teamId\"]\n        self.seasons = self.data[\"season\"]\n    \n    def __len__(self):\n        return len(self.heatmaps)\n    \n    def __getitem__(self, idx):\n        # Parse heatmap JSON string\n        heatmap = json.loads(self.heatmaps.iloc[idx])\n        heatmap_tensor = torch.tensor(heatmap, dtype=torch.float32)  # Convert to tensor\n        \n        # Add channel dimension for PyTorch (batch, channel, height, width)\n        heatmap_tensor = heatmap_tensor.unsqueeze(0)\n        \n        # Return heatmap tensor and its metadata\n        return heatmap_tensor, self.ids.iloc[idx], self.playerIds.iloc[idx], self.teamIds.iloc[idx], self.seasons.iloc[idx]    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:24:21.316728Z","iopub.execute_input":"2024-12-30T10:24:21.317180Z","iopub.status.idle":"2024-12-30T10:24:21.575324Z","shell.execute_reply.started":"2024-12-30T10:24:21.317151Z","shell.execute_reply":"2024-12-30T10:24:21.574630Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load datasets\ntrain_set = HeatmapCSVLoader(csv_path=\"/kaggle/input/sigmod-1chunk/1passes-start-train.csv\")\ntrain_loader = DataLoader(train_set, batch_size=64, shuffle=True)\nval_set = HeatmapCSVLoader(csv_path=\"/kaggle/input/sigmod-1chunk/1passes-start-validation.csv\")\nval_loader = DataLoader(val_set, batch_size=64, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:24:52.390155Z","iopub.execute_input":"2024-12-30T10:24:52.390606Z","iopub.status.idle":"2024-12-30T10:24:53.918961Z","shell.execute_reply.started":"2024-12-30T10:24:52.390581Z","shell.execute_reply":"2024-12-30T10:24:53.918282Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#import torch.nn as nn\n\nclass Encoder(nn.Module):\n    def __init__(self, input_channels, bottleneck_size):\n        super(Encoder, self).__init__()\n        \n        # Conv Block 1\n        self.conv_block1 = nn.Sequential(\n            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Dropout2d(p=0.3)\n        )\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        \n        # Conv Block 2\n        self.conv_block2 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, padding=1, groups=64),\n            nn.Conv2d(128, 128, kernel_size=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1, groups=128),\n            nn.Conv2d(128, 128, kernel_size=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Dropout2d(p=0.3)\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        \n        # Conv Block 3\n        self.conv_block3 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Dropout2d(p=0.3)\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        \n        # Compute flattened size after convolutions and pooling\n        self.flattened_size = 256 * 4 * 2  # Computed from input size [1, 35, 23]\n        self.flatten = nn.Flatten()\n        self.dense = nn.Sequential(\n            nn.Linear(self.flattened_size, bottleneck_size),\n            nn.Dropout(p=0.4)\n        )\n    \n    def forward(self, x):\n        # Conv Block 1\n        x = self.conv_block1(x)\n        x, indices1 = self.pool1(x)\n        #print(\"After Pool1:\", x.shape)\n        \n        # Conv Block 2\n        x = self.conv_block2(x)\n        x, indices2 = self.pool2(x)\n        #print(\"After Pool2:\", x.shape)\n        \n        # Conv Block 3\n        x = self.conv_block3(x)\n        x, indices3 = self.pool3(x)\n        #print(\"After Pool3:\", x.shape)\n\n        # Flatten and bottleneck\n        x = self.flatten(x)\n        #print(\"After Flatten:\", x.shape)\n        bottleneck = self.dense(x)\n        \n        return bottleneck, [indices1, indices2, indices3]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, input_channels, bottleneck_size):\n        super(Decoder, self).__init__()\n\n        self.dense = nn.Sequential(\n            nn.Linear(bottleneck_size, 256 * 4 * 2),\n            nn.ReLU()\n        )\n\n        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(256, 4, 2))\n\n        self.unpool3 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.deconv_block3 = nn.Sequential(\n            nn.ConvTranspose2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU()\n        )\n\n        self.unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.deconv_block2 = nn.Sequential(\n            nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU()\n        )\n\n        self.unpool1 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.deconv_block1 = nn.Sequential(\n            nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, input_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(input_channels),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x, indices_list, output_sizes):\n        # Dense layer and unflatten\n        x = self.dense(x)\n        x = self.unflatten(x)\n        #print(\"After Unflatten:\", x.shape)\n\n        # Unpooling and deconvolution\n        #print(f\"output_sizes[2]: {output_sizes[2]}\")\n        x = self.unpool3(x, indices_list[2], output_size=output_sizes[2])\n        x = self.deconv_block3(x)\n        #print(\"Decoder After UnPool1:\", x.shape)\n\n        #print(f\"output_sizes[2]: {output_sizes[1]}\")\n        x = self.unpool2(x, indices_list[1], output_size=output_sizes[1])\n        x = self.deconv_block2(x)\n        #print(\"Decoder After UnPool2:\", x.shape)\n\n        #print(f\"output_sizes[2]: {output_sizes[0]}\")\n        x = self.unpool1(x, indices_list[0], output_size=output_sizes[0])\n        x = self.deconv_block1(x)\n        #print(\"Decoder After UnPool3:\", x.shape)\n\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Autoencoder(nn.Module):\n    def __init__(self, input_channels, bottleneck_size):\n        super(Autoencoder, self).__init__()\n        self.encoder = Encoder(input_channels, bottleneck_size)\n        self.decoder = Decoder(input_channels, bottleneck_size)\n\n    def forward(self, x):\n        # Encoder\n        bottleneck, indices_list = self.encoder(x)\n\n        # Verify output_sizes match shapes before pooling\n        output_sizes = [\n            torch.Size([x.size(0), 1, 35, 23]),    # Before Pool1\n            torch.Size([x.size(0), 64, 17, 11]),  # Before Pool2\n            torch.Size([x.size(0), 128, 8, 5])    # Before Pool3\n        ]\n        #for i, size in enumerate(output_sizes):\n            #print(f\"output_sizes[{i}]: {size}\")\n\n        # Decoder\n        reconstructed = self.decoder(bottleneck, indices_list, output_sizes)\n        return reconstructed","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# To be deleted\nimport time\nstart_time = time.time()\n# Training parameters\nnum_epochs = 10\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\nautoencoder = Autoencoder(input_channels=1, bottleneck_size=64).to(device)\ncriterion = nn.MSELoss()  # Mean Squared Error for reconstruction loss\noptimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-4)\n\nfor epoch in range(num_epochs):\n    autoencoder.train()  # Set model to training mode\n    epoch_loss = 0.0\n    \n    for batch in train_loader:\n        images, ids, playerIds, teamIds, seasons = batch  # Unpack images and ignore the rest\n        #print(f\"Images Shape: {images.shape}\")\n        images = images.to(device)  # Move images to GPU if available\n        \n        # Forward pass\n        reconstructed = autoencoder(images)\n        loss = criterion(reconstructed, images)\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}\")\n\nend_time = time.time()\nprint(f\"Execution time: {end_time - start_time:.2f} seconds\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# To be deleted\n\nautoencoder.eval()  # Set model to evaluation mode\nval_loss = 0.0\n\nwith torch.no_grad():\n    for batch in val_loader:\n        images, ids, playerIds, teamIds, seasons = batch  # Unpack images and ignore the rest\n        images = images.to(device)  # Move images to AMD GPU\n        \n        # Forward pass\n        reconstructed = autoencoder(images)\n        loss = criterion(reconstructed, images)\n        \n        val_loss += loss.item()\n\nprint(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test: save embeddings and use \nimport numpy as np\n\n# Training parameters\nnum_epochs = 25\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\nautoencoder = Autoencoder(input_channels=1, bottleneck_size=64).to(device)\ncriterion = nn.MSELoss()  # Mean Squared Error for reconstruction loss\noptimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-4)\n\nlatent_vectors_train = {}  # Store latent vectors for training data\nlatent_vectors_val = {}  # Store latent vectors for validation data\n\nfor epoch in range(num_epochs):\n    autoencoder.train()  # Set model to training mode\n    epoch_loss = 0.0\n    \n    # Training loop\n    for batch in train_loader:\n        images, ids, playerIds, teamIds, seasons = batch  # Unpack images and metadata\n        images = images.to(device)  # Move images to GPU if available\n        \n        # Forward pass\n        reconstructed = autoencoder(images)\n        loss = criterion(reconstructed, images)\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n        # Save latent representations for training data\n        latent_output = autoencoder.encoder(images)\n        embeddings = latent_output[0]\n        embeddings = embeddings.detach().cpu().numpy()  # Convert to numpy array\n        for i, emb in enumerate(embeddings):\n            key = (playerIds[i].item(), teamIds[i].item(), seasons[i])\n            if key not in latent_vectors_train:\n                latent_vectors_train[key] = []\n            latent_vectors_train[key].append(emb)\n    \n    # Print training loss\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}\")\n    \n    # Validation loop (if needed)\n    autoencoder.eval()  # Set model to evaluation mode\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in val_loader:\n            images, ids, playerIds, teamIds, seasons = batch\n            images = images.to(device)\n            \n            # Forward pass\n            reconstructed = autoencoder(images)\n            loss = criterion(reconstructed, images)\n            val_loss += loss.item()\n            \n            # Save latent representations for validation data\n            latent_output = autoencoder.encoder(images)\n            embeddings = latent_output[0]\n            embeddings = embeddings.detach().cpu().numpy()  # Convert to numpy array\n            for i, emb in enumerate(embeddings):\n                key = (playerIds[i].item(), teamIds[i].item(), seasons[i])\n                if key not in latent_vectors_val:\n                    latent_vectors_val[key] = []\n                latent_vectors_val[key].append(emb)\n    \n    print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")\n\n# Save latent vectors for retrieval task evaluation\nnp.save(\"latent_vectors_train.npy\", latent_vectors_train)\nnp.save(\"latent_vectors_val.npy\", latent_vectors_val)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter test set to use only players with multiple heatmaps in a team/season for the retrieval task\nfrom collections import defaultdict\n\n# Load test dataset\ntest_data = pd.read_csv('/kaggle/input/sigmoid-10-chunks/passes-start-test.csv')\n\n# Group by playerId, teamId, and season and count rows in each group\ngrouped = test_data.groupby([\"playerId\", \"teamId\", \"season\"]).size()\n\n# Filter combinations with more than one heatmap\nvalid_combinations = grouped[grouped > 1].index  # Get the indices of valid groups\n\n# Create filtered test instances\nfiltered_test_instances = defaultdict(list)\nfor _, row in test_data.iterrows():\n    key = (row[\"playerId\"], row[\"teamId\"], row[\"season\"])\n    if key in valid_combinations:\n        # Add heatmap and heatmapsId to corresponding key\n        heatmap = json.loads(row[\"passes-start-heatmap\"])\n        heatmaps_id = row[\"heatmapsId\"]\n        filtered_test_instances[key].append((heatmaps_id, heatmap))\n\n# Convert to a list of tuples for later use\nfiltered_test_instances = [\n    (playerId, teamId, season, heatmaps)\n    for (playerId, teamId, season), heatmaps in filtered_test_instances.items()\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:12:49.120054Z","iopub.execute_input":"2024-12-30T11:12:49.120475Z","iopub.status.idle":"2024-12-30T11:12:51.843094Z","shell.execute_reply.started":"2024-12-30T11:12:49.120442Z","shell.execute_reply":"2024-12-30T11:12:51.842155Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"len(filtered_test_instances)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:12:55.496752Z","iopub.execute_input":"2024-12-30T11:12:55.497113Z","iopub.status.idle":"2024-12-30T11:12:55.505119Z","shell.execute_reply.started":"2024-12-30T11:12:55.497057Z","shell.execute_reply":"2024-12-30T11:12:55.504025Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"1016"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"filtered_test_instances[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:13:01.385403Z","iopub.execute_input":"2024-12-30T11:13:01.385702Z","iopub.status.idle":"2024-12-30T11:13:01.402572Z","shell.execute_reply.started":"2024-12-30T11:13:01.385679Z","shell.execute_reply":"2024-12-30T11:13:01.401725Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"(384138.0,\n 28,\n '2023-2024',\n [('384138.0-28-2023-2024-1',\n   [[0.500020748099875,\n     0.500381559222259,\n     0.5041419442528845,\n     0.5181856701394938,\n     0.5301595942457386,\n     0.5221749589056821,\n     0.5221956866352074,\n     0.530147256176981,\n     0.5181200891279351,\n     0.5040660997411661,\n     0.5003395204713309,\n     0.5000107294692892,\n     0.5000000228451991,\n     0.5,\n     0.5,\n     0.5000000228451991,\n     0.5000007565290928,\n     0.5000092164111041,\n     0.5000413050888233,\n     0.500068100578265,\n     0.5000413050888233,\n     0.5000092164111041,\n     0.5000007793742919],\n    [0.5004594259023604,\n     0.503186803155176,\n     0.5207414392160441,\n     0.5792497423003723,\n     0.626358098138007,\n     0.5947067177210353,\n     0.5953607504633875,\n     0.6259261545183193,\n     0.5770132477195047,\n     0.5180921350745067,\n     0.5017166102384562,\n     0.5001344813123901,\n     0.5000420616179109,\n     0.5000092164111041,\n     0.5000007565290928,\n     0.5000007793742919,\n     0.5000250528028095,\n     0.5003052055812744,\n     0.5013678332754452,\n     0.5022551661492969,\n     0.5013678332754452,\n     0.5003052055812744,\n     0.5000258093319004],\n    [0.5064439091984648,\n     0.5268810565620492,\n     0.5796737405447574,\n     0.6801620331018091,\n     0.74073407380128,\n     0.6946225696398729,\n     0.7014096315696763,\n     0.7355398282162685,\n     0.6531789915827573,\n     0.5449823642455331,\n     0.5073690787261025,\n     0.5026292427792775,\n     0.5013770724621359,\n     0.5003052055812744,\n     0.5000250528028095,\n     0.5000099729401966,\n     0.5003052055812744,\n     0.5037180970769892,\n     0.5166574954249955,\n     0.5274461178583224,\n     0.5166574954249955,\n     0.5037180970769892,\n     0.5003144219888407],\n    [0.5444581002985471,\n     0.6389952216044061,\n     0.7634904473012173,\n     0.8325724427982187,\n     0.8316041387148403,\n     0.7993440667470423,\n     0.8197976563362172,\n     0.8057635954324663,\n     0.7197639394832428,\n     0.5983851649597745,\n     0.5351136534187194,\n     0.5288873234813027,\n     0.5167010672510927,\n     0.5037273358211198,\n     0.5003465106526246,\n     0.5001186220764828,\n     0.501409138045812,\n     0.5166667016012564,\n     0.5741316432575754,\n     0.6206987299662216,\n     0.5741309033583407,\n     0.5166574954249955,\n     0.501409138045812],\n    [0.6492383312345575,\n     0.8225053251187625,\n     0.9304366804065219,\n     0.9481893967870605,\n     0.9186346688482664,\n     0.8900983245140497,\n     0.9049670237064511,\n     0.8707948081349539,\n     0.7929717898780541,\n     0.668202779142705,\n     0.6070673739142157,\n     0.6234701688192547,\n     0.5742577398235749,\n     0.516963134539752,\n     0.5027356460776667,\n     0.5023645694815327,\n     0.5036229547219545,\n     0.5277503936117808,\n     0.6207223010749374,\n     0.692540400582072,\n     0.6206987084522745,\n     0.5274461178583224,\n     0.5023232653003511],\n    [0.7935839784024971,\n     0.9226488856801403,\n     0.9657464334869813,\n     0.9692124602969732,\n     0.9553874045405233,\n     0.9350058975754766,\n     0.9408366946321981,\n     0.9370096594516784,\n     0.8886495671823069,\n     0.751464880089648,\n     0.6767549781249504,\n     0.6988573628599698,\n     0.6214007448064646,\n     0.5312025778844595,\n     0.5189778229749662,\n     0.5275963720089797,\n     0.5180328904740786,\n     0.520371301242662,\n     0.5744293953710723,\n     0.6207073877594806,\n     0.5741309033583407,\n     0.5166574954249955,\n     0.501409138045812],\n    [0.9487717602997623,\n     0.9840932665294548,\n     0.9866179436854944,\n     0.9802107717260952,\n     0.9766460471808452,\n     0.9714240604475848,\n     0.9855414424504491,\n     0.9944540080249129,\n     0.9867016176263713,\n     0.905178421055492,\n     0.7227422750665754,\n     0.6449661723629657,\n     0.5776243070925982,\n     0.5348002439541923,\n     0.5777772239796344,\n     0.6220810082769417,\n     0.574737538615737,\n     0.5203963352149547,\n     0.5180244644065345,\n     0.5274872983016666,\n     0.5166574954249955,\n     0.5037180970769892,\n     0.5003144219888407],\n    [0.9906537664589941,\n     0.9985425351943424,\n     0.999135382580225,\n     0.9985009856058087,\n     0.9960244117619247,\n     0.9922460361354154,\n     0.997881702863219,\n     0.9995860229882877,\n     0.998261570237845,\n     0.9677360969998596,\n     0.7752378059072047,\n     0.5810535571567611,\n     0.5287100739312754,\n     0.5514560298031832,\n     0.6497928799024182,\n     0.7079413401885323,\n     0.624527198727057,\n     0.5280803995254629,\n     0.5036336836262439,\n     0.5023324815116811,\n     0.5014091608908297,\n     0.5003740626571735,\n     0.5000763308313527],\n    [0.9954970282372058,\n     0.9997503018899072,\n     0.9999594767636129,\n     0.9999764161333581,\n     0.9999229539215827,\n     0.9995777114307391,\n     0.9996177373072898,\n     0.9997571312524127,\n     0.9979194759626022,\n     0.9638604860167262,\n     0.7911610917744469,\n     0.5911420510620178,\n     0.5512097398842206,\n     0.6332550916421815,\n     0.7247152989201227,\n     0.7053219500620291,\n     0.5942291843806476,\n     0.5183476518703464,\n     0.5014847117156339,\n     0.5004146111917543,\n     0.5014098945688927,\n     0.5022894346554434,\n     0.5016738154285098],\n    [0.9996458902107342,\n     0.9999913474979408,\n     0.999997795409083,\n     0.9999978720508647,\n     0.9999952797914159,\n     0.9999508290458705,\n     0.9996950360223072,\n     0.9994294068542448,\n     0.9959950974784895,\n     0.9583672495087258,\n     0.8602696542656418,\n     0.7059985141607861,\n     0.6558037895504222,\n     0.7881563035601831,\n     0.8416629776076323,\n     0.7246873470720829,\n     0.5618512344895801,\n     0.5073911896454565,\n     0.5007198164086817,\n     0.5037273129771903,\n     0.5166667016012564,\n     0.5277503936117808,\n     0.5203705459693303],\n    [0.999955050351351,\n     0.9999989557460522,\n     0.9999991735148329,\n     0.999995591187271,\n     0.9999843198734226,\n     0.9998270136254999,\n     0.9978394996959359,\n     0.9952335312282052,\n     0.9895409354601447,\n     0.9676016973874292,\n     0.9456018252577055,\n     0.8524508131466202,\n     0.752663171062969,\n     0.841338713996633,\n     0.8595089954201761,\n     0.713297520612562,\n     0.5476405319460007,\n     0.5040382981340124,\n     0.5015022906374521,\n     0.5166582511143873,\n     0.5741713000019627,\n     0.6219859842771172,\n     0.590342168032759],\n    [0.9997174174892643,\n     0.9999866121444083,\n     0.9999834838323048,\n     0.9997957550813831,\n     0.9990521153174601,\n     0.9965313050391812,\n     0.9809769853712029,\n     0.9625225808925463,\n     0.9674398447087776,\n     0.9680991089066813,\n     0.9704240433607713,\n     0.9281570966674694,\n     0.7973015151886964,\n     0.7562466173941774,\n     0.7272841285655901,\n     0.6100709410989116,\n     0.5220825571330354,\n     0.5018008958542071,\n     0.5023071996243493,\n     0.5274553292659473,\n     0.6208017548230268,\n     0.6945156210800095,\n     0.6462536543763171],\n    [0.9941381630547529,\n     0.9993365259195724,\n     0.9994733052071162,\n     0.9958447245858424,\n     0.9869899416257759,\n     0.983265243942566,\n     0.9557556440576827,\n     0.9190719716905129,\n     0.9454940890143566,\n     0.9576834921330939,\n     0.9636420290677906,\n     0.9442181063782285,\n     0.8445423313545888,\n     0.7267890177818827,\n     0.6267171559222781,\n     0.5388854562205635,\n     0.5057558971695408,\n     0.5004328143721375,\n     0.5014036381177293,\n     0.5169623788813791,\n     0.5755092300075458,\n     0.6241280954660635,\n     0.5919596068971973],\n    [0.9457055950933103,\n     0.9945343133173955,\n     0.9987002341952801,\n     0.9940267762209835,\n     0.9843613108561428,\n     0.9888037442324323,\n     0.977438232375419,\n     0.9239050212469659,\n     0.8741719247309644,\n     0.8482726717772028,\n     0.8524320365186047,\n     0.8649781861300821,\n     0.843133730497723,\n     0.7811883492248122,\n     0.6455980128489315,\n     0.5342758043422742,\n     0.5043501989891133,\n     0.5004525739714323,\n     0.5006377561278084,\n     0.5074365621795147,\n     0.5332964070395981,\n     0.5553303510747627,\n     0.5406735804423138],\n    [0.7981406145272035,\n     0.9722557874381123,\n     0.9952725918085612,\n     0.98976211422562,\n     0.9889701658187486,\n     0.9964070105015644,\n     0.9964571333256315,\n     0.9840719096473622,\n     0.8883475659423777,\n     0.7212444715624514,\n     0.6751945052912789,\n     0.7245480279151397,\n     0.7850305326471085,\n     0.7503761565484482,\n     0.6441901502011956,\n     0.5650102742135138,\n     0.5231789489742759,\n     0.5044126083688162,\n     0.5017323802366292,\n     0.5169723175182102,\n     0.5755092300075458,\n     0.6241280954660635,\n     0.5919596068971973],\n    [0.7202429958014847,\n     0.9327690140207914,\n     0.981264551926077,\n     0.9739427632184666,\n     0.9851687360041567,\n     0.9967301186530428,\n     0.9987999458329296,\n     0.9972723220433851,\n     0.9664338731147463,\n     0.8648156189663926,\n     0.7735953336279883,\n     0.6851762904496114,\n     0.6731397812477695,\n     0.6893800905826157,\n     0.7293985184005505,\n     0.7153446589503013,\n     0.6066902156876234,\n     0.521813149842466,\n     0.5039702019231352,\n     0.5275056753788332,\n     0.6208017548230268,\n     0.6945156210800095,\n     0.6462536543763171],\n    [0.7310763854782386,\n     0.9206863125122277,\n     0.9673415512897954,\n     0.9480972456693884,\n     0.9568643838095439,\n     0.9832516718234858,\n     0.9928205560002041,\n     0.9897978153873139,\n     0.9585877073360567,\n     0.9264365730720081,\n     0.8636544526341273,\n     0.6881242939938786,\n     0.590990192843998,\n     0.6801328250009131,\n     0.8353299140925368,\n     0.858087031914697,\n     0.7112109035543892,\n     0.5463016034904619,\n     0.5050587827709792,\n     0.5167667788661067,\n     0.5741713000019627,\n     0.6219859842771172,\n     0.590342168032759],\n    [0.6912224983358043,\n     0.8865483227932942,\n     0.9526773227290197,\n     0.9223504431912942,\n     0.8763043937125635,\n     0.900647105416532,\n     0.9341725984481014,\n     0.9183011212268914,\n     0.8470176050549939,\n     0.8169830598346194,\n     0.7519389568639046,\n     0.6092837510832653,\n     0.541925885429502,\n     0.6193658369037388,\n     0.7833522285139345,\n     0.8324926795087624,\n     0.702221937206247,\n     0.5453891726661568,\n     0.5039694454417325,\n     0.5038274965150369,\n     0.5166667016012564,\n     0.5277503936117808,\n     0.5203705459693303],\n    [0.7615110823228818,\n     0.9270599951285253,\n     0.9732102761326916,\n     0.9490359204577086,\n     0.8867250000479129,\n     0.8828858625118834,\n     0.9002739343082943,\n     0.8597599397531827,\n     0.7303981008720631,\n     0.6218413477893514,\n     0.5682603050041137,\n     0.5271126480103455,\n     0.5111631059516001,\n     0.5376442691077107,\n     0.6166726336947809,\n     0.661295014619617,\n     0.5939340769951152,\n     0.5206759926173893,\n     0.5017073048852858,\n     0.5003557270591844,\n     0.5013685897988732,\n     0.5022802184367933,\n     0.5016730360629479],\n    [0.9269646401570598,\n     0.9810072245788848,\n     0.9876067414640237,\n     0.9611567669327341,\n     0.934871461752705,\n     0.9454925496195994,\n     0.9270233200239416,\n     0.8572547369650138,\n     0.7411736343404836,\n     0.6078367715550118,\n     0.5403301076923397,\n     0.5308918694644832,\n     0.5176891363306467,\n     0.5091356790921172,\n     0.52059020787775,\n     0.5311070462190403,\n     0.5183964970610536,\n     0.5040896607637063,\n     0.500340987838439,\n     0.5000199458803852,\n     0.5000413507792212,\n     0.5000688571073437,\n     0.5000505214998504],\n    [0.9679633338778219,\n     0.99326420583866,\n     0.9954573127347832,\n     0.9771870128042106,\n     0.9579152106627127,\n     0.9712261131028743,\n     0.9593436436500886,\n     0.9073316583129817,\n     0.8185158753779387,\n     0.6736010457665629,\n     0.6061765925241552,\n     0.6232119307217152,\n     0.5742520056913182,\n     0.5170386442882119,\n     0.5031502658213194,\n     0.5037736572319684,\n     0.5036734735322771,\n     0.5016830088908198,\n     0.5003310377479341,\n     0.5000258321770995,\n     0.5000007565290928,\n     0.5,\n     0.5],\n    [0.9063815535547782,\n     0.9871326811997114,\n     0.9976775153601517,\n     0.9940961266580897,\n     0.9825350349238942,\n     0.9747034318303515,\n     0.9595721173352595,\n     0.9237657208210629,\n     0.8456803064487212,\n     0.6973594446829716,\n     0.6537180419551114,\n     0.6949232596723138,\n     0.6207858926028412,\n     0.5277595816238294,\n     0.5060143620449293,\n     0.5167935441762354,\n     0.5274872983016666,\n     0.5166667016012564,\n     0.5037188535642398,\n     0.500305228426465,\n     0.5000092164111041,\n     0.5,\n     0.5],\n    [0.7032582618998559,\n     0.9323708674465239,\n     0.9927560547269051,\n     0.9960024783660978,\n     0.9920841346408812,\n     0.9682776259104913,\n     0.913906915911347,\n     0.8642487191528349,\n     0.7997393240078098,\n     0.6693008215357921,\n     0.605715531808638,\n     0.6231457186954453,\n     0.5742469138555552,\n     0.5180236860450154,\n     0.5180236860450154,\n     0.5741713000019627,\n     0.6206987084522745,\n     0.5741309033583407,\n     0.5166574954249955,\n     0.5013678332754452,\n     0.5000413050888233,\n     0.5,\n     0.5],\n    [0.5737450614115231,\n     0.7784342352504671,\n     0.9416781718626979,\n     0.9818109976962003,\n     0.9877143769894448,\n     0.9630068208001724,\n     0.8775205472208053,\n     0.76174983317788,\n     0.6721856248973139,\n     0.5855768479090426,\n     0.5339726015098811,\n     0.5288437801890222,\n     0.5167675345444447,\n     0.5059730628915089,\n     0.5277503936117808,\n     0.6207073877594806,\n     0.6925397562362022,\n     0.6206987084522745,\n     0.5274461178583224,\n     0.5022551661492969,\n     0.500068100578265,\n     0.5,\n     0.5],\n    [0.5243398042412526,\n     0.6073421800883868,\n     0.7306572907752158,\n     0.845247340781684,\n     0.9204940772274177,\n     0.9192343110542809,\n     0.849612969908243,\n     0.6916602143380708,\n     0.5646277860915246,\n     0.5201348466930619,\n     0.5052034732007457,\n     0.5025618776999721,\n     0.5014183772282494,\n     0.5016730360629479,\n     0.5166825203802111,\n     0.5741316432575754,\n     0.6206987084522745,\n     0.5741309033583407,\n     0.5166574954249955,\n     0.5013678332754452,\n     0.5000413050888233,\n     0.5,\n     0.5],\n    [0.504473449318094,\n     0.5197896056253134,\n     0.5455254081516513,\n     0.5922718354352497,\n     0.6878018551373795,\n     0.7793073533838543,\n     0.7804996088817123,\n     0.6537450114202402,\n     0.5348388358027915,\n     0.5039886564214971,\n     0.500424606970111,\n     0.5000931762256371,\n     0.5000512780289355,\n     0.5003144219888407,\n     0.5037188535642398,\n     0.516657518244839,\n     0.5274461178583224,\n     0.5166574954249955,\n     0.5037180970769892,\n     0.5003052055812744,\n     0.5000092164111041,\n     0.5,\n     0.5],\n    [0.5003440824889555,\n     0.5015134806993535,\n     0.5036779263662674,\n     0.5107995333204192,\n     0.5407724942710014,\n     0.6060768491300258,\n     0.6392097246232367,\n     0.5792125103082737,\n     0.51734671811426,\n     0.5014847117156339,\n     0.5001193786055329,\n     0.5000420616179109,\n     0.5000099957853957,\n     0.5000258093319004,\n     0.500305228426465,\n     0.5013678332754452,\n     0.5022551661492969,\n     0.5013678332754452,\n     0.5003052055812744,\n     0.5000250528028095,\n     0.5000007565290928,\n     0.5,\n     0.5],\n    [0.5000100414757939,\n     0.5000443540503716,\n     0.5001300418682038,\n     0.5007763874991382,\n     0.5053085602240653,\n     0.519076312977903,\n     0.5289150181215161,\n     0.517072161289761,\n     0.5041257159213488,\n     0.5017158537182765,\n     0.5022736214381799,\n     0.5013685897988732,\n     0.5003052512716556,\n     0.5000258093319004,\n     0.5000099729401966,\n     0.5000413050888233,\n     0.500068100578265,\n     0.5000413050888233,\n     0.5000092164111041,\n     0.5000007565290928,\n     0.5000000228451991,\n     0.5,\n     0.5],\n    [0.5,\n     0.5000000228451991,\n     0.5000015130581857,\n     0.5000342692138819,\n     0.5003472671813533,\n     0.5014609859112586,\n     0.5026108848304275,\n     0.5030500575725125,\n     0.5062789791014797,\n     0.5180487290641266,\n     0.5277511478104454,\n     0.5166825203802111,\n     0.5037188535642398,\n     0.5003052055812744,\n     0.5000092164111041,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5],\n    [0.5,\n     0.5,\n     0.5000000228451991,\n     0.5000007565290928,\n     0.5000184328222017,\n     0.5003465106526246,\n     0.5038274965150369,\n     0.5180649373386264,\n     0.5440322516661166,\n     0.5903428998633755,\n     0.624193889277599,\n     0.5744293730320984,\n     0.5166667016012564,\n     0.5013678332754452,\n     0.5000413050888233,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5],\n    [0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5000413050888233,\n     0.5013678332754452,\n     0.5167255201105393,\n     0.5763350235128826,\n     0.6462074541570665,\n     0.6880975956338082,\n     0.7065474705949799,\n     0.6219859842771172,\n     0.5274872983016666,\n     0.5022551661492969,\n     0.500068100578265,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5],\n    [0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.500068100578265,\n     0.5022551661492969,\n     0.5274872983016666,\n     0.6219859842771172,\n     0.7065474705949799,\n     0.6880975956338082,\n     0.6462074541570665,\n     0.5763350235128826,\n     0.5167255201105393,\n     0.5013678332754452,\n     0.5000413050888233,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5],\n    [0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5000413050888233,\n     0.5013678332754452,\n     0.5166667016012564,\n     0.5744293730320984,\n     0.6241938678418681,\n     0.590342168032759,\n     0.5440231067167238,\n     0.5180236860450154,\n     0.5037593998563932,\n     0.5003052055812744,\n     0.5000092164111041,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5],\n    [0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5000092164111041,\n     0.5003052055812744,\n     0.5037188535642398,\n     0.5166825203802111,\n     0.5277503936117808,\n     0.5180236860450154,\n     0.5059730628915089,\n     0.5016730360629479,\n     0.5003144219888407,\n     0.5000250528028095,\n     0.5000007565290928,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5],\n    [0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5000007793742919,\n     0.5000258093319004,\n     0.5003144448340308,\n     0.5014098945688927,\n     0.5023324815116811,\n     0.5014504427969457,\n     0.5003825225343429,\n     0.5000671144204376,\n     0.5000099957853957,\n     0.5000007565290928,\n     0.5000000228451991,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5]]),\n  ('384138.0-28-2023-2024-5',\n   [[0.5005068293191912,\n     0.5053955994269422,\n     0.5294722082064827,\n     0.5789971363882819,\n     0.6000284992810258,\n     0.5553172807934783,\n     0.5122843066458509,\n     0.5013864543722815,\n     0.5054416226565535,\n     0.560867289488976,\n     0.7699733338000929,\n     0.931719168008646,\n     0.9454708010946413,\n     0.8690336238805868,\n     0.7124726346334533,\n     0.5826508017437045,\n     0.5218378346060387,\n     0.5056569523479029,\n     0.5058875631603746,\n     0.5122075615412416,\n     0.5172545665753324,\n     0.5102560156641184,\n     0.5025113673307174],\n    [0.5033410052406535,\n     0.5264956210744984,\n     0.6094574464388941,\n     0.7184981434477404,\n     0.7303300117024383,\n     0.6266303740250541,\n     0.5319069479702105,\n     0.5062618833717701,\n     0.5157355698379948,\n     0.5971637129284766,\n     0.8506274295263098,\n     0.9776799222501066,\n     0.986767325165532,\n     0.9564184596368831,\n     0.8409731815109954,\n     0.6857708173917791,\n     0.5794113751662444,\n     0.5445836973347661,\n     0.5560997733070014,\n     0.5823193095309604,\n     0.5998415190424125,\n     0.558746080386393,\n     0.5155926038830153],\n    [0.5113708781917479,\n     0.5710195221993628,\n     0.7232065740053768,\n     0.844430877810429,\n     0.8370103564404527,\n     0.7139615724378955,\n     0.5747832534656391,\n     0.5285013377083788,\n     0.5566173501835849,\n     0.6358859037246437,\n     0.812552404531943,\n     0.9519400214016859,\n     0.9795865081532217,\n     0.9707658318201164,\n     0.9191432553119,\n     0.8124171615866272,\n     0.7009281315758698,\n     0.6832837353546315,\n     0.7357369115544267,\n     0.7686460469205316,\n     0.7769559529852192,\n     0.673274604502351,\n     0.5605173439399208],\n    [0.518609971414563,\n     0.6003826868148383,\n     0.7602155566263035,\n     0.855285088722822,\n     0.845601560502128,\n     0.7584970968247952,\n     0.6293210707956698,\n     0.5848944888390307,\n     0.6456156615050368,\n     0.7276167310477722,\n     0.8019961790486295,\n     0.8915982065355786,\n     0.9617385343813287,\n     0.9798559193452375,\n     0.9607240350998124,\n     0.8807431642892372,\n     0.7950884335459678,\n     0.8304971905058269,\n     0.8938742263982818,\n     0.9023122227647423,\n     0.8860779063746698,\n     0.7732815921025945,\n     0.647688608871763],\n    [0.5203469024789449,\n     0.5990093796357457,\n     0.7245858504114824,\n     0.7774255974916422,\n     0.7571720810228209,\n     0.7137687630393532,\n     0.6547753462995319,\n     0.6639009772579724,\n     0.7345236501773517,\n     0.8129185234504557,\n     0.8622307875114961,\n     0.909128113384434,\n     0.97015181570517,\n     0.9860675965350908,\n     0.9705320571206952,\n     0.8968883527692123,\n     0.8342214560141942,\n     0.8633307294187679,\n     0.9018575178582622,\n     0.9063704611332221,\n     0.8884699273650257,\n     0.7882410926211116,\n     0.6979570981477846],\n    [0.5253678591960739,\n     0.6077620209132856,\n     0.6975756085770163,\n     0.6805661206427679,\n     0.6285865806628593,\n     0.628252102201427,\n     0.6692142152144552,\n     0.7493561772553943,\n     0.8149592965635208,\n     0.8702464724477652,\n     0.9088627379875991,\n     0.9546112430595923,\n     0.9886060460570955,\n     0.9950716827453278,\n     0.9925582363324589,\n     0.9767147253523862,\n     0.9490300567316505,\n     0.894973254063894,\n     0.821525943796525,\n     0.8207238125036348,\n     0.8370427346015807,\n     0.7450587148974411,\n     0.6453709835031838],\n    [0.5332294725693468,\n     0.6113479065378191,\n     0.6721239850632769,\n     0.6225126375003441,\n     0.5851578748408652,\n     0.6412280870164374,\n     0.7460502121173339,\n     0.8710435410001918,\n     0.9358989506494422,\n     0.9468157998594374,\n     0.9518295351780183,\n     0.9794511596794757,\n     0.9966127171645287,\n     0.9995606876188834,\n     0.9998331167370256,\n     0.9994926706294263,\n     0.9970738205033146,\n     0.9721235454118269,\n     0.8224149848690127,\n     0.7197784857968531,\n     0.7504974684693412,\n     0.6922447486124217,\n     0.5933080536213767],\n    [0.5597889692983531,\n     0.6300548626460554,\n     0.6563252539569072,\n     0.6004651769539454,\n     0.6060034926008615,\n     0.7235224057039836,\n     0.8556495561842512,\n     0.9592438391531143,\n     0.9885040364967816,\n     0.9881250303628748,\n     0.987366422024542,\n     0.9937219780022549,\n     0.998674246508994,\n     0.9999195181736592,\n     0.999988212273868,\n     0.9999610188157915,\n     0.9994598261570034,\n     0.9903617529544121,\n     0.8895642991535464,\n     0.6928933814360727,\n     0.6550863355118228,\n     0.6421698926211598,\n     0.5818710939736801],\n    [0.5837262369431726,\n     0.6547965822200656,\n     0.6651366987542782,\n     0.6056159927793159,\n     0.6199765961841337,\n     0.75786337855947,\n     0.8985402318683113,\n     0.9758063835165726,\n     0.9932499087355746,\n     0.9955668120960481,\n     0.9984170099277226,\n     0.9994383710253212,\n     0.9996923620286576,\n     0.9999302266960153,\n     0.9999750262588847,\n     0.9998999224352785,\n     0.9981993092929305,\n     0.9790037368508976,\n     0.881715713180474,\n     0.6799807471807452,\n     0.5790390973541506,\n     0.568737382603884,\n     0.5446477610972651],\n    [0.5505970240247329,\n     0.5980256251234326,\n     0.6241501147675497,\n     0.6128234167852252,\n     0.6289532449654262,\n     0.7377382850005167,\n     0.871773418332863,\n     0.9552519308724822,\n     0.9852144862231195,\n     0.9965373895455749,\n     0.9995892522117305,\n     0.9998881187437886,\n     0.9998951050195009,\n     0.9998647912595421,\n     0.9997815115427224,\n     0.9991721292413283,\n     0.9904208692003106,\n     0.9266434564221252,\n     0.7841162908105782,\n     0.6237104307128224,\n     0.54335541376585,\n     0.5292106945997792,\n     0.5189838786971106],\n    [0.5122137399319252,\n     0.5328524826092725,\n     0.5765362216756438,\n     0.6093651609071571,\n     0.6078118995287959,\n     0.6678086573490005,\n     0.8173427258335495,\n     0.9267579664140496,\n     0.9790935574574475,\n     0.997787602829717,\n     0.9997087478308496,\n     0.9998827100472603,\n     0.9998770127741539,\n     0.9996663273570663,\n     0.9986756261843617,\n     0.9954423668687293,\n     0.9752348045978207,\n     0.8869024655662731,\n     0.7489864035753232,\n     0.6389707023352266,\n     0.5826420174209676,\n     0.5673029926789172,\n     0.5438374454103144],\n    [0.5015824975188012,\n     0.5093275600221572,\n     0.5361978053274727,\n     0.5629154442669929,\n     0.5731407808794144,\n     0.6542635682383948,\n     0.8334594538731331,\n     0.9349613220344966,\n     0.9790331360003035,\n     0.9973219934429269,\n     0.9992404467078635,\n     0.9994693995564848,\n     0.9994061422445417,\n     0.9987703183329619,\n     0.9955027377913211,\n     0.9860615622002944,\n     0.9583360074797003,\n     0.8794029740055568,\n     0.7790522096128017,\n     0.6989420090845665,\n     0.6409535290522672,\n     0.6197737340791554,\n     0.5787344154338318],\n    [0.5009474651295518,\n     0.5029697458115729,\n     0.5102670998882538,\n     0.5314680172147147,\n     0.6041482385424873,\n     0.7480678325750316,\n     0.8737688784420713,\n     0.9187949684419917,\n     0.9432667805210659,\n     0.9786644147074086,\n     0.9872348649215955,\n     0.9894185488837081,\n     0.9907746270083491,\n     0.9900296639614691,\n     0.9840233465215901,\n     0.9719788677033573,\n     0.9475393368405949,\n     0.8650913226964839,\n     0.7728082869347891,\n     0.7158490070592682,\n     0.6566953202897673,\n     0.6291514231249243,\n     0.584155340126422],\n    [0.5094669911518354,\n     0.5146160884741188,\n     0.5185620273729002,\n     0.5486400241568221,\n     0.6717950291494545,\n     0.83415114965269,\n     0.8996614886054981,\n     0.8998635078572413,\n     0.903496570902129,\n     0.9373284394206359,\n     0.946742332463258,\n     0.9493932435361131,\n     0.9436220267739646,\n     0.936321917632238,\n     0.9459502986049361,\n     0.954466954800798,\n     0.9350551535914595,\n     0.8515141991391455,\n     0.7970863039695586,\n     0.7607239525360161,\n     0.6787389750678425,\n     0.628506815492375,\n     0.5801060421461547],\n    [0.5422922532572573,\n     0.5645086933140525,\n     0.5732866273876898,\n     0.6118255386362405,\n     0.7379389506564945,\n     0.8820189520906798,\n     0.9291122453200784,\n     0.9275985465929342,\n     0.9338833689826125,\n     0.96298830068964,\n     0.9799839396486623,\n     0.985162817017197,\n     0.9738943778645881,\n     0.9495723003918008,\n     0.9533222949196087,\n     0.958304140206529,\n     0.9150654379524181,\n     0.816999892536321,\n     0.8011583888945478,\n     0.7689171182282654,\n     0.662892902309373,\n     0.5938033078281061,\n     0.5535381761804732],\n    [0.5695763284057741,\n     0.6070727770894829,\n     0.6282903858760253,\n     0.6864996380931871,\n     0.7974952692382825,\n     0.8980991599703482,\n     0.9260727734622687,\n     0.9167947833205607,\n     0.9214666825165871,\n     0.9695074670288549,\n     0.9955575628689506,\n     0.9991350980284593,\n     0.9985432039544266,\n     0.9923951444837393,\n     0.9805303429370498,\n     0.9628457481941296,\n     0.8992248294865316,\n     0.7821930884172174,\n     0.7524882248097747,\n     0.7205595621882024,\n     0.6371884459128068,\n     0.5880377028557708,\n     0.5525111626730742],\n    [0.5429512567817194,\n     0.5727020076018747,\n     0.614853710639864,\n     0.6908607987211888,\n     0.7571685880599367,\n     0.7979389234681692,\n     0.8090661014916056,\n     0.7979269575380644,\n     0.8060350320397633,\n     0.9296872606318848,\n     0.996311671448411,\n     0.9998155791998439,\n     0.9997095470657619,\n     0.9955537970704976,\n     0.971523000398164,\n     0.9287989467773914,\n     0.8586112693119301,\n     0.7469422833841421,\n     0.6911913704891411,\n     0.6756181830429596,\n     0.641643170688018,\n     0.6156626707214727,\n     0.5721696915997924],\n    [0.5107087862547962,\n     0.5284533604061852,\n     0.5810842987417915,\n     0.6449502306909387,\n     0.6447625906199491,\n     0.6120707987529667,\n     0.6108519922958707,\n     0.6295619969916948,\n     0.6785652178402922,\n     0.8761730018534281,\n     0.9927028137278433,\n     0.9995524983961908,\n     0.9991056270041415,\n     0.987474947823289,\n     0.9468036573586611,\n     0.908303011956282,\n     0.8398768709090676,\n     0.7607097831936516,\n     0.7022903168140564,\n     0.6601983846993131,\n     0.6348403941194678,\n     0.5994585580167187,\n     0.5512618624527817],\n    [0.503549607786335,\n     0.5199393005863379,\n     0.5645213150551611,\n     0.5969883798092355,\n     0.5670640015914375,\n     0.5361736343460028,\n     0.5659547112026224,\n     0.6215206251378012,\n     0.7044441779494617,\n     0.8885431355603612,\n     0.9855688605598524,\n     0.9969972184069354,\n     0.9933804298723677,\n     0.9638863954812383,\n     0.945261290109224,\n     0.9434280541718473,\n     0.8834945461264977,\n     0.822630717295997,\n     0.7694886627710331,\n     0.6819175500272739,\n     0.6347267978016278,\n     0.5842928097389332,\n     0.5323554685686159],\n    [0.509691996386181,\n     0.5509344363343394,\n     0.6292948279896124,\n     0.6548214303071671,\n     0.5867511838362734,\n     0.5478105783060692,\n     0.6279317753364725,\n     0.7216764494054093,\n     0.7627044436854152,\n     0.8628955620867403,\n     0.9423528947152772,\n     0.9646899456918954,\n     0.9442131590843059,\n     0.8810337239191571,\n     0.8905893255885124,\n     0.9129722073093167,\n     0.8600661535851554,\n     0.803966543527391,\n     0.7485985121580733,\n     0.6519825464777019,\n     0.6118168103212195,\n     0.5928412893151394,\n     0.5506281711987732],\n    [0.5157119747788669,\n     0.5808188478847512,\n     0.6951103910439576,\n     0.7268855306626568,\n     0.6343353936176197,\n     0.5785773900050866,\n     0.6644223481757985,\n     0.750068140691708,\n     0.7454753986329108,\n     0.775155697755924,\n     0.8171231529399677,\n     0.8273523516804212,\n     0.7922891017541719,\n     0.7281517796922116,\n     0.7435802555515508,\n     0.776424864900653,\n     0.760222000903533,\n     0.722641255670485,\n     0.6525649758825905,\n     0.5789285994989536,\n     0.5798683663575005,\n     0.6014339097023101,\n     0.5703720472303231],\n    [0.5095249783415519,\n     0.5492285726408191,\n     0.6226074648877431,\n     0.6501645320725685,\n     0.6167982591055491,\n     0.6207460668680193,\n     0.6622278873567949,\n     0.6704088238788177,\n     0.638141985495608,\n     0.6568121148215771,\n     0.6893469938829754,\n     0.6737028044915212,\n     0.6305307341086224,\n     0.5931359977547831,\n     0.5942684859812052,\n     0.6060861166941189,\n     0.6169178704175535,\n     0.6039108482939752,\n     0.5590958785598683,\n     0.5325744305178329,\n     0.550076775374701,\n     0.5649550327001145,\n     0.5436387826338216],\n    [0.502125474836339,\n     0.5110442472185548,\n     0.5288080841489734,\n     0.5461766900077436,\n     0.5851834595426277,\n     0.6512015415788995,\n     0.659536893148605,\n     0.596218914589234,\n     0.5454759262461782,\n     0.554665981548747,\n     0.5749510198124822,\n     0.5575484387341816,\n     0.5310333651209961,\n     0.518800295248226,\n     0.5178203355190756,\n     0.5193232807910106,\n     0.5233863302773354,\n     0.5219230164910877,\n     0.5181999675956473,\n     0.5383926265338105,\n     0.5644558032325736,\n     0.547761190620708,\n     0.5183112377354885],\n    [0.5001744706428681,\n     0.5009230308177371,\n     0.5029239744352721,\n     0.5109227419956293,\n     0.5430391562007473,\n     0.5899585913960746,\n     0.5906688455083382,\n     0.5439955799211437,\n     0.5118632354374403,\n     0.509770790664628,\n     0.5140458083002779,\n     0.5096003311528713,\n     0.5041664441289688,\n     0.5025988494249451,\n     0.50200322060596,\n     0.5016034518449342,\n     0.501879573165784,\n     0.5027292248099243,\n     0.5134962450336541,\n     0.5580006601415534,\n     0.5996593381342767,\n     0.5695882306615576,\n     0.5236910347219862],\n    [0.5000052685571122,\n     0.5000315140438197,\n     0.5002125082068176,\n     0.5019192934658976,\n     0.5093535306255318,\n     0.5201715821840207,\n     0.520193482963341,\n     0.5093887887601617,\n     0.5020345836709666,\n     0.5008168859122398,\n     0.501216644053406,\n     0.502378229363028,\n     0.5078069390346192,\n     0.51260740620722,\n     0.5076720760585771,\n     0.5017721579457877,\n     0.5002313899632039,\n     0.5007025657137953,\n     0.5082615448022472,\n     0.5416571679394592,\n     0.5895862138970251,\n     0.5907961702967596,\n     0.5514669724591066],\n    [0.5,\n     0.5000003456403315,\n     0.5000117916761213,\n     0.500150887293122,\n     0.5007643730459038,\n     0.5016552656391524,\n     0.5016552760764716,\n     0.5007647186854272,\n     0.5001550980539785,\n     0.5000495343213124,\n     0.5006563911938154,\n     0.5076318591257648,\n     0.5340828714812429,\n     0.5561564805453102,\n     0.534689275740812,\n     0.5086427133171652,\n     0.5012540728904505,\n     0.500328867350173,\n     0.502744710864922,\n     0.5202981730532751,\n     0.5702090397148498,\n     0.6089335856454607,\n     0.5898247553040831],\n    [0.5,\n     0.5000000104374336,\n     0.500000356077765,\n     0.5000045564015821,\n     0.5000230820839099,\n     0.5000499848736103,\n     0.5000499848736103,\n     0.5000230820839099,\n     0.5000045564015821,\n     0.5000314696288248,\n     0.5010303482857429,\n     0.5125536742878303,\n     0.5561561392649289,\n     0.5933394182408079,\n     0.5635227424815701,\n     0.5250831310810327,\n     0.5086430588542201,\n     0.5017601659695183,\n     0.5009226851785832,\n     0.5088852463123228,\n     0.5432988054461093,\n     0.5979553344511409,\n     0.6292582890727653],\n    [0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5000188713226665,\n     0.5006249320540425,\n     0.5076315135659574,\n     0.5346892653536178,\n     0.5635227424815701,\n     0.5678197338764431,\n     0.5635227424815701,\n     0.534689275740812,\n     0.5076360689054465,\n     0.5007996972718268,\n     0.5027979086299622,\n     0.5212182501512355,\n     0.5808024982242711,\n     0.6532322239172933],\n    [0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5000042107612507,\n     0.5001394412582952,\n     0.5017298489803317,\n     0.5086427133171653,\n     0.5250831310810327,\n     0.5635227424815701,\n     0.5933394182408079,\n     0.5561561392649289,\n     0.5125540197102714,\n     0.5010606550742566,\n     0.5008143577950571,\n     0.5088625170303274,\n     0.5426711215122509,\n     0.5901430048383121],\n    [0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5000003456403315,\n     0.5000114460357901,\n     0.5001583125792954,\n     0.5012498621555999,\n     0.5086427133171652,\n     0.5346892653536178,\n     0.5561561392649289,\n     0.5340786802831768,\n     0.5076130026239044,\n     0.5006294884484549,\n     0.5001667341009071,\n     0.5018570568139001,\n     0.5093423490394329,\n     0.5201774555674541],\n    [0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5000000104374336,\n     0.5000003456403315,\n     0.5000084215225008,\n     0.5001583125792954,\n     0.5017298489803317,\n     0.5076315135659574,\n     0.5125536742878303,\n     0.5076129921888906,\n     0.5016987462322994,\n     0.5001397868985997,\n     0.5000160024373689,\n     0.500150887293122,\n     0.5007643730459038,\n     0.5016552656391524],\n    [0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5000003456403315,\n     0.5000114460357901,\n     0.5001394412582952,\n     0.5006249320540425,\n     0.5010303378483535,\n     0.5006249320540425,\n     0.5001394412582952,\n     0.5000114564732236,\n     0.5000007017180965,\n     0.5000045564015821,\n     0.5000230820839099,\n     0.5000499848736103],\n    [0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5000000104374336,\n     0.5000003456403315,\n     0.5000042107612507,\n     0.5000188713226665,\n     0.5000311135510611,\n     0.5000188713226665,\n     0.5000042107612507,\n     0.5000003456403315,\n     0.5000000104374336,\n     0.5,\n     0.5,\n     0.5],\n    [0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5],\n    [0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5,\n     0.5]])])"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"# CPU Embeddings\n# Create embeddings for test instances, to compute MMR and top-k metrics\nlatent_vectors = {}\nfor playerId, teamId, season, heatmaps in filtered_test_instances:\n    vectors_with_ids = []\n    for heatmaps_id, heatmap in heatmaps:\n        # Convert heatmap to tensor and move to the device\n        hm_tensor = torch.tensor(heatmap, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n        bottleneck, _ = autoencoder.encoder(hm_tensor)  # Get the bottleneck representation, ignore the indices_list\n        vector = bottleneck.detach().cpu().numpy()  # Detach and convert to NumPy array\n        # Append tuple of heatmapsId and latent vector\n        vectors_with_ids.append((heatmaps_id, vector))\n    latent_vectors[(playerId, teamId, season)] = vectors_with_ids","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_keys = len(latent_vectors)\nprint(f\"Number of unique (playerId, teamId, season) combinations: {num_keys}\")\ntotal_latent_vectors = sum(len(vectors) for vectors in latent_vectors.values())\nprint(f\"Total number of latent vectors: {total_latent_vectors}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create GPU-based latent_vectors_torch embeddings for similarity computation\nlatent_vectors_torch = {}\nfor playerId, teamId, season, heatmaps in filtered_test_instances:\n    vectors_with_ids = []\n    for heatmaps_id, heatmap in heatmaps:\n        # Convert heatmap to a PyTorch tensor and move it to the GPU\n        hm_tensor = torch.tensor(heatmap, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n        bottleneck, _ = autoencoder.encoder(hm_tensor)  # Get the bottleneck representation\n        # Store the tensor directly without detaching or moving it to CPU\n        vectors_with_ids.append((heatmaps_id, bottleneck))\n    latent_vectors_torch[(playerId, teamId, season)] = vectors_with_ids\n\nnum_keys = len(latent_vectors_torch)\nprint(f\"Number of unique (playerId, teamId, season) combinations: {num_keys}\")\ntotal_latent_vectors_torch = sum(len(vectors) for vectors in latent_vectors_torch.values())\nprint(f\"Total number of latent vectors: {total_latent_vectors_torch}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T20:28:45.465736Z","iopub.execute_input":"2024-12-29T20:28:45.466032Z","iopub.status.idle":"2024-12-29T20:28:45.536159Z","shell.execute_reply.started":"2024-12-29T20:28:45.466009Z","shell.execute_reply":"2024-12-29T20:28:45.535092Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3d122cfd8bad>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create GPU-based latent_vectors_torch embeddings for similarity computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlatent_vectors_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mplayerId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteamId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheatmaps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_test_instances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvectors_with_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mheatmaps_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheatmap\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheatmaps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'filtered_test_instances' is not defined"],"ename":"NameError","evalue":"name 'filtered_test_instances' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"num_keys = len(latent_vectors_torch)\nprint(f\"Number of unique (playerId, teamId, season) combinations: {num_keys}\")\ntotal_latent_vectors_torch = sum(len(vectors) for vectors in latent_vectors_torch.values())\nprint(f\"Total number of latent vectors: {total_latent_vectors_torch}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T19:00:41.320789Z","iopub.execute_input":"2024-12-29T19:00:41.321086Z","iopub.status.idle":"2024-12-29T19:00:41.326473Z","shell.execute_reply.started":"2024-12-29T19:00:41.321065Z","shell.execute_reply":"2024-12-29T19:00:41.325688Z"}},"outputs":[{"name":"stdout","text":"Number of unique (playerId, teamId, season) combinations: 1016\nTotal number of latent vectors: 2193\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Use this to leverage GPU\nimport torch\nimport numpy as np\n\nresults = []\nk_values = [1, 3, 5, 10]  # List of k values\n\n# Loop through each query vector\nfor (playerId, teamId, season), vectors_with_ids in latent_vectors_torch.items():\n    for i, (heatmaps_id, query_vector) in enumerate(vectors_with_ids):\n        similarities = []  # Store similarities for the current query vector\n\n        # Compare with all other vectors in the dataset\n        for (other_playerId, other_teamId, other_season), other_vectors_with_ids in latent_vectors_torch.items():\n            for other_heatmaps_id, other_vector in other_vectors_with_ids:\n                if other_heatmaps_id == heatmaps_id:  # Skip self-comparison\n                    continue\n                \n                # Compute cosine similarity\n                sim = torch.nn.functional.cosine_similarity(\n                    query_vector.flatten().unsqueeze(0),\n                    other_vector.flatten().unsqueeze(0)\n                ).item()\n                similarities.append((sim, (other_playerId, other_teamId, other_season)))\n\n        # Sort by similarity in descending order\n        similarities.sort(reverse=True, key=lambda x: x[0])\n\n        # Evaluate rankings for each k\n        ranks = {k: [key == (playerId, teamId, season) for _, key in similarities[:k]] for k in k_values}\n\n        # Calculate top-k accuracy for each k value\n        top_k_values = {k: 1 if any(ranks[k]) else 0 for k in k_values}\n\n        # Calculate MRR\n        mrr = 1 / (ranks[10].index(True) + 1) if any(ranks[10]) else 0  # MRR for k=10\n\n        # Store the result for the current query\n        results.append({\n            'query': (playerId, teamId, season),\n            'mrr': mrr,\n            'top_k_values': top_k_values,  # Dictionary with top-k accuracy for each k\n        })\n\n# Calculate averages for each k\navg_mrr = np.mean([result['mrr'] for result in results])\navg_top_k_values = {k: np.mean([result['top_k_values'][k] for result in results]) for k in k_values}\n\n# Output results\nprint(f\"Average MRR: {avg_mrr:.4f}\")\nfor k, avg_top_k in avg_top_k_values.items():\n    print(f\"Average Top-{k} Accuracy: {avg_top_k:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T19:00:44.517314Z","iopub.execute_input":"2024-12-29T19:00:44.517666Z","iopub.status.idle":"2024-12-29T19:12:34.050157Z","shell.execute_reply.started":"2024-12-29T19:00:44.517631Z","shell.execute_reply":"2024-12-29T19:12:34.049406Z"}},"outputs":[{"name":"stdout","text":"Average MRR: 0.1021\nAverage Top-1 Accuracy: 0.0552\nAverage Top-3 Accuracy: 0.1222\nAverage Top-5 Accuracy: 0.1569\nAverage Top-10 Accuracy: 0.2371\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nresults = []\nk_values = [1, 3, 5, 10]  # List of k values\n\nfor (playerId, teamId, season), vectors_with_ids in latent_vectors.items():\n    # Loop over each query vector for the current (playerId, teamId, season)\n    for i, (heatmaps_id, query_vector) in enumerate(vectors_with_ids):\n        # Compute similarity between the query vector and all other vectors\n        similarities = []\n        for (other_playerId, other_teamId, other_season), other_vectors_with_ids in latent_vectors.items():\n            for other_heatmaps_id, other_vector in other_vectors_with_ids:\n                # Skip comparison if the heatmap is the same (i.e., avoid comparing the vector with itself)\n                if other_heatmaps_id == heatmaps_id:\n                    continue\n                # Compute cosine similarity between query vector and other vector\n                sim = cosine_similarity(query_vector.reshape(1, -1), other_vector.reshape(1, -1))[0][0]\n                similarities.append((sim, (other_playerId, other_teamId, other_season)))\n        \n        # Sort by similarity in descending order\n        similarities.sort(reverse=True, key=lambda x: x[0])\n\n        # Evaluate rankings for each k\n        ranks = {k: [key == (playerId, teamId, season) for _, key in similarities[:k]] for k in k_values}\n        \n        # Calculate top-k accuracy for each k value\n        top_k_values = {k: 1 if any(ranks[k]) else 0 for k in k_values}\n\n        # Calculate MRR\n        mrr = 1 / (ranks[10].index(True) + 1) if any(ranks[10]) else 0  # MRR for k=10 (you can adjust this as needed)\n\n        results.append({\n            'query': (playerId, teamId, season),\n            'mrr': mrr,\n            'top_k_values': top_k_values,  # Dictionary with top-k accuracy for each k\n        })\n\n# Calculate averages for each k\navg_mrr = np.mean([result['mrr'] for result in results])\navg_top_k_values = {k: np.mean([result['top_k_values'][k] for result in results]) for k in k_values}\n\nprint(f\"Average MRR: {avg_mrr:.4f}\")\nfor k, avg_top_k in avg_top_k_values.items():\n    print(f\"Average Top-{k} Accuracy: {avg_top_k:.4f}\")\n\n# In the comparison with Matteo's results, from top-3 upwards I should expect higher scores since in my method I have more than 1 correct instance retrievable for some players (the ones with more than 2 heatmaps)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntest_set = HeatmapCSVLoader(csv_path=\"/kaggle/input/sigmoid-pass-heatmaps/passes-start-test.csv\")\ntest_loader = DataLoader(test_set, batch_size=16, shuffle=False)\n\n# Get a batch of images\nautoencoder.eval()\nimages, ids, playerIds, teamIds, seasons = next(iter(test_loader))\nimages = images.to(device)  # Move images to the selected device\n\n# Reconstruct images\nwith torch.no_grad():\n    reconstructed = autoencoder(images)\n\n# Plot original and reconstructed images (limit to 5 for better visualization)\nnum_examples = min(len(images), 5)  # Display up to 5 examples\nfig, axes = plt.subplots(2, num_examples, figsize=(15, 5))\nfor i in range(num_examples):\n    # Original\n    axes[0, i].imshow(images[i].cpu().squeeze(), cmap='gray')  # Move back to CPU for plotting\n    if isinstance(ids, (list, tuple)):\n        axes[0, i].set_title(f\"Original ({ids[i]})\")\n    else:\n        axes[0, i].set_title(\"Original\")\n    axes[0, i].axis('off')\n    \n    # Reconstructed\n    axes[1, i].imshow(reconstructed[i].cpu().squeeze(), cmap='gray')  # Move back to CPU for plotting\n    axes[1, i].set_title(\"Reconstructed\")\n    axes[1, i].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encoder class for grid search\n\nclass Encoder(nn.Module):\n    def __init__(self, input_channels, bottleneck_size, dropout_rate):\n        super(Encoder, self).__init__()\n        \n        # Conv Block 1\n        self.conv_block1 = nn.Sequential(\n            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Dropout2d(p=dropout_rate)\n        )\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        \n        # Conv Block 2\n        self.conv_block2 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, padding=1, groups=64),\n            nn.Conv2d(128, 128, kernel_size=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1, groups=128),\n            nn.Conv2d(128, 128, kernel_size=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Dropout2d(p=dropout_rate)\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        \n        # Conv Block 3\n        self.conv_block3 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Dropout2d(p=dropout_rate)\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        \n        # Compute flattened size after convolutions and pooling\n        self.flattened_size = 256 * 4 * 2  # Computed from input size [1, 35, 23]\n        self.flatten = nn.Flatten()\n        self.dense = nn.Sequential(\n            nn.Linear(self.flattened_size, bottleneck_size),\n            nn.Dropout(p=dropout_rate)\n        )\n    \n    def forward(self, x):\n        # Conv Block 1\n        x = self.conv_block1(x)\n        x, indices1 = self.pool1(x)\n        \n        # Conv Block 2\n        x = self.conv_block2(x)\n        x, indices2 = self.pool2(x)\n        \n        # Conv Block 3\n        x = self.conv_block3(x)\n        x, indices3 = self.pool3(x)\n\n        # Flatten and bottleneck\n        x = self.flatten(x)\n        bottleneck = self.dense(x)\n        \n        return bottleneck, [indices1, indices2, indices3]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:25:48.745070Z","iopub.execute_input":"2024-12-30T10:25:48.745469Z","iopub.status.idle":"2024-12-30T10:25:48.755415Z","shell.execute_reply.started":"2024-12-30T10:25:48.745439Z","shell.execute_reply":"2024-12-30T10:25:48.754552Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Decoder class for grid search\n\nclass Decoder(nn.Module):\n    def __init__(self, input_channels, bottleneck_size, dropout_rate):\n        super(Decoder, self).__init__()\n\n        self.dense = nn.Sequential(\n            nn.Linear(bottleneck_size, 256 * 4 * 2),\n            nn.ReLU()\n        )\n\n        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(256, 4, 2))\n\n        self.unpool3 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.deconv_block3 = nn.Sequential(\n            nn.ConvTranspose2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU()\n        )\n\n        self.unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.deconv_block2 = nn.Sequential(\n            nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU()\n        )\n\n        self.unpool1 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.deconv_block1 = nn.Sequential(\n            nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, input_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(input_channels),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x, indices_list, output_sizes):\n        # Dense layer and unflatten\n        x = self.dense(x)\n        x = self.unflatten(x)\n\n        # Unpooling and deconvolution\n        x = self.unpool3(x, indices_list[2], output_size=output_sizes[2])\n        x = self.deconv_block3(x)\n\n        x = self.unpool2(x, indices_list[1], output_size=output_sizes[1])\n        x = self.deconv_block2(x)\n\n        x = self.unpool1(x, indices_list[0], output_size=output_sizes[0])\n        x = self.deconv_block1(x)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:25:54.421445Z","iopub.execute_input":"2024-12-30T10:25:54.421784Z","iopub.status.idle":"2024-12-30T10:25:54.429956Z","shell.execute_reply.started":"2024-12-30T10:25:54.421757Z","shell.execute_reply":"2024-12-30T10:25:54.429194Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# To be deleted (?)\n# Decoder with ReLU also in deconvolution layer of block1, to match the convolution layer of the Encoder\n\nclass Decoder(nn.Module):\n    def __init__(self, input_channels, bottleneck_size, dropout_rate):\n        super(Decoder, self).__init__()\n\n        self.dense = nn.Sequential(\n            nn.Linear(bottleneck_size, 256 * 4 * 2),\n            nn.ReLU()\n        )\n\n        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(256, 4, 2))\n\n        self.unpool3 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.deconv_block3 = nn.Sequential(\n            nn.ConvTranspose2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU()\n        )\n\n        self.unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.deconv_block2 = nn.Sequential(\n            nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU()\n        )\n\n        self.unpool1 = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        self.deconv_block1 = nn.Sequential(\n            nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, input_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(input_channels),\n            nn.ReLU()\n        )\n\n    def forward(self, x, indices_list, output_sizes):\n        # Dense layer and unflatten\n        x = self.dense(x)\n        x = self.unflatten(x)\n\n        # Unpooling and deconvolution\n        x = self.unpool3(x, indices_list[2], output_size=output_sizes[2])\n        x = self.deconv_block3(x)\n\n        x = self.unpool2(x, indices_list[1], output_size=output_sizes[1])\n        x = self.deconv_block2(x)\n\n        x = self.unpool1(x, indices_list[0], output_size=output_sizes[0])\n        x = self.deconv_block1(x)\n\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Autoencoder class for grid search\n\nclass Autoencoder(nn.Module):\n    def __init__(self, input_channels, bottleneck_size, dropout_rate):\n        super(Autoencoder, self).__init__()\n        self.encoder = Encoder(input_channels, bottleneck_size, dropout_rate)\n        self.decoder = Decoder(input_channels, bottleneck_size, dropout_rate)\n\n    def forward(self, x):\n        # Encoder\n        bottleneck, indices_list = self.encoder(x)\n\n        # Verify output_sizes match shapes before pooling\n        output_sizes = [\n            torch.Size([x.size(0), 1, 35, 23]),    # Before Pool1\n            torch.Size([x.size(0), 64, 17, 11]),  # Before Pool2\n            torch.Size([x.size(0), 128, 8, 5])    # Before Pool3\n        ]\n\n        # Decoder\n        reconstructed = self.decoder(bottleneck, indices_list, output_sizes)\n        return reconstructed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:26:00.564866Z","iopub.execute_input":"2024-12-30T10:26:00.565201Z","iopub.status.idle":"2024-12-30T10:26:00.570064Z","shell.execute_reply.started":"2024-12-30T10:26:00.565174Z","shell.execute_reply":"2024-12-30T10:26:00.569165Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Define EarlyStopping class\n\nclass EarlyStopping:\n    def __init__(self, patience=5, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n\n    def __call__(self, val_loss):\n        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:26:05.406038Z","iopub.execute_input":"2024-12-30T10:26:05.406519Z","iopub.status.idle":"2024-12-30T10:26:05.411693Z","shell.execute_reply.started":"2024-12-30T10:26:05.406480Z","shell.execute_reply":"2024-12-30T10:26:05.410845Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Hyperparameter grid\n\nlearning_rates = [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\nbottleneck_sizes = [16, 32, 64]\noptimizers = ['Adam', 'AdamW', 'SGD']\nloss_functions = ['MSELoss'] #'SmoothL1Loss'\nbatch_sizes = [16, 32, 64]\nweight_decays = [0, 1e-4, 1e-3]\ndropout_rates = [0.0, 0.1, 0.2]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# testing with one parameter per type\n\nlearning_rates = [1e-4,1e-5]\nbottleneck_sizes = [32,64,128]\noptimizers = ['Adam']\nloss_functions = ['MSELoss']\nbatch_sizes = [64]\nweight_decays = [0]\ndropout_rates = [0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:16:03.816249Z","iopub.execute_input":"2024-12-30T11:16:03.816564Z","iopub.status.idle":"2024-12-30T11:16:03.820642Z","shell.execute_reply.started":"2024-12-30T11:16:03.816533Z","shell.execute_reply":"2024-12-30T11:16:03.819763Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# Function to get optimizer\n\ndef get_optimizer(optimizer_name, model_params, lr, weight_decay):\n    if optimizer_name == 'Adam':\n        return optim.Adam(model_params, lr=lr, weight_decay=weight_decay)\n    elif optimizer_name == 'AdamW':\n        return optim.AdamW(model_params, lr=lr, weight_decay=weight_decay)\n    elif optimizer_name == 'SGD':\n        return optim.SGD(model_params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n    else:\n        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:29:00.339318Z","iopub.execute_input":"2024-12-30T10:29:00.339637Z","iopub.status.idle":"2024-12-30T10:29:00.344006Z","shell.execute_reply.started":"2024-12-30T10:29:00.339613Z","shell.execute_reply":"2024-12-30T10:29:00.343262Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Function to get loss function\n\ndef get_loss_function(loss_name):\n    if loss_name == 'MSELoss':\n        return nn.MSELoss()\n    elif loss_name == 'SmoothL1Loss':\n        return nn.SmoothL1Loss()\n    else:\n        raise ValueError(f\"Unknown loss function: {loss_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:29:03.098289Z","iopub.execute_input":"2024-12-30T10:29:03.098596Z","iopub.status.idle":"2024-12-30T10:29:03.103274Z","shell.execute_reply.started":"2024-12-30T10:29:03.098574Z","shell.execute_reply":"2024-12-30T10:29:03.102300Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import json\nimport time\n\nresults = []\n\n# Function to add a result\ndef save_result(hyperparams, metrics):\n    result = {\n        \"hyperparameters\": hyperparams,\n        \"metrics\": metrics,\n        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    }\n    results.append(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:29:05.956458Z","iopub.execute_input":"2024-12-30T10:29:05.956752Z","iopub.status.idle":"2024-12-30T10:29:05.961220Z","shell.execute_reply.started":"2024-12-30T10:29:05.956728Z","shell.execute_reply":"2024-12-30T10:29:05.960324Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"train_set = HeatmapCSVLoader(csv_path=\"/kaggle/input/sigmoid-10-chunks/passes-start-train.csv\")\nval_set = HeatmapCSVLoader(csv_path=\"/kaggle/input/sigmoid-10-chunks/passes-start-validation.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:11:57.294589Z","iopub.execute_input":"2024-12-30T11:11:57.294925Z","iopub.status.idle":"2024-12-30T11:12:10.491244Z","shell.execute_reply.started":"2024-12-30T11:11:57.294898Z","shell.execute_reply":"2024-12-30T11:12:10.490527Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Create GPU-based latent_vectors_torch embeddings for similarity computation\n\ndef create_test_embeddings(embedder,filtered_test_instances):\n    latent_vectors_torch = {}\n    for playerId, teamId, season, heatmaps in filtered_test_instances:\n        vectors_with_ids = []\n        for heatmaps_id, heatmap in heatmaps:\n            # Convert heatmap to a PyTorch tensor and move it to the GPU\n            hm_tensor = torch.tensor(heatmap, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n            bottleneck, _ = embedder(hm_tensor)  # Get the bottleneck representation\n            # Store the tensor directly without detaching or moving it to CPU\n            vectors_with_ids.append((heatmaps_id, bottleneck))\n        latent_vectors_torch[(playerId, teamId, season)] = vectors_with_ids\n    \n    num_keys = len(latent_vectors_torch)\n    print(f\"Number of unique (playerId, teamId, season) combinations: {num_keys}\")\n    total_latent_vectors_torch = sum(len(vectors) for vectors in latent_vectors_torch.values())\n    print(f\"Total number of latent vectors: {total_latent_vectors_torch}\")\n    return latent_vectors_torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:33:11.802011Z","iopub.execute_input":"2024-12-30T10:33:11.802374Z","iopub.status.idle":"2024-12-30T10:33:11.807907Z","shell.execute_reply.started":"2024-12-30T10:33:11.802347Z","shell.execute_reply":"2024-12-30T10:33:11.806948Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Function to compute similarity (GPU/CPU)\n\nimport torch\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef compute_similarity(latent_vectors, k_values, use_gpu):\n    \"\"\"\n    Computes MRR and Top-k accuracy metrics for a set of latent vectors.\n\n    Parameters:\n        latent_vectors (dict): Dictionary of latent vectors.\n        k_values (list): List of k values for top-k accuracy.\n        use_gpu (bool): Whether to use GPU for computations.\n\n    Returns:\n        avg_mrr (float): Average MRR.\n        avg_top_k_values (dict): Average Top-k accuracy for each k.\n    \"\"\"\n    results = []\n\n    for (playerId, teamId, season), vectors_with_ids in latent_vectors.items():\n        for i, (heatmaps_id, query_vector) in enumerate(vectors_with_ids):\n            similarities = []\n\n            for (other_playerId, other_teamId, other_season), other_vectors_with_ids in latent_vectors.items():\n                for other_heatmaps_id, other_vector in other_vectors_with_ids:\n                    if other_heatmaps_id == heatmaps_id:\n                        continue\n\n                    if use_gpu:\n                        query_tensor = torch.tensor(query_vector, dtype=torch.float32).flatten().unsqueeze(0).cuda()\n                        other_tensor = torch.tensor(other_vector, dtype=torch.float32).flatten().unsqueeze(0).cuda()\n                        sim = torch.nn.functional.cosine_similarity(query_tensor, other_tensor).item()\n                    else:\n                        sim = cosine_similarity(query_vector.reshape(1, -1), other_vector.reshape(1, -1))[0][0]\n\n                    similarities.append((sim, (other_playerId, other_teamId, other_season)))\n\n            similarities.sort(reverse=True, key=lambda x: x[0])\n            ranks = {k: [key == (playerId, teamId, season) for _, key in similarities[:k]] for k in k_values}\n            top_k_values = {k: 1 if any(ranks[k]) else 0 for k in k_values}\n            mrr = 1 / (ranks[10].index(True) + 1) if any(ranks[10]) else 0\n\n            results.append({\n                'query': (playerId, teamId, season),\n                'mrr': mrr,\n                'top_k_values': top_k_values,\n            })\n\n    avg_mrr = np.mean([result['mrr'] for result in results])\n    avg_top_k_values = {k: np.mean([result['top_k_values'][k] for result in results]) for k in k_values}\n\n    return avg_mrr, avg_top_k_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T10:33:41.691427Z","iopub.execute_input":"2024-12-30T10:33:41.691764Z","iopub.status.idle":"2024-12-30T10:33:41.703456Z","shell.execute_reply.started":"2024-12-30T10:33:41.691735Z","shell.execute_reply":"2024-12-30T10:33:41.702333Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Define result outside of model train cell, so I can accumulate multiple grid search results in the same file\nresults = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:18:18.167727Z","iopub.execute_input":"2024-12-30T11:18:18.168024Z","iopub.status.idle":"2024-12-30T11:18:18.171525Z","shell.execute_reply.started":"2024-12-30T11:18:18.168001Z","shell.execute_reply":"2024-12-30T11:18:18.170774Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Training with grid search and evaluation on MSE and Retrieval Task\n# Here the goal is to find the best hyperparameters to then use them to train the final (large) model\n\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport time\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\nk_values = [1, 3, 5, 10]  # List of k values\nuse_gpu = True\n\n#results = []\nnum_epochs = 20\n\nstart_time = time.time()\n# Grid Search Loop\nfor lr, bottleneck, optimizer_name, loss_name, batch_size, weight_decay, dropout_rate in product(\n    learning_rates, bottleneck_sizes, optimizers, loss_functions, batch_sizes, weight_decays, dropout_rates\n):\n    # Prepare DataLoader\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size)\n    \n    # Initialize model\n    autoencoder = Autoencoder(input_channels=1, bottleneck_size=bottleneck, dropout_rate=dropout_rate).to(device)\n    \n    # Get optimizer and loss function\n    optimizer = get_optimizer(optimizer_name, autoencoder.parameters(), lr, weight_decay)\n    criterion = get_loss_function(loss_name)\n\n    # Train model\n    early_stopping = EarlyStopping(patience=5)\n    model_train_start_time = time.time()\n    print(f\"Training model with parameters: \\nLearning Rate: {lr}, Bottleneck: {bottleneck}, Optimizer: {optimizer_name}, Loss Function: {loss_name},\\n Batch Size: {batch_size}, Weight Decay: {weight_decay}, Dropout Rate: {dropout_rate}\")\n    for epoch in range(num_epochs):\n        autoencoder.train()\n        epoch_loss = 0\n        for batch in train_loader:\n            images, ids, playerIds, teamIds, seasons = batch  # Unpack images and metadata\n            images = images.to(device)\n            \n            #forward pass \n            reconstructed = autoencoder(images)\n            loss = criterion(reconstructed, images)\n            \n            #backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n            \n        train_loss = epoch_loss / len(train_loader)  # Average training loss\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Batch Training Loss: {train_loss:.4f}\")\n        \n        # Validate model\n        autoencoder.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                images, ids, playerIds, teamIds, seasons = batch # Unpack images and metadata\n                images = images.to(device)\n                reconstructed = autoencoder(images)\n                val_loss += criterion(reconstructed, images).item() \n        \n        # Check early stopping\n        early_stopping(val_loss)\n        if early_stopping.early_stop:\n            break\n\n        val_loss /= len(val_loader)  # Average validation loss  # Average validation loss\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}\")\n\n    model_train_end_time = time.time()\n    print(f\"Model train time: {model_train_end_time - model_train_start_time:.2f} seconds\")\n    \n    # Create embeddings with current encoder\n    print(\"Creating embeddings...\")\n    latent_vectors_torch = create_test_embeddings(autoencoder.encoder,filtered_test_instances)\n    print(\"Computing metrics...\")\n    metrics_start_time = time.time()\n    avg_mrr,avg_top_k_values = compute_similarity(latent_vectors_torch, k_values, use_gpu)\n    metrics_end_time = time.time()\n    print(f\"Metrics computation time: {metrics_end_time - metrics_start_time:.2f} seconds\")\n\n    # Save result for the current configuration\n    hyperparams = {\n        \"learning_rate\": lr,\n        \"bottleneck_size\": bottleneck,\n        \"optimizer\": optimizer_name,\n        \"loss_function\": loss_name,\n        \"batch_size\": batch_size,\n        \"weight_decay\": weight_decay,\n        \"dropout_rate\": dropout_rate\n    }\n    metrics = {\n        \"train_loss\": train_loss,\n        \"val_loss\": val_loss,\n        \"epochs_run\": epoch + 1,\n        \"avg_mrr\": avg_mrr\n    }\n\n    print(f\"Average MRR: {avg_mrr:.4f}\")\n    for k, avg_top_k in avg_top_k_values.items():\n        metrics[f\"avg_top_{k}\"] = avg_top_k\n        print(f\"Average Top-{k} Accuracy: {avg_top_k:.4f}\")\n\n    save_result(hyperparams, metrics)\n    # Save results to file after every configuration\n    with open(\"results.json\", \"w\") as f:\n        json.dump(results, f, indent=4)\n\nend_time = time.time()\nprint(f\"Execution time: {end_time - start_time:.2f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T11:18:24.055268Z","iopub.execute_input":"2024-12-30T11:18:24.055564Z","iopub.status.idle":"2024-12-30T13:50:24.360397Z","shell.execute_reply.started":"2024-12-30T11:18:24.055541Z","shell.execute_reply":"2024-12-30T13:50:24.359638Z"}},"outputs":[{"name":"stdout","text":"Training model with parameters: \nLearning Rate: 0.0001, Bottleneck: 32, Optimizer: Adam, Loss Function: MSELoss,\n Batch Size: 64, Weight Decay: 0, Dropout Rate: 0\nEpoch [1/20], Average Batch Training Loss: 0.0310\nEpoch [1/20], Validation Loss: 0.0254\nEpoch [2/20], Average Batch Training Loss: 0.0228\nEpoch [2/20], Validation Loss: 0.0198\nEpoch [3/20], Average Batch Training Loss: 0.0180\nEpoch [3/20], Validation Loss: 0.0157\nEpoch [4/20], Average Batch Training Loss: 0.0142\nEpoch [4/20], Validation Loss: 0.0132\nEpoch [5/20], Average Batch Training Loss: 0.0111\nEpoch [5/20], Validation Loss: 0.0102\nEpoch [6/20], Average Batch Training Loss: 0.0087\nEpoch [6/20], Validation Loss: 0.0079\nEpoch [7/20], Average Batch Training Loss: 0.0068\nEpoch [7/20], Validation Loss: 0.0057\nEpoch [8/20], Average Batch Training Loss: 0.0052\nEpoch [8/20], Validation Loss: 0.0043\nEpoch [9/20], Average Batch Training Loss: 0.0039\nEpoch [9/20], Validation Loss: 0.0034\nEpoch [10/20], Average Batch Training Loss: 0.0029\nEpoch [10/20], Validation Loss: 0.0024\nEpoch [11/20], Average Batch Training Loss: 0.0021\nEpoch [11/20], Validation Loss: 0.0019\nEpoch [12/20], Average Batch Training Loss: 0.0015\nEpoch [12/20], Validation Loss: 0.0012\nEpoch [13/20], Average Batch Training Loss: 0.0011\nEpoch [13/20], Validation Loss: 0.0009\nEpoch [14/20], Average Batch Training Loss: 0.0008\nEpoch [14/20], Validation Loss: 0.0006\nEpoch [15/20], Average Batch Training Loss: 0.0006\nEpoch [15/20], Validation Loss: 0.0005\nEpoch [16/20], Average Batch Training Loss: 0.0004\nEpoch [16/20], Validation Loss: 0.0003\nEpoch [17/20], Average Batch Training Loss: 0.0004\nEpoch [17/20], Validation Loss: 0.0003\nEpoch [18/20], Average Batch Training Loss: 0.0003\nEpoch [18/20], Validation Loss: 0.0002\nEpoch [19/20], Average Batch Training Loss: 0.0003\nEpoch [19/20], Validation Loss: 0.0002\nEpoch [20/20], Average Batch Training Loss: 0.0002\nEpoch [20/20], Validation Loss: 0.0002\nModel train time: 687.25 seconds\nCreating embeddings...\nNumber of unique (playerId, teamId, season) combinations: 1016\nTotal number of latent vectors: 2193\nComputing metrics...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-19-1e59b7d7a165>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  query_tensor = torch.tensor(query_vector, dtype=torch.float32).flatten().unsqueeze(0).cuda()\n<ipython-input-19-1e59b7d7a165>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  other_tensor = torch.tensor(other_vector, dtype=torch.float32).flatten().unsqueeze(0).cuda()\n","output_type":"stream"},{"name":"stdout","text":"Metrics computation time: 824.37 seconds\nAverage MRR: 0.0852\nAverage Top-1 Accuracy: 0.0433\nAverage Top-3 Accuracy: 0.0962\nAverage Top-5 Accuracy: 0.1404\nAverage Top-10 Accuracy: 0.2134\nTraining model with parameters: \nLearning Rate: 0.0001, Bottleneck: 64, Optimizer: Adam, Loss Function: MSELoss,\n Batch Size: 64, Weight Decay: 0, Dropout Rate: 0\nEpoch [1/20], Average Batch Training Loss: 0.0290\nEpoch [1/20], Validation Loss: 0.0222\nEpoch [2/20], Average Batch Training Loss: 0.0201\nEpoch [2/20], Validation Loss: 0.0183\nEpoch [3/20], Average Batch Training Loss: 0.0162\nEpoch [3/20], Validation Loss: 0.0150\nEpoch [4/20], Average Batch Training Loss: 0.0132\nEpoch [4/20], Validation Loss: 0.0118\nEpoch [5/20], Average Batch Training Loss: 0.0106\nEpoch [5/20], Validation Loss: 0.0097\nEpoch [6/20], Average Batch Training Loss: 0.0085\nEpoch [6/20], Validation Loss: 0.0071\nEpoch [7/20], Average Batch Training Loss: 0.0069\nEpoch [7/20], Validation Loss: 0.0063\nEpoch [8/20], Average Batch Training Loss: 0.0057\nEpoch [8/20], Validation Loss: 0.0050\nEpoch [9/20], Average Batch Training Loss: 0.0046\nEpoch [9/20], Validation Loss: 0.0040\nEpoch [10/20], Average Batch Training Loss: 0.0037\nEpoch [10/20], Validation Loss: 0.0028\nEpoch [11/20], Average Batch Training Loss: 0.0025\nEpoch [11/20], Validation Loss: 0.0021\nEpoch [12/20], Average Batch Training Loss: 0.0019\nEpoch [12/20], Validation Loss: 0.0018\nEpoch [13/20], Average Batch Training Loss: 0.0015\nEpoch [13/20], Validation Loss: 0.0012\nEpoch [14/20], Average Batch Training Loss: 0.0010\nEpoch [14/20], Validation Loss: 0.0009\nEpoch [15/20], Average Batch Training Loss: 0.0007\nEpoch [15/20], Validation Loss: 0.0005\nEpoch [16/20], Average Batch Training Loss: 0.0006\nEpoch [16/20], Validation Loss: 0.0004\nEpoch [17/20], Average Batch Training Loss: 0.0005\nEpoch [17/20], Validation Loss: 0.0004\nEpoch [18/20], Average Batch Training Loss: 0.0004\nEpoch [18/20], Validation Loss: 0.0004\nEpoch [19/20], Average Batch Training Loss: 0.0004\nEpoch [19/20], Validation Loss: 0.0012\nEpoch [20/20], Average Batch Training Loss: 0.0003\nEpoch [20/20], Validation Loss: 0.0003\nModel train time: 687.54 seconds\nCreating embeddings...\nNumber of unique (playerId, teamId, season) combinations: 1016\nTotal number of latent vectors: 2193\nComputing metrics...\nMetrics computation time: 831.90 seconds\nAverage MRR: 0.0707\nAverage Top-1 Accuracy: 0.0369\nAverage Top-3 Accuracy: 0.0780\nAverage Top-5 Accuracy: 0.1135\nAverage Top-10 Accuracy: 0.1742\nTraining model with parameters: \nLearning Rate: 0.0001, Bottleneck: 128, Optimizer: Adam, Loss Function: MSELoss,\n Batch Size: 64, Weight Decay: 0, Dropout Rate: 0\nEpoch [1/20], Average Batch Training Loss: 0.0309\nEpoch [1/20], Validation Loss: 0.0266\nEpoch [2/20], Average Batch Training Loss: 0.0228\nEpoch [2/20], Validation Loss: 0.0202\nEpoch [3/20], Average Batch Training Loss: 0.0180\nEpoch [3/20], Validation Loss: 0.0160\nEpoch [4/20], Average Batch Training Loss: 0.0142\nEpoch [4/20], Validation Loss: 0.0125\nEpoch [5/20], Average Batch Training Loss: 0.0111\nEpoch [5/20], Validation Loss: 0.0098\nEpoch [6/20], Average Batch Training Loss: 0.0087\nEpoch [6/20], Validation Loss: 0.0077\nEpoch [7/20], Average Batch Training Loss: 0.0068\nEpoch [7/20], Validation Loss: 0.0060\nEpoch [8/20], Average Batch Training Loss: 0.0052\nEpoch [8/20], Validation Loss: 0.0047\nEpoch [9/20], Average Batch Training Loss: 0.0039\nEpoch [9/20], Validation Loss: 0.0033\nEpoch [10/20], Average Batch Training Loss: 0.0029\nEpoch [10/20], Validation Loss: 0.0024\nEpoch [11/20], Average Batch Training Loss: 0.0021\nEpoch [11/20], Validation Loss: 0.0016\nEpoch [12/20], Average Batch Training Loss: 0.0015\nEpoch [12/20], Validation Loss: 0.0013\nEpoch [13/20], Average Batch Training Loss: 0.0011\nEpoch [13/20], Validation Loss: 0.0008\nEpoch [14/20], Average Batch Training Loss: 0.0008\nEpoch [14/20], Validation Loss: 0.0006\nEpoch [15/20], Average Batch Training Loss: 0.0006\nEpoch [15/20], Validation Loss: 0.0005\nEpoch [16/20], Average Batch Training Loss: 0.0004\nEpoch [16/20], Validation Loss: 0.0004\nEpoch [17/20], Average Batch Training Loss: 0.0004\nEpoch [17/20], Validation Loss: 0.0003\nEpoch [18/20], Average Batch Training Loss: 0.0003\nEpoch [18/20], Validation Loss: 0.0002\nEpoch [19/20], Average Batch Training Loss: 0.0003\nEpoch [19/20], Validation Loss: 0.0002\nEpoch [20/20], Average Batch Training Loss: 0.0002\nEpoch [20/20], Validation Loss: 0.0002\nModel train time: 687.29 seconds\nCreating embeddings...\nNumber of unique (playerId, teamId, season) combinations: 1016\nTotal number of latent vectors: 2193\nComputing metrics...\nMetrics computation time: 829.83 seconds\nAverage MRR: 0.0981\nAverage Top-1 Accuracy: 0.0529\nAverage Top-3 Accuracy: 0.1145\nAverage Top-5 Accuracy: 0.1500\nAverage Top-10 Accuracy: 0.2339\nTraining model with parameters: \nLearning Rate: 1e-05, Bottleneck: 32, Optimizer: Adam, Loss Function: MSELoss,\n Batch Size: 64, Weight Decay: 0, Dropout Rate: 0\nEpoch [1/20], Average Batch Training Loss: 0.0451\nEpoch [1/20], Validation Loss: 0.0324\nEpoch [2/20], Average Batch Training Loss: 0.0300\nEpoch [2/20], Validation Loss: 0.0285\nEpoch [3/20], Average Batch Training Loss: 0.0276\nEpoch [3/20], Validation Loss: 0.0276\nEpoch [4/20], Average Batch Training Loss: 0.0262\nEpoch [4/20], Validation Loss: 0.0262\nEpoch [5/20], Average Batch Training Loss: 0.0252\nEpoch [5/20], Validation Loss: 0.0252\nEpoch [6/20], Average Batch Training Loss: 0.0244\nEpoch [6/20], Validation Loss: 0.0248\nEpoch [7/20], Average Batch Training Loss: 0.0236\nEpoch [7/20], Validation Loss: 0.0233\nEpoch [8/20], Average Batch Training Loss: 0.0229\nEpoch [8/20], Validation Loss: 0.0228\nEpoch [9/20], Average Batch Training Loss: 0.0223\nEpoch [9/20], Validation Loss: 0.0221\nEpoch [10/20], Average Batch Training Loss: 0.0217\nEpoch [10/20], Validation Loss: 0.0207\nEpoch [11/20], Average Batch Training Loss: 0.0212\nEpoch [11/20], Validation Loss: 0.0211\nEpoch [12/20], Average Batch Training Loss: 0.0206\nEpoch [12/20], Validation Loss: 0.0206\nEpoch [13/20], Average Batch Training Loss: 0.0202\nEpoch [13/20], Validation Loss: 0.0195\nEpoch [14/20], Average Batch Training Loss: 0.0197\nEpoch [14/20], Validation Loss: 0.0188\nEpoch [15/20], Average Batch Training Loss: 0.0192\nEpoch [15/20], Validation Loss: 0.0193\nEpoch [16/20], Average Batch Training Loss: 0.0187\nEpoch [16/20], Validation Loss: 0.0193\nEpoch [17/20], Average Batch Training Loss: 0.0183\nEpoch [17/20], Validation Loss: 0.0181\nEpoch [18/20], Average Batch Training Loss: 0.0179\nEpoch [18/20], Validation Loss: 0.0174\nEpoch [19/20], Average Batch Training Loss: 0.0175\nEpoch [19/20], Validation Loss: 0.0176\nEpoch [20/20], Average Batch Training Loss: 0.0170\nEpoch [20/20], Validation Loss: 0.0159\nModel train time: 684.01 seconds\nCreating embeddings...\nNumber of unique (playerId, teamId, season) combinations: 1016\nTotal number of latent vectors: 2193\nComputing metrics...\nMetrics computation time: 832.61 seconds\nAverage MRR: 0.0702\nAverage Top-1 Accuracy: 0.0356\nAverage Top-3 Accuracy: 0.0784\nAverage Top-5 Accuracy: 0.1145\nAverage Top-10 Accuracy: 0.1774\nTraining model with parameters: \nLearning Rate: 1e-05, Bottleneck: 64, Optimizer: Adam, Loss Function: MSELoss,\n Batch Size: 64, Weight Decay: 0, Dropout Rate: 0\nEpoch [1/20], Average Batch Training Loss: 0.0422\nEpoch [1/20], Validation Loss: 0.0302\nEpoch [2/20], Average Batch Training Loss: 0.0290\nEpoch [2/20], Validation Loss: 0.0282\nEpoch [3/20], Average Batch Training Loss: 0.0268\nEpoch [3/20], Validation Loss: 0.0269\nEpoch [4/20], Average Batch Training Loss: 0.0255\nEpoch [4/20], Validation Loss: 0.0249\nEpoch [5/20], Average Batch Training Loss: 0.0246\nEpoch [5/20], Validation Loss: 0.0245\nEpoch [6/20], Average Batch Training Loss: 0.0239\nEpoch [6/20], Validation Loss: 0.0233\nEpoch [7/20], Average Batch Training Loss: 0.0232\nEpoch [7/20], Validation Loss: 0.0230\nEpoch [8/20], Average Batch Training Loss: 0.0226\nEpoch [8/20], Validation Loss: 0.0221\nEpoch [9/20], Average Batch Training Loss: 0.0221\nEpoch [9/20], Validation Loss: 0.0213\nEpoch [10/20], Average Batch Training Loss: 0.0216\nEpoch [10/20], Validation Loss: 0.0211\nEpoch [11/20], Average Batch Training Loss: 0.0210\nEpoch [11/20], Validation Loss: 0.0210\nEpoch [12/20], Average Batch Training Loss: 0.0206\nEpoch [12/20], Validation Loss: 0.0195\nEpoch [13/20], Average Batch Training Loss: 0.0201\nEpoch [13/20], Validation Loss: 0.0200\nEpoch [14/20], Average Batch Training Loss: 0.0196\nEpoch [14/20], Validation Loss: 0.0185\nEpoch [15/20], Average Batch Training Loss: 0.0192\nEpoch [15/20], Validation Loss: 0.0189\nEpoch [16/20], Average Batch Training Loss: 0.0187\nEpoch [16/20], Validation Loss: 0.0183\nEpoch [17/20], Average Batch Training Loss: 0.0184\nEpoch [17/20], Validation Loss: 0.0180\nEpoch [18/20], Average Batch Training Loss: 0.0179\nEpoch [18/20], Validation Loss: 0.0184\nEpoch [19/20], Average Batch Training Loss: 0.0175\nEpoch [19/20], Validation Loss: 0.0174\nEpoch [20/20], Average Batch Training Loss: 0.0171\nEpoch [20/20], Validation Loss: 0.0163\nModel train time: 686.09 seconds\nCreating embeddings...\nNumber of unique (playerId, teamId, season) combinations: 1016\nTotal number of latent vectors: 2193\nComputing metrics...\nMetrics computation time: 830.98 seconds\nAverage MRR: 0.1032\nAverage Top-1 Accuracy: 0.0570\nAverage Top-3 Accuracy: 0.1163\nAverage Top-5 Accuracy: 0.1614\nAverage Top-10 Accuracy: 0.2376\nTraining model with parameters: \nLearning Rate: 1e-05, Bottleneck: 128, Optimizer: Adam, Loss Function: MSELoss,\n Batch Size: 64, Weight Decay: 0, Dropout Rate: 0\nEpoch [1/20], Average Batch Training Loss: 0.0430\nEpoch [1/20], Validation Loss: 0.0303\nEpoch [2/20], Average Batch Training Loss: 0.0289\nEpoch [2/20], Validation Loss: 0.0271\nEpoch [3/20], Average Batch Training Loss: 0.0268\nEpoch [3/20], Validation Loss: 0.0257\nEpoch [4/20], Average Batch Training Loss: 0.0255\nEpoch [4/20], Validation Loss: 0.0240\nEpoch [5/20], Average Batch Training Loss: 0.0247\nEpoch [5/20], Validation Loss: 0.0240\nEpoch [6/20], Average Batch Training Loss: 0.0239\nEpoch [6/20], Validation Loss: 0.0235\nEpoch [7/20], Average Batch Training Loss: 0.0233\nEpoch [7/20], Validation Loss: 0.0232\nEpoch [8/20], Average Batch Training Loss: 0.0227\nEpoch [8/20], Validation Loss: 0.0219\nEpoch [9/20], Average Batch Training Loss: 0.0222\nEpoch [9/20], Validation Loss: 0.0213\nEpoch [10/20], Average Batch Training Loss: 0.0216\nEpoch [10/20], Validation Loss: 0.0213\nEpoch [11/20], Average Batch Training Loss: 0.0211\nEpoch [11/20], Validation Loss: 0.0206\nEpoch [12/20], Average Batch Training Loss: 0.0206\nEpoch [12/20], Validation Loss: 0.0203\nEpoch [13/20], Average Batch Training Loss: 0.0201\nEpoch [13/20], Validation Loss: 0.0196\nEpoch [14/20], Average Batch Training Loss: 0.0197\nEpoch [14/20], Validation Loss: 0.0186\nEpoch [15/20], Average Batch Training Loss: 0.0192\nEpoch [15/20], Validation Loss: 0.0193\nEpoch [16/20], Average Batch Training Loss: 0.0188\nEpoch [16/20], Validation Loss: 0.0189\nEpoch [17/20], Average Batch Training Loss: 0.0183\nEpoch [17/20], Validation Loss: 0.0180\nEpoch [18/20], Average Batch Training Loss: 0.0180\nEpoch [18/20], Validation Loss: 0.0184\nEpoch [19/20], Average Batch Training Loss: 0.0175\nEpoch [19/20], Validation Loss: 0.0178\nEpoch [20/20], Average Batch Training Loss: 0.0171\nEpoch [20/20], Validation Loss: 0.0168\nModel train time: 685.59 seconds\nCreating embeddings...\nNumber of unique (playerId, teamId, season) combinations: 1016\nTotal number of latent vectors: 2193\nComputing metrics...\nMetrics computation time: 833.47 seconds\nAverage MRR: 0.1057\nAverage Top-1 Accuracy: 0.0597\nAverage Top-3 Accuracy: 0.1254\nAverage Top-5 Accuracy: 0.1669\nAverage Top-10 Accuracy: 0.2330\nExecution time: 9120.29 seconds\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# Find the configuration with the highest avg_mrr\nbest_result = max(results, key=lambda x: x[\"metrics\"][\"avg_mrr\"])\n# Extract details\nbest_hyperparameters = best_result[\"hyperparameters\"]\nbest_metrics = best_result[\"metrics\"]\n\n# Print the hyperparameters\nprint(\"Best Model Configuration according to MRR\\n(Hyperparameters):\")\nfor key, value in best_hyperparameters.items():\n    print(f\"{key}: {value}\")\n\n# Print the metrics\nprint(\"\\nMetrics:\")\nprint(f\"Train loss: {best_metrics['train_loss']:.4f}\")\nprint(f\"Train loss: {best_metrics['val_loss']:.4f}\")\nprint(f\"Avg MRR: {best_metrics['avg_mrr']:.4f}\")\nfor k in [1, 3, 5, 10]:\n    print(f\"Avg Top-{k}: {best_metrics[f'avg_top_{k}']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T13:54:55.284716Z","iopub.execute_input":"2024-12-30T13:54:55.285051Z","iopub.status.idle":"2024-12-30T13:54:55.293569Z","shell.execute_reply.started":"2024-12-30T13:54:55.285025Z","shell.execute_reply":"2024-12-30T13:54:55.292767Z"}},"outputs":[{"name":"stdout","text":"Best Model Configuration according to MRR\n(Hyperparameters):\nlearning_rate: 1e-05\nbottleneck_size: 128\noptimizer: Adam\nloss_function: MSELoss\nbatch_size: 64\nweight_decay: 0\ndropout_rate: 0\n\nMetrics:\nTrain loss: 0.0171\nTrain loss: 0.0168\nAvg MRR: 0.1057\nAvg Top-1: 0.0597\nAvg Top-3: 0.1254\nAvg Top-5: 0.1669\nAvg Top-10: 0.2330\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntest_set = HeatmapCSVLoader(csv_path=\"/kaggle/input/sigmoid-10-chunks/passes-start-test.csv\")\ntest_loader = DataLoader(test_set, batch_size=16, shuffle=False)\n\n# Get a batch of images\nautoencoder.eval()\nimages, ids, playerIds, teamIds, seasons = next(iter(test_loader))\nimages = images.to(device)  # Move images to the selected device\n\n# Reconstruct images\nwith torch.no_grad():\n    reconstructed = autoencoder(images)\n\n# Plot original and reconstructed images (limit to 5 for better visualization)\nnum_examples = min(len(images), 5)  # Display up to 5 examples\nfig, axes = plt.subplots(2, num_examples, figsize=(15, 5))\nfor i in range(num_examples):\n    # Original\n    axes[0, i].imshow(images[i].cpu().squeeze(), cmap='gray')  # Move back to CPU for plotting\n    if isinstance(ids, (list, tuple)):\n        axes[0, i].set_title(f\"Original ({ids[i]})\")\n    else:\n        axes[0, i].set_title(\"Original\")\n    axes[0, i].axis('off')\n    \n    # Reconstructed\n    axes[1, i].imshow(reconstructed[i].cpu().squeeze(), cmap='gray')  # Move back to CPU for plotting\n    axes[1, i].set_title(\"Reconstructed\")\n    axes[1, i].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T13:56:24.407122Z","iopub.execute_input":"2024-12-30T13:56:24.407410Z","iopub.status.idle":"2024-12-30T13:56:25.880824Z","shell.execute_reply.started":"2024-12-30T13:56:24.407387Z","shell.execute_reply":"2024-12-30T13:56:25.879930Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 10 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":57}]}